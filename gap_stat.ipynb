{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4887e505-68fb-40c0-adb5-06ef3199e312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# Parallelization and monitoring\n",
    "from itertools import product\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "\n",
    "# Fitting\n",
    "from stepmix.stepmix import StepMix\n",
    "from src.model_fit import build_latent_model, do_StepMix, do_kmeans, do_AHC, do_hdbscan\n",
    "\n",
    "# Selection\n",
    "from kneed import KneeLocator\n",
    "from src.model_select import bootstrap_gap, compute_gap, get_gap, baseline_chi2, bootstrap_chi2\n",
    "\n",
    "# Statistical tests\n",
    "from scipy.stats import chi2\n",
    "from src.hopkins import hopkins\n",
    "from stepmix.bootstrap import blrt_sweep\n",
    "\n",
    "# Visualization\n",
    "from src.model_plot import plot_clusters, plot_cluster_profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d1e61c-3574-4482-9529-59268b7fb8d6",
   "metadata": {},
   "source": [
    "# Preparation\n",
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28820ec-976b-4d7b-92f4-62c50eb66c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_list = [\n",
    "    # Q2\n",
    "    'clseusa', # 'clsetown', 'clsestat', 'clsenoam',\n",
    "    # Q3\n",
    "    'ambornin', 'amcit', 'amlived', 'amenglsh', \n",
    "    'amchrstn', 'amgovt', 'amfeel', # 'amancstr',\n",
    "    # Q4\n",
    "    'amcitizn', 'amshamed', 'belikeus', 'ambetter', 'ifwrong', # 'amsports', 'lessprd',\n",
    "    # Q5\n",
    "    'proudsss', 'proudgrp', 'proudpol', 'prouddem', 'proudeco',\n",
    "    'proudspt', 'proudart', 'proudhis', 'proudmil', 'proudsci'\n",
    "]\n",
    "\n",
    "var_list_f = [var + \"_f\" for var in var_list]\n",
    "var_list_n = [var + \"_n\" for var in var_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c620b3f0-5c01-435c-a081-e4d4edfb7a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load imputed data\n",
    "data2004_i = pd.read_parquet(\"data/data2004_i.parquet\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e767a2f-4830-453c-a044-bcd771b288ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset with categorical outcomes\n",
    "data_f = data2004_i[var_list_f]\n",
    "\n",
    "## Label encoding\n",
    "data_f_lb = data_f.apply(lambda col: LabelEncoder().fit_transform(col))\n",
    "\n",
    "## One-hot encoding (for BVR calculation)\n",
    "columns = []\n",
    "for col in data_f_lb.columns:\n",
    "    for val in data_f_lb[col].unique():\n",
    "        columns.append((data_f_lb[col] == val).astype(int).rename(f'{col}_{val}'))\n",
    "data_f_oh = pd.concat(columns, axis=1)\n",
    "\n",
    "# Dataset with numeric outcomes\n",
    "data_n = data2004_i[var_list_n]\n",
    "\n",
    "## Scaling and normalizing / not used\n",
    "# scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "# data_n_scaled = scaler.fit_transform(data_n)\n",
    "# normalizer = StandardScaler()\n",
    "# data_n_norm = normalizer.fit_transform(data_n)\n",
    "\n",
    "# Dataset with controls\n",
    "controls = data2004_i[['sex', 'race_f', 'born_usa', 'party_fs', 'religstr_f', 'reltrad_f', 'region_f']]\n",
    "controls_dum = pd.get_dummies(controls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485d3846-2199-4199-9f73-e5e17ce4613f",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "*For the Silhouette and Dunn indices, the Mahnattan distance is used.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55b24ac-f239-4dea-9283-e3491ec65ee5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CVI = ['silhouette', 'calinski_harabasz', 'davies_bouldin', 'dunn']\n",
    "max_clust = 33\n",
    "max_threads = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1513b5bc-963b-41e5-a6bd-12f45f22c2f5",
   "metadata": {},
   "source": [
    "# 1. Fit models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f63d684-cd9c-43f6-9e4d-f457752063f6",
   "metadata": {},
   "source": [
    "## 1.1. Latent\n",
    "\n",
    "*With the [StepMix package](https://github.com/Labo-Lacourse/stepmix?tab=readme-ov-file).*\n",
    "\n",
    "*The methods used are **categorical** (multinomial) for LCA and **gaussian_tied** for LPA (where all Gaussian components share the same general covariance matrix). The default gaussian_diag (where each Gaussian component has its own diagonal covariance matrix) as well as gaussian_full (where each gaussian component has its own general covariance matrix) encountered severe convergence issues and produced highly unstable results. They also showed a tendency to overfit, as they yielded much higher log-likelihoods (LL) compared to the other models and proved very sensitive to scaling.*\n",
    "\n",
    "*Models with covariates are fitted through the 1 step approach, where the EM algorithm is run on both the measurement and structural models. Overall, 5 initializations with kmeans++ and slightly relaxed convergence thresholds (abs_tol = rel_tol = 1e-4) proved enough to get consistent results.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd2c93b-6c33-4b31-855f-2b5a1d37a37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "msrt = ['categorical', 'gaussian_tied']\n",
    "covar = ['without', 'with']\n",
    "latent_params = list(product(msrt, covar))\n",
    "\n",
    "clust_range = range(1, max_clust+1)\n",
    "latent_grid = product(clust_range, latent_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a46013-27ca-46c9-bc6a-f9b44aa2f9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "time1 = time.time()\n",
    "results = Parallel(n_jobs=max_threads)(\n",
    "    delayed(do_StepMix)(\n",
    "        data_f_lb if msrt == 'categorical' else data_n,\n",
    "        controls_dum if covar == 'with' else None,\n",
    "        data_f_oh if msrt == 'categorical' else None,\n",
    "        n, \n",
    "        msrt, \n",
    "        covar)\n",
    "    for n, (msrt, covar) in tqdm(latent_grid, desc='Fitting latent models')\n",
    ")\n",
    "time2 = time.time()\n",
    "\n",
    "latent_all = pd.DataFrame(results)\n",
    "print(f\"Time to fit latent models: {time2-time1:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1f7d26-3c78-4175-8ecf-bd03e57cc533",
   "metadata": {},
   "outputs": [],
   "source": [
    "LCA_nocov = latent_all[latent_all['params'] == {'msrt': 'categorical', 'covar': 'without'}].reset_index(drop=True)\n",
    "LCA_covs = latent_all[latent_all['params'] == {'msrt': 'categorical', 'covar': 'with'}].reset_index(drop=True)\n",
    "LPA_nocov = latent_all[latent_all['params'] == {'msrt': 'gaussian_tied', 'covar': 'without'}].reset_index(drop=True)\n",
    "LPA_covs = latent_all[latent_all['params'] == {'msrt': 'gaussian_tied', 'covar': 'with'}].reset_index(drop=True)\n",
    "\n",
    "improv_covs_LCA = 100 * ((LCA_covs['sabic'] / LCA_nocov['sabic']) - 1)\n",
    "improv_covs_LPA = 100 * ((LPA_covs['sabic'] / LPA_nocov['sabic']) - 1)\n",
    "improv_gaussian_nocov = 100 * ((LPA_nocov['sabic'] / LCA_nocov['sabic']) - 1)\n",
    "improv_gaussian_covs = 100 * ((LPA_covs['sabic'] / LCA_covs['sabic']) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289d18f7-f2b5-4edf-ad93-49db4015e0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "improv = pd.DataFrame({\n",
    "    \"Clusters\": clust_range,\n",
    "    \"LCA\": improv_covs_LCA.round(1),\n",
    "    \"LPA\": improv_covs_LPA.round(1),\n",
    "    \"no covar.\": improv_gaussian_nocov.round(1),\n",
    "    \"covars.\": improv_gaussian_covs.round(1)\n",
    "})\n",
    "improv.set_index('Clusters', inplace=True)\n",
    "improv.columns = pd.MultiIndex.from_tuples([\n",
    "    ('Covariates', 'LCA'), \n",
    "    ('Covariates', 'LPA'), \n",
    "    ('Gaussian', 'no covar.'), \n",
    "    ('Gaussian', 'covars.')\n",
    "])\n",
    "\n",
    "print(\"Evolution of SABIC (in %) brought by:\")\n",
    "print(\"- introducing covariates in LCA and LPA models\")\n",
    "print(\"- switching from LCA to LPA for models without and with covariates\")\n",
    "improv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa257eee-187a-4c71-9f45-6001cce0b478",
   "metadata": {},
   "source": [
    "*The inclusion of covariates proves detrimental, as it increases the SABIC for both categorical and continuous models. While it slightly improves fit, it also introduces many additional free parameters, increasing the risk of overfitting. Moreover, covariates significantly increase computation time, so they will be excluded from subsequent analyses.*\n",
    "\n",
    "*Switching from categorical to continuous models has mixed effects. It negatively impacts SABIC, which decreases steadily as the number of classes increases. This is due to differences in the number of free parameters: for Gaussian tied models, the parameter count starts higher but grows more slowly with additional classes. In terms of entropy, Gaussian models yield lower values up to five classes but higher values beyond that. Since their usefulness depends on the final number of classes chosen, they will be retained for further analysis.*\n",
    "\n",
    "## 1.2. k-means\n",
    "\n",
    "*With a custom implementation, as scikit-learn does not allow to change the linkage function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87945e4f-7069-4a5e-8ec1-b0ef4cb07994",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = ['euclidean', 'manhattan', 'chebyshev']\n",
    "link = ['mean', 'median', 'medoid']\n",
    "kmeans_params = list(product(dist, link))\n",
    "\n",
    "clust_range = range(2, max_clust+1)\n",
    "kmeans_grid = product(clust_range, kmeans_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa878a3-e45f-4205-89dd-279a0c921f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "time1 = time.time()\n",
    "results = Parallel(n_jobs=max_threads)(\n",
    "    delayed(do_kmeans)(data_n, n, dist, link) \n",
    "    for n, (dist, link) in tqdm(kmeans_grid, desc='Fitting KMeans models')\n",
    ")\n",
    "time2 = time.time()\n",
    "print(f\"Time to fit k-means models: {time2-time1:.2f} seconds\")\n",
    "\n",
    "kmeans_all = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a232344-8695-4408-8564-f72d79b7ecd2",
   "metadata": {},
   "source": [
    "## 1.3. AHC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6ab7f5-cafd-49ac-84da-e97c878c05ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = ['manhattan', 'euclidean', 'chebyshev', 'hamming']\n",
    "linkages = ['single', 'average', 'complete']\n",
    "ahc_params = [*product(distances, linkages), ('euclidean', 'ward')]\n",
    "\n",
    "clust_range = range(1, max_clust+1)\n",
    "ahc_grid = product(clust_range, ahc_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32474867-62d1-43fe-a818-47ef322016d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "time1 = time.time()\n",
    "results = Parallel(n_jobs=max_threads)(\n",
    "    delayed(do_AHC)(data_n, n, dist, link) \n",
    "    for n, (dist, link) in tqdm(ahc_grid, desc='Fitting AHC models')\n",
    ")\n",
    "time2 = time.time()\n",
    "print(f\"Time to fit AHC models: {time2-time1:.2f} seconds\")\n",
    "\n",
    "ahc_all = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b26c1c-d087-4f08-84fe-c724be527d65",
   "metadata": {},
   "source": [
    "## 1.4. HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c0f087-2870-4ea9-9b31-c1fe9ab3288a",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = ['manhattan', 'euclidean', 'chebyshev', 'mahalanobis', 'hamming']\n",
    "min_cluster_sizes = range(2, 21)\n",
    "min_samples_range = range(1, 21)\n",
    "hdb_params = product(distances, min_cluster_sizes, min_samples_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4d6adc-55fe-43cf-9fc0-0b6e4dc98031",
   "metadata": {},
   "outputs": [],
   "source": [
    "time1 = time.time()\n",
    "results = Parallel(n_jobs=max_threads)(\n",
    "    delayed(do_hdbscan)(data_n, dist, min_clust, min_smpl)\n",
    "    for dist, min_clust, min_smpl in tqdm(hdb_params, desc='Fitting HDBSCAN models')\n",
    ")\n",
    "time2 = time.time()\n",
    "print(f\"Time to fit HDBSCAN models: {time2-time1:.2f} seconds\")\n",
    "\n",
    "hdbscan_all = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd77545-5713-4fe0-970c-11c4d346a180",
   "metadata": {},
   "source": [
    "## 1.5. Aggregate results and compare CVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce063cf-1433-4461-851a-646030af0069",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = pd.concat([latent_all, kmeans_all, ahc_all, hdbscan_all]).reset_index(drop=True)\n",
    "all_models.to_csv(\"output/models/all_models.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7026f836-18b1-47e7-a2ab-f6fdd626bdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_CVI = all_models[['silhouette', 'calinski_harabasz', 'davies_bouldin', 'dunn']]\n",
    "labels = {\n",
    "    'silhouette': 'Silhouette',\n",
    "    'calinski_harabasz': 'Calinski-Harabasz',\n",
    "    'davies_bouldin': 'Davies-Bouldin',\n",
    "    'dunn': 'Dunn 43'\n",
    "}\n",
    "\n",
    "correlations = all_CVI.corr(method='spearman')\n",
    "correlations = correlations.rename(index=labels, columns=labels)\n",
    "\n",
    "plt.figure(figsize=(5, 5)) \n",
    "sns.heatmap(correlations, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True, \n",
    "            square=True, linewidths=0.5, vmin=-1, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37942d04-b031-4ad2-a101-5ee01cdeca2c",
   "metadata": {},
   "source": [
    "*The correlation between CVIs is generally low. Therefore, all will be retained for subsequent analyses.*\n",
    "\n",
    "# 2. Select models\n",
    "## 2.1. Fit criteria for latent models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4307d70-0075-4f6a-b57a-cfbbffbd8ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_stats = latent_all.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef720af-16a1-4b88-baad-228065962cde",
   "metadata": {},
   "source": [
    "### 2.1.1. Absolute value for AIC / BIC / SABIC / entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b545b53-4345-4385-b183-0a5e43b1d341",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_aic = latent_all.sort_values('aic', ascending=True).iloc[0]\n",
    "min_bic = latent_all.sort_values('bic', ascending=True).iloc[0]\n",
    "min_sabic = latent_all.sort_values('sabic', ascending=True).iloc[0]\n",
    "max_entropy = latent_all.sort_values('relative_entropy', ascending=False).iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a17626-aa26-48d5-a7a3-1137011025ca",
   "metadata": {},
   "source": [
    "### 2.1.2. Elbow method for AIC / BIC / SABIC / entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbebbae1-2287-4498-b09b-90924002aba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_method(df, val_index):\n",
    "    res = df.dropna(subset=[val_index])\n",
    "    x = res['n_clust']\n",
    "    y = res[val_index]\n",
    "\n",
    "    if val_index == 'relative_entropy':\n",
    "        knee_locator = KneeLocator(x, y, curve='concave', direction='increasing')\n",
    "    else:\n",
    "        knee_locator = KneeLocator(x, y, curve='convex', direction='decreasing')\n",
    "    \n",
    "    return res[res[\"n_clust\"] == knee_locator.knee]\n",
    "\n",
    "def best_elbow_model(index):\n",
    "    candidate_models = pd.DataFrame()\n",
    "\n",
    "    for msrt in ['categorical', 'gaussian_tied']:\n",
    "        for covar in ['without', 'with']:\n",
    "            mask = (latent_stats['params'] == {'msrt': msrt, 'covar': covar})\n",
    "            models = latent_stats[mask]\n",
    "            elbow_res = elbow_method(models, index)\n",
    "            if elbow_res is not None:\n",
    "                candidate_models = pd.concat([candidate_models, elbow_res], ignore_index=True)\n",
    "    \n",
    "    if candidate_models.empty:\n",
    "        return None\n",
    "    return candidate_models.sort_values(index, ascending=True).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31753ef9-fb01-4f96-9f05-62a6e690d1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "elbow_aic = best_elbow_model('aic')\n",
    "elbow_bic = best_elbow_model('bic')\n",
    "elbow_sabic = best_elbow_model('sabic')\n",
    "elbow_entropy = best_elbow_model('relative_entropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44e4012-06a8-4078-b471-5a9406952159",
   "metadata": {},
   "source": [
    "### 2.1.3. Statistical tests for log-likelihood\n",
    "*LRT - not advisable for comparing models with $C$ and $C-1$ classes as the resulting test statistics does not converge towards a $\\chi^2$ distribution under the null hypothesis.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74c0d36-1781-426c-96f8-24e94d838bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LRT(models):\n",
    "    # LRT test\n",
    "    _2LL_stat = - 2 * (models['LL'].diff())\n",
    "    _2LL_df = models['df'].diff()\n",
    "    _2LL_p_val = 1 - chi2.cdf(_2LL_stat, _2LL_df)\n",
    "\n",
    "    # L2 reduction\n",
    "    _2LL_red = 1 - (models['LL'] / models['LL'].iloc[0])\n",
    "\n",
    "    results = pd.DataFrame({\n",
    "        '-2LL': _2LL_stat,\n",
    "        '-2LL_df': _2LL_df,\n",
    "        'LRT_pval': _2LL_p_val,\n",
    "        '-2LL_red_%': 100 * _2LL_red\n",
    "    }, index=models.index)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fff783-dca3-4849-bf25-3e4605978411",
   "metadata": {},
   "outputs": [],
   "source": [
    "for msrt in ['categorical', 'gaussian_tied']:\n",
    "    for covar in ['without', 'with']:\n",
    "        mask = (latent_stats['params'] == {'msrt': msrt, 'covar': covar})\n",
    "        models = latent_stats[mask]\n",
    "        lrt_results = LRT(models)\n",
    "        latent_stats.loc[mask, ['-2LL', '-2LL_df', 'LRT_pval', '-2LL_red_%']] = lrt_results.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cd8486-d85b-49dd-bad1-a53d18e4cdab",
   "metadata": {},
   "source": [
    "*BLRT - not implemented in StepMix for models with covariates.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257ce177-2fd3-477a-8d18-ca10054b9a53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iters = 100\n",
    "\n",
    "time1 = time.time()\n",
    "for msrt in ['categorical', 'gaussian_tied']:\n",
    "    latent_mod = build_latent_model(n = None, msrt = msrt, covar = 'without')\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "        \n",
    "        blrt = blrt_sweep(\n",
    "            latent_mod,\n",
    "            data_f_lb if msrt == 'categorical' else data_n,\n",
    "            low=1,\n",
    "            high=max_clust,\n",
    "            n_repetitions=iters)\n",
    "        \n",
    "        # Add a row for the 1-class model\n",
    "        blrt = pd.concat([pd.DataFrame({'p': [np.nan]}), blrt]).reset_index(drop=True)\n",
    "        \n",
    "    mask = (latent_stats['params'] == {'msrt': msrt, 'covar': 'without'})\n",
    "    latent_stats.loc[mask, ['BLRT_pval']] = blrt.values\n",
    "    \n",
    "time2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2af59f-4c14-4fa4-b281-451d6ef0fa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Time to bootstrap LRT: {time2-time1:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f4108d-5bed-4b31-824c-6ba6167c61f4",
   "metadata": {},
   "source": [
    "*BVRT - requires a different bootstrapping than the BLRT with samples generated under an alternative assumption*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8cac6e-0e41-4484-8e8f-d6fbe45e1c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 100\n",
    "\n",
    "covar = 'without'\n",
    "config = {'msrt': 'categorical', 'covar': covar}\n",
    "controls = controls_dum if covar == 'with' else None\n",
    "\n",
    "time1 = time.time()\n",
    "for n in range(1, max_clust+1):\n",
    "    ref_l2, ref_data = baseline_chi2(\n",
    "        data = data_f_lb,\n",
    "        bvr_data = data_f_oh,\n",
    "        n = n,\n",
    "        covar = covar,\n",
    "        controls = controls)\n",
    "    \n",
    "    btsp_results = Parallel(n_jobs=max_threads)(\n",
    "        delayed(bootstrap_chi2)(ref_data, controls, n, covar, b)\n",
    "        for b in tqdm(range(1, B+1), desc = f\"Bootstrapping Chi2 (n={n})\"))\n",
    "    \n",
    "    s = (btsp_results > ref_l2).sum()\n",
    "    btsp_chi2_pval = 100 * (s+1) / (B+1)\n",
    "    \n",
    "    row_id = ((latent_stats['params'] == config) & (latent_stats['n_clust'] == n))\n",
    "    latent_stats.loc[row_id, 'btsp_chi2_pval'] = btsp_chi2_pval\n",
    "\n",
    "time2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8080a6d8-8772-48be-8706-4b000f47f50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Time to bootstrap Chi2: {time2-time1:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d8e1dc-5d1b-415a-b51f-a7b75be5ca46",
   "metadata": {},
   "source": [
    "## 2.2. Gap statistic for latent models / kmeans / AHC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a01e25-6da4-4f9a-ac5c-681fe87b4307",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = pd.concat([latent_all, kmeans_all, ahc_all]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea213eae-d620-482e-9e5c-1434859f6291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure params are a dictionary to avoid errors afterwards\n",
    "if isinstance(all_models['params'].iloc[0], str):\n",
    "    all_models['params'] = all_models['params'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6726f137-707b-4c61-9b88-04f607d25026",
   "metadata": {},
   "source": [
    "*Step 1: compute the gap statistic for each model-config*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eed8683-ac4a-4048-a5a7-4d31e355fb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude categorical models with covariates\n",
    "latent_params = list([('categorical', 'without'),\n",
    "                      ('gaussian_tied', 'without')])\n",
    "\n",
    "params = {'kmeans': kmeans_params,\n",
    "          'AHC': ahc_params,\n",
    "          'latent': latent_params}\n",
    "\n",
    "param_names = {'kmeans': ['dist', 'link'],\n",
    "               'AHC': ['dist', 'link'],\n",
    "               'latent': ['msrt', 'covar']}\n",
    "\n",
    "models = ['kmeans', 'AHC', 'latent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fec7cd-3b63-4d4b-bc0d-3752debd1ae6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Bootstrap\n",
    "iters = 100\n",
    "\n",
    "bootstrap_grid = [\n",
    "    (model, {key: value for key, value in zip(param_names[model], param_values)}, n_val, n_iter)\n",
    "    for model in models\n",
    "    for param_values in params[model]\n",
    "    for n_val in (range(1, max_clust+1) if model == 'latent' else range(2, max_clust+1))\n",
    "    for n_iter in range(iters)\n",
    "]\n",
    "\n",
    "time1 = time.time()\n",
    "results = Parallel(n_jobs=max_threads)(\n",
    "    delayed(bootstrap_gap)(\n",
    "        data = data_f_lb if model == 'latent' and config.get('msrt') == 'categorical' else data_n,\n",
    "        controls = controls_dum if model == 'latent' and config.get('covar') == 'with' else None,\n",
    "        bvr_data = data_f_oh if model == 'latent' and config.get('msrt') == 'categorical' else None,\n",
    "        n = n,\n",
    "        model = model,\n",
    "        params = config,\n",
    "        iter_num = iter_num)\n",
    "    for model, config, n, iter_num in tqdm(bootstrap_grid, desc='Bootstrapping CVIs')\n",
    ")\n",
    "time2 = time.time()\n",
    "\n",
    "bootstrap_results = pd.concat(results).reset_index(drop=True)\n",
    "print(f\"Time to compute gap statistics: {time2-time1:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc8cea1-2e68-43d5-90b0-81ee18d68158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gap values\n",
    "model_grid = [\n",
    "    (model, dict(zip(param_names[model], param_values)))\n",
    "    for model in models\n",
    "    for param_values in params[model]\n",
    "]\n",
    "\n",
    "gap_values = []\n",
    "\n",
    "for model, config in model_grid:\n",
    "    rows_id = ((bootstrap_results['model'] == model) & (bootstrap_results['params'] == config))    \n",
    "    bs_select_res = bootstrap_results[rows_id]\n",
    "    gap_stats = compute_gap(bs_select_res, all_models, model, config, CVI)\n",
    "    gap_values.append(gap_stats)\n",
    "\n",
    "gap_values = pd.concat(gap_values, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657f0eaa-0fbf-4fe5-9459-2f6d251da20e",
   "metadata": {},
   "source": [
    "*Step 2: identify the optimal number of clusters for each model-config*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baad9661-0ffa-4ce3-ba43-d0f5c60a73d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df to store results\n",
    "cols = ['model', 'params', 'n_clust'] + \\\n",
    "       [index for index in CVI] + \\\n",
    "       [f'{index}_gap' for index in CVI]\n",
    "\n",
    "candidate_models = pd.DataFrame(columns=cols)\n",
    "candidate_models['model'] = candidate_models['model'].astype('object')\n",
    "candidate_models['params'] = candidate_models['params'].astype('object')\n",
    "\n",
    "float_cols = [col for col in cols if col not in ['model', 'params', 'n_clust'] + CVI]\n",
    "candidate_models[float_cols] = candidate_models[float_cols].astype('float64')\n",
    "int_cols = [col for col in cols if col in ['n_clust'] + CVI]\n",
    "candidate_models[int_cols] = candidate_models[int_cols].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95226fed-3e91-4bf2-bc52-6de7cae48c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best n\n",
    "for model, config in model_grid:\n",
    "    for index in CVI:\n",
    "        best_n = get_gap(gap_values, model, config, index)\n",
    "\n",
    "        # Check if a best value has been identified\n",
    "        if best_n != 'none':\n",
    "            row_id = ((candidate_models['model'] == model) & \n",
    "                      (candidate_models['params'] == config) &\n",
    "                      (candidate_models['n_clust'] == best_n))\n",
    "            \n",
    "            # Check if the corresponding row exists in the df\n",
    "            if candidate_models[row_id].empty:\n",
    "\n",
    "                model_id = ((all_models['model'] == model) & \n",
    "                           (all_models['params'] == config) &\n",
    "                           (all_models['n_clust'] == best_n))\n",
    "                \n",
    "                new_row = {\n",
    "                    'model': model,\n",
    "                    'params': config,\n",
    "                    'n_clust': best_n,\n",
    "                    'min_clust_size': all_models.loc[model_id, 'min_clust_size'].values[0],\n",
    "                    'max_clust_size': all_models.loc[model_id, 'max_clust_size'].values[0],\n",
    "                    'silhouette': all_models.loc[model_id, 'silhouette'].values[0],\n",
    "                    'calinski_harabasz': all_models.loc[model_id, 'calinski_harabasz'].values[0],\n",
    "                    'davies_bouldin': all_models.loc[model_id, 'davies_bouldin'].values[0],\n",
    "                    'dunn': all_models.loc[model_id, 'dunn'].values[0],\n",
    "                    f'{index}_gap': 1\n",
    "                }\n",
    "                \n",
    "                new_row = pd.DataFrame([new_row])\n",
    "                candidate_models = pd.concat([candidate_models, new_row], ignore_index=True)\n",
    "\n",
    "            # Otherwise, update the existing row\n",
    "            else:\n",
    "                candidate_models.loc[row_id, f'{index}_gap'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cfd4b8-26e2-44c8-94a4-4784e487c101",
   "metadata": {},
   "source": [
    "*Step 3: identify the best model for each class*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44b56f0-371d-4c52-b91f-dc24963172c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CVI_results = {}\n",
    "\n",
    "for index in CVI:\n",
    "    CVI_results[index] = []\n",
    "    df = candidate_models[candidate_models[f'{index}_gap'] == 1]\n",
    "    \n",
    "    for model in models:\n",
    "        sub_df = df[df['model'] == model]\n",
    "\n",
    "        if sub_df.empty:\n",
    "            continue\n",
    "        else:\n",
    "            if index == 'davies_bouldin':\n",
    "                best_mod = sub_df.sort_values(index, ascending=True).iloc[0]\n",
    "            else:\n",
    "                best_mod = sub_df.sort_values(index, ascending=False).iloc[0]\n",
    "            CVI_results[index].append(best_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e846f716-04a3-40a4-9615-407f482c7471",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_sil = pd.DataFrame(CVI_results['silhouette'])\n",
    "best_ch = pd.DataFrame(CVI_results['calinski_harabasz'])\n",
    "best_db = pd.DataFrame(CVI_results['davies_bouldin'])\n",
    "best_dunn = pd.DataFrame(CVI_results['dunn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36cf2af-7104-42dc-b24c-f50879a8b1b2",
   "metadata": {},
   "source": [
    "## 2.3. Min/max for HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c43f41-ddbb-4bd9-90af-ed1f9f7b2411",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_sil = pd.concat([best_sil, hdbscan_all.sort_values('silhouette', ascending=False).iloc[0:1]], axis=0)\n",
    "best_ch = pd.concat([best_ch, hdbscan_all.sort_values('calinski_harabasz', ascending=False).iloc[0:1]], axis=0)\n",
    "best_db = pd.concat([best_db, hdbscan_all.sort_values('davies_bouldin', ascending=True).iloc[0:1]], axis=0)\n",
    "best_dunn = pd.concat([best_dunn, hdbscan_all.sort_values('dunn', ascending=False).iloc[0:1]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c1c535-d42c-42c5-b221-a61950ff32b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_sil = best_sil.drop(columns=[col for col in best_sil.columns if col.endswith(('elbow', 'abs', 'gap'))])\n",
    "best_ch = best_ch.drop(columns=[col for col in best_ch.columns if col.endswith(('elbow', 'abs', 'gap'))])\n",
    "best_db = best_db.drop(columns=[col for col in best_db.columns if col.endswith(('elbow', 'abs', 'gap'))])\n",
    "best_dunn = best_dunn.drop(columns=[col for col in best_dunn.columns if col.endswith(('elbow', 'abs', 'gap'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d238da8-31c8-4254-938f-d7b6739eb653",
   "metadata": {},
   "source": [
    "# 3. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5d25c4-065c-4727-ba3e-66a023351d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disp_params(d):\n",
    "    return f\"{d['msrt']} {d['covar']} covariates\"\n",
    "\n",
    "def refit_best_model(df):\n",
    "    model = df.loc[0, 'model']\n",
    "    config = df.loc[0, 'params']\n",
    "    n_clust = int(df.loc[0, 'n_clust'])\n",
    "\n",
    "    if model == 'latent':\n",
    "        results = do_StepMix(\n",
    "            data_f_lb if config['msrt'] == 'categorical' else data_n,\n",
    "            controls_dum if covar == 'with' else None,\n",
    "            data_f_oh if config['msrt'] == 'categorical' else None,                \n",
    "            n_clust, \n",
    "            refit = True, \n",
    "            **config)\n",
    "\n",
    "    elif model == 'kmeans':\n",
    "        results = do_kmeans(data_n, n_clust, refit = True, **config)\n",
    "    \n",
    "    elif model == 'AHC':\n",
    "        results = do_AHC(data_n, n_clust, refit = True, **config)\n",
    "    \n",
    "    elif model == 'HDBSCAN':\n",
    "        results = do_hdbscan(data_n, refit = True, **config)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c48d35e-6c11-47a8-b55b-ef94b464e761",
   "metadata": {},
   "source": [
    "## 3.1. Latent models / fit criteria\n",
    "### 3.1.1. Absolute values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6f7b48-07f4-46f4-82c6-da26a74db5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model minimizing AIC is {disp_params(min_aic['params'])} and {min_aic['n_clust']} clusters.\")\n",
    "print(f\"Model minimizing BIC is {disp_params(min_bic['params'])} and {min_bic['n_clust']} clusters.\")\n",
    "print(f\"Model minimizing SABIC is {disp_params(min_sabic['params'])} and {min_sabic['n_clust']} clusters.\")\n",
    "print(f\"Model maximizing entropy is {disp_params(max_entropy['params'])} and {max_entropy['n_clust']} clusters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4223b8f9-c54d-4ec3-8fea-31bb1e89981a",
   "metadata": {},
   "source": [
    "### 3.1.2 Elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8680bdc3-db48-478c-9f7e-0edf57c70281",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best model according to the Elbow method applied to...\")\n",
    "if elbow_aic is None: print(\"- AIC is None\")\n",
    "else: print(f\"- AIC is {disp_params(elbow_aic['params'])} and {elbow_aic['n_clust']} clusters.\")\n",
    "if elbow_bic is None: print(\"- BIC is None\")\n",
    "else: print(f\"- BIC is {disp_params(elbow_bic['params'])} and {elbow_bic['n_clust']} clusters.\")\n",
    "if elbow_sabic is None: print(\"- SABIC is None\")\n",
    "else: print(f\"- SABIC is {disp_params(elbow_sabic['params'])} and {elbow_sabic['n_clust']} clusters.\")\n",
    "if elbow_entropy is None: print(\"- Entropy is None\")\n",
    "else: print(f\"- Relative entropy is {disp_params(elbow_entropy['params'])} and {elbow_entropy['n_clust']} clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cc2d16-0c02-4a6d-9808-08a5c719224e",
   "metadata": {},
   "source": [
    "### 3.1.3. Statistical tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe47109-3775-49b8-8976-b81942668fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LCA = latent_stats[latent_stats['params'] == {'msrt': 'categorical', 'covar': 'without'}].reset_index(drop=True)\n",
    "LPA = latent_stats[latent_stats['params'] == {'msrt': 'gaussian_tied', 'covar': 'without'}].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b58691e-616d-43a8-804e-3240c5ddc968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_pval(df, crit, gap=0, threshold=0.05):\n",
    "    if df[df[crit] > threshold].empty:\n",
    "        return f\"None (up to max_clust = {max_clust})\"\n",
    "    else:\n",
    "        return df[df[crit] > threshold].iloc[0]['n_clust'] + gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b78cbb7-c091-4927-9249-56470b8ca65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LRT - select the last model to be significantly different from the previous one\n",
    "print(\"Optimal number of clusters according to LRT:\")\n",
    "print(f\"- {select_pval(LCA, 'LRT_pval', -1)} for LCA\")\n",
    "print(f\"- {select_pval(LPA, 'LRT_pval', -1)} for LPA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1470c371-13f0-461b-beec-a2f6ca3a4b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLRT - select the last model to be significantly different from the previous one\n",
    "print(\"Optimal number of clusters according to BLRT:\")\n",
    "print(f\"- {select_pval(LCA, 'BLRT_pval', -1)} for LCA\")\n",
    "print(f\"- {select_pval(LPA, 'BLRT_pval', -1)} for LPA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dbc5f7-c2bf-4c70-a43d-732e292686b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi2 - select the first model to becom non-significant (i.e., where the local independance assumption cannot be rejected)\n",
    "print(\"Optimal number of clusters according to raw Chi2:\")\n",
    "print(f\"- {select_pval(LCA, 'chi2_pval')} for LCA\")\n",
    "print(f\"- {select_pval(LPA, 'chi2_pval')} for LPA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47be9b0-7498-43ab-a551-7e600d8463fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrapped Chi2 - select the first model to becom non-significant (i.e., where the local independance assumption cannot be rejected)\n",
    "print(\"Optimal number of clusters according to bootstrapped Chi2:\")\n",
    "print(f\"- {select_pval(LCA, 'btsp_chi2_pval')} for LCA\")\n",
    "print(f\"- {select_pval(LPA, 'btsp_chi2_pval')} for LPA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61b491a-9c1e-45e9-9e33-7c0ff8195808",
   "metadata": {},
   "source": [
    "### 3.1.3. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193df2a9-de85-4d3b-a36f-6128a0a9ee25",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['model', 'params', 'n_clust', 'min_clust_size', 'max_clust_size', 'silhouette', 'calinski_harabasz',\n",
    "        'davies_bouldin', 'dunn', 'aic', 'bic', 'sabic','relative_entropy', 'classif_error', 'df', 'LL',\n",
    "        '-2LL_red_%', 'LRT_pval',  'BLRT_pval']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f467f6b-9e62-42ff-8b1c-820b610ab7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "LCA = LCA[cols + ['chi2_pval', 'btsp_chi2_pval']]\n",
    "LCA.to_csv(\"output/models/LCA_models.csv\", index=False)\n",
    "LCA.drop(columns=['model', 'params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892dd2e8-142c-40d7-b34c-46e218adc1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "LPA = LPA[cols]\n",
    "LPA.to_csv(\"output/models/LPA_models.csv\", index=False)\n",
    "LPA.drop(columns=['model', 'params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253b6d5a-56e6-4349-bb49-d64bcb3f10b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f_blrt = select_pval(LCA, 'BLRT_pval', -1)\n",
    "best_c_blrt = select_pval(LPA, 'BLRT_pval', -1)\n",
    "\n",
    "if isinstance(best_f_blrt, str) and isinstance(best_c_blrt, str):\n",
    "    print(\"No best model according to BLRT\")\n",
    "elif isinstance(best_f_blrt, str):\n",
    "    blrt_model = LPA[LPA['n_clust'] == best_c_blrt]\n",
    "elif isinstance(best_c_blrt, str):\n",
    "    blrt_model = LCA[LCA['n_clust'] == best_f_blrt]\n",
    "else:\n",
    "    LCA_sabic = LCA[LCA['n_clust'] == best_c_blrt]['sabic'].iloc[0]\n",
    "    LPA_sabic = LPA[LPA['n_clust'] == best_f_blrt]['sabic'].iloc[0]\n",
    "    if LCA_sabic >= LPA_sabic:\n",
    "        blrt_model = LCA[LCA['n_clust'] == best_f_blrt]\n",
    "    else:\n",
    "        blrt_model = LPA[LPA['n_clust'] == best_c_blrt]\n",
    "\n",
    "blrt_model = blrt_model.reset_index(drop=True)\n",
    "blrt_model.to_csv(\"output/models/best_latent.csv\", index=False)\n",
    "pred_clust_latent = refit_best_model(blrt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a4ac49-67ee-423d-bf81-0e1b83ba311c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(\n",
    "    data_f_lb if blrt_model.loc[0, 'params'].get('msrt') == 'categorical' else data_n,\n",
    "    pred_clust_latent,\n",
    "    '2D PCA Projection of the Partition According from the best latent model according to the BLRT',\n",
    "    'latent_clust')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a4433c-0798-4a41-8c10-8899014b7fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.unique(pred_clust_latent, return_counts=True)[1].min() > 5:\n",
    "    plot_cluster_profiles(data_n, pred_clust_latent, feature_names = var_list, sd = 1, title = 'BLRT', filename = 'latent_patterns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486dfe02-4c83-4bec-9945-ee8e5969d539",
   "metadata": {},
   "source": [
    "## 3.2. CVI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1d6456-54d9-4779-9ee6-4be2dc5802cf",
   "metadata": {},
   "source": [
    "### 3.2.1. Silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4f01f2-3d9b-4427-896c-53746e4d8e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "sil = best_sil.sort_values('silhouette', ascending=False).drop(columns=['calinski_harabasz', 'davies_bouldin', 'dunn']).reset_index(drop=True)\n",
    "sil.to_csv(\"output/models/best_sil.csv\", index=False)\n",
    "sil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6e0967-d0b9-4828-b37a-4ba1f7664350",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_clust_sil = refit_best_model(sil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27952809-eaa2-4a20-bf09-697f6cffe981",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(\n",
    "    data_f_lb if (sil.loc[0, 'model'] == 'latent') and (sil.loc[0, 'params'].get('msrt') == 'categorical') else data_n,\n",
    "    pred_clust_sil,\n",
    "    '2D PCA Projection of the Best Partition According to the Silhouette Index',\n",
    "    'sil_clust')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dd76e3-2cf8-4e10-9ed5-71c6a4870767",
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.unique(pred_clust_sil, return_counts=True)[1].min() > 5:\n",
    "    plot_cluster_profiles(data_n, pred_clust_sil, feature_names = var_list, sd = 1, title = 'Silhouette', filename = 'sil_patterns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e032f3-c798-4de8-b540-2754b600a1ed",
   "metadata": {},
   "source": [
    "### 3.2.2. Calinski-Harabasz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932b49d9-62fa-41a5-9ff9-b51b336bc0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch = best_ch.sort_values('calinski_harabasz', ascending=False).drop(columns=['silhouette', 'davies_bouldin', 'dunn']).reset_index(drop=True)\n",
    "ch.to_csv(\"output/models/best_ch.csv\", index=False)\n",
    "ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b27502c-c1c8-4347-bf70-a6989038b1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_clust_ch = refit_best_model(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a4d4c8-b7b8-4177-b6d1-1d6306ea8594",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(\n",
    "    data_f_lb if (ch.loc[0, 'model'] == 'latent') and (ch.loc[0, 'params'].get('msrt') == 'categorical') else data_n, \n",
    "    pred_clust_ch,\n",
    "    '2D PCA Projection of the Best Partition According to the Calinski-Harabasz Index',\n",
    "    'ch_clust')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb110f5a-a42e-452d-b2ec-daa9b817ffcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.unique(pred_clust_ch, return_counts=True)[1].min() > 5:\n",
    "    plot_cluster_profiles(data_n, pred_clust_ch, feature_names = var_list, sd = 1, title = 'Calinski-Harabaz', filename = 'ch_patterns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e198a76-d112-407a-a016-cee4d03f7e30",
   "metadata": {},
   "source": [
    "### 3.2.3. Davies-Bouldin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7a791b-0d40-448d-9587-8b0e553c7531",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = best_db.sort_values('davies_bouldin', ascending=True).drop(columns=['silhouette', 'calinski_harabasz', 'dunn']).reset_index(drop=True)\n",
    "db.to_csv(\"output/models/best_db.csv\", index=False)\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02fcd9c-d6c4-4acf-ba73-c901d5f2c47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_clust_db = refit_best_model(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c8375e-7246-4c8c-a76b-8c8b8268f12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(\n",
    "    data_f_lb if (db.loc[0, 'model'] == 'latent') and (db.loc[0, 'params'].get('msrt') == 'categorical') else data_n, \n",
    "    pred_clust_db,\n",
    "    '2D PCA Projection of the Best Partition According to the Davies-Bouldin Index',\n",
    "    'db_clust')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc5ab7f-39a0-4be7-9d04-192012a64a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.unique(pred_clust_db, return_counts=True)[1].min() > 5:\n",
    "    plot_cluster_profiles(data_n, pred_clust_db, feature_names = var_list, sd = 1, title = 'Davies-Bouldin', filename = 'db_patterns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e0681e-eaf8-4529-bf7a-3053a3e147d0",
   "metadata": {},
   "source": [
    "### 3.2.4. Generalized Dunn 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431b13ad-8a7a-4474-987f-053aa008ca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gd = best_dunn.sort_values('dunn', ascending=False).drop(columns=['silhouette', 'calinski_harabasz', 'davies_bouldin']).reset_index(drop=True)\n",
    "gd.to_csv(\"output/models/best_gd.csv\", index=False)\n",
    "gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4054c4c0-3c08-4ef8-9112-b795c7f3d511",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_clust_gd = refit_best_model(gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3765e69e-2b8a-4c67-b910-1430eea1f2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(\n",
    "    data_f_lb if (gd.loc[0, 'model'] == 'latent') and (gd.loc[0, 'params'].get('msrt') == 'categorical') else data_n, \n",
    "    pred_clust_gd,\n",
    "    '2D PCA Projection of the Best Partition According to the Generalized Dunn Index',\n",
    "    'gd_clust')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e4f05e-90c7-4754-99c1-04b014d68891",
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.unique(pred_clust_gd, return_counts=True)[1].min() > 5:\n",
    "    plot_cluster_profiles(data_n, pred_clust_gd, feature_names = var_list, sd = 1, title = 'Generalized Dunn', filename = 'gd_patterns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a66edd-aaaa-41e3-8a52-a40fa8b90a2a",
   "metadata": {},
   "source": [
    "# 4. Clusterability - Hopkins Statistic\n",
    "\n",
    "*Function from the [pyclustertend package](https://pyclustertend.readthedocs.io/en/latest/_modules/pyclustertend/hopkins.html), which could not be installed because its depencies are outdated.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ec0caa-efaa-4671-8c21-6324d00e6ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hopkins_stat = hopkins(data_n.values, data_n.shape[0])\n",
    "print(f\"Hopkins stat on restricted data set: {hopkins_stat:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148fedcb-f35c-45db-91bf-87bf7efbb1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_var_list = var_list + ['clsetown', 'clsestat', 'clsenoam', 'amancstr', 'amsports', 'lessprd']\n",
    "full_var_list_n = [var + \"_n\" for var in full_var_list]\n",
    "data_n_full = data2004_i[full_var_list_n]\n",
    "hopkins_stat = hopkins(data_n_full.values, data_n.shape[0])\n",
    "print(f\"Hopkins stat on full data set: {hopkins_stat:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0aee49-8e3d-43db-ab1e-dbc62753691f",
   "metadata": {},
   "source": [
    "*The inclusion of questions discared by the authors slighly improves clusterability.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94471667-c481-4865-ac88-1d1b3d84f30a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
