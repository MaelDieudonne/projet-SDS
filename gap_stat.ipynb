{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ed18269-9962-41f2-beb3-45893a3ad8f9",
   "metadata": {},
   "source": [
    "Experimental implementation of the gap statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4887e505-68fb-40c0-adb5-06ef3199e312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "from joblib import Parallel, delayed # for parallelization\n",
    "from itertools import product\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Clustering\n",
    "from stepmix.stepmix import StepMix\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import AgglomerativeClustering, HDBSCAN\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.neighbors import BallTree\n",
    "import torch\n",
    "from torchmetrics.clustering import DunnIndex\n",
    "from collections import Counter\n",
    "from kneed import KneeLocator\n",
    "\n",
    "# Visualization\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import ConvexHull"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d1e61c-3574-4482-9529-59268b7fb8d6",
   "metadata": {},
   "source": [
    "# Preparation\n",
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1190dd79-e4b5-4684-98b0-0f6ce7797686",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2004_i = pd.read_parquet(\"data/data2004_i.parquet\") # load imputed data\n",
    "\n",
    "# Dataset with numeric outcomes\n",
    "data_n = data2004_i[[\n",
    "    # Q2\n",
    "    'clseusa_n', # 'clsetown_n', 'clsestat_n', 'clsenoam_n',\n",
    "    # Q3\n",
    "    'ambornin_n', 'amcit_n', 'amlived_n', 'amenglsh_n', \n",
    "    'amchrstn_n', 'amgovt_n', 'amfeel_n', # 'amancstr_n',\n",
    "    # Q4\n",
    "    'amcitizn_n', 'amshamed_n', 'belikeus_n', 'ambetter_n', 'ifwrong_n', # 'amsports_n', 'lessprd_n',\n",
    "    # Q5\n",
    "    'proudsss_n', 'proudgrp_n', 'proudpol_n', 'prouddem_n', 'proudeco_n',\n",
    "    'proudspt_n', 'proudart_n', 'proudhis_n', 'proudmil_n', 'proudsci_n']]\n",
    "\n",
    "# Dataset with categorical outcomes\n",
    "data_f = data2004_i[[\n",
    "    # Q2\n",
    "    'clseusa_f', # 'clsetown_f', 'clsestat_f', 'clsenoam_f',\n",
    "    # Q3\n",
    "    'ambornin_f', 'amcit_f', 'amlived_f', 'amenglsh_f', \n",
    "    'amchrstn_f', 'amgovt_f', 'amfeel_f', # 'amancstr_f',\n",
    "    # Q4\n",
    "    'amcitizn_f', 'amshamed_f', 'belikeus_f', 'ambetter_f', 'ifwrong_f', # 'amsports_f', 'lessprd_f',\n",
    "    # Q5\n",
    "    'proudsss_f', 'proudgrp_f', 'proudpol_f', 'prouddem_f', 'proudeco_f',\n",
    "    'proudspt_f', 'proudart_f', 'proudhis_f', 'proudmil_f', 'proudsci_f']]\n",
    "\n",
    "# Dataset with controls\n",
    "controls = data2004_i[[\n",
    "    'sex', 'race_f', 'born_usa', 'party_fs', 'religstr_f', \n",
    "    'reltrad_f', 'region_f']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0b97bf-43e0-4516-b3ee-f226e4ebd772",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55b24ac-f239-4dea-9283-e3491ec65ee5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_clust = 12\n",
    "max_threads = 8\n",
    "\n",
    "val_indexes = ['silhouette', 'calinski_harabasz', 'davies_bouldin', 'dunn', 'inertia']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d170e0b-2c98-40ec-bda1-12743bfd5b9e",
   "metadata": {},
   "source": [
    "## Validity indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c84df2-d928-437c-b4f8-200cc8b0d208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom score functions to avoid throwing errors when undefined\n",
    "def sil_score(data, pred_clust):\n",
    "    try:\n",
    "        sil_score = silhouette_score(data, pred_clust)\n",
    "    except ValueError:\n",
    "        sil_score = np.nan\n",
    "    return sil_score\n",
    "\n",
    "def ch_score(data, pred_clust):\n",
    "    try:\n",
    "        ch_score = calinski_harabasz_score(data, pred_clust)\n",
    "    except ValueError:\n",
    "        ch_score = np.nan\n",
    "    return ch_score\n",
    "\n",
    "def db_score(data, pred_clust):\n",
    "    try:\n",
    "        db_score = davies_bouldin_score(data, pred_clust)\n",
    "    except ValueError:\n",
    "        db_score = np.nan\n",
    "    return db_score\n",
    "\n",
    "def dunn_score(data, pred_clust):\n",
    "    torch_data = np.array(data)\n",
    "    torch_data = torch.tensor(torch_data, dtype=torch.float32)\n",
    "    torch_pred_clust = torch.tensor(pred_clust, dtype=torch.int64)\n",
    "\n",
    "    dunn_metric = DunnIndex()\n",
    "    \n",
    "    try:\n",
    "        dunn_score = float(dunn_metric(torch_data, torch_pred_clust))\n",
    "    except Exception:\n",
    "        dunn_score = np.nan\n",
    " \n",
    "    return dunn_score\n",
    "\n",
    "def inertia(data, pred_clust):\n",
    "    data = np.asarray(data)\n",
    "    \n",
    "    inertia = 0\n",
    "    for cluster in np.unique(pred_clust):\n",
    "        cluster_points = data[pred_clust == cluster]\n",
    "        cluster_centroid = np.mean(cluster_points, axis=0)\n",
    "        inertia += np.sum((cluster_points - cluster_centroid) ** 2)\n",
    "        \n",
    "    return inertia\n",
    "\n",
    "def clust_size(pred_clust):\n",
    "    cluster_sizes = Counter(pred_clust)\n",
    "    min_size = min(cluster_sizes.values())\n",
    "    max_size = max(cluster_sizes.values())\n",
    "    \n",
    "    return min_size, max_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf65c0e6-238a-4879-87e3-074f85d8333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return all validity indexes at once\n",
    "def get_metrics(model, params, n, data, pred_clust, **additional_metrics):\n",
    "    base_metrics = {\n",
    "        'model': model,\n",
    "        'params': params,\n",
    "        'n_clust': n,\n",
    "        'min_clust_size': clust_size(pred_clust)[0],\n",
    "        'max_clust_size': clust_size(pred_clust)[1],\n",
    "        'silhouette': sil_score(data, pred_clust),\n",
    "        'calinski_harabasz': ch_score(data, pred_clust),\n",
    "        'davies_bouldin': db_score(data, pred_clust),\n",
    "        'dunn': dunn_score(data, pred_clust),\n",
    "        'inertia': inertia(data, pred_clust)\n",
    "    }\n",
    "\n",
    "    base_metrics.update(additional_metrics)\n",
    "    return base_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1513b5bc-963b-41e5-a6bd-12f45f22c2f5",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78aabed-4cb1-4e87-a35f-797240d34b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "clust_range = range(1, max_clust+1)\n",
    "\n",
    "opt_params = {\n",
    "    'method': 'gradient',\n",
    "    'intercept': True,\n",
    "    'max_iter': 2500,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa743e87-3ef4-4e07-9610-59a15540bca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit models without covariates\n",
    "def do_StepMix(n, type, data):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "\n",
    "        latent_mod = StepMix(\n",
    "            n_components = n, \n",
    "            measurement = type, \n",
    "            n_init = 3,\n",
    "            init_params = 'kmeans',\n",
    "            structural_params = opt_params,\n",
    "            random_state = 123,\n",
    "            progress_bar = 0)\n",
    "        \n",
    "        latent_mod.fit(data)\n",
    "        pred_clust = latent_mod.predict(data)\n",
    "\n",
    "        model = 'LCA' if type == 'categorical' else 'LPA'\n",
    "        params = 'without covariates'\n",
    "        loglik = latent_mod.score(data)\n",
    "        aic = latent_mod.aic(data)\n",
    "        bic = latent_mod.aic(data)\n",
    "        entropy = latent_mod.entropy(data)\n",
    "\n",
    "    return get_metrics(model, params, n, data, pred_clust, LL = loglik, aic = aic, bic = bic, entropy = entropy)\n",
    "\n",
    "data = data_f.apply(lambda col: LabelEncoder().fit_transform(col))\n",
    "cat_results = Parallel(n_jobs=8)(delayed(do_StepMix)(n, 'categorical', data) for n in clust_range)\n",
    "LCA_all = pd.DataFrame(cat_results)\n",
    "\n",
    "num_results = Parallel(n_jobs=8)(delayed(do_StepMix)(n, 'continuous', data_n) for n in clust_range)\n",
    "LPA_all = pd.DataFrame(num_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9748fe56-0c8f-4493-be53-969f3b3ccf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlexibleKMeans:\n",
    "    \"\"\"\n",
    "    K-Means implementation supporting different distance metrics and center computation methods.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_clusters : int\n",
    "        Number of clusters\n",
    "    metric : str, default='euclidean'\n",
    "        Distance metric: 'euclidean', 'manhattan', 'chebyshev'\n",
    "    center_method : str, default='mean'\n",
    "        Method to compute cluster centers: 'mean', 'median', 'medoid'\n",
    "    max_iter : int, default=100\n",
    "        Maximum number of iterations\n",
    "    n_init : int, default=10\n",
    "        Number of times the k-means algorithm will be run with different centroid seeds.\n",
    "        The final result will be the best output of n_init consecutive runs in terms of inertia.\n",
    "    random_state : int or None, default=None\n",
    "        Random state for reproducibility\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_clusters, metric='euclidean', center_method='mean', \n",
    "                 max_iter=100, n_init=10, random_state=None):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.metric = metric\n",
    "        self.center_method = center_method\n",
    "        self.max_iter = max_iter\n",
    "        self.n_init = n_init\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Define mapping from user-friendly names to scipy metrics\n",
    "        self.metric_mapping = {\n",
    "            'euclidean': 'euclidean',\n",
    "            'manhattan': 'cityblock',\n",
    "            'chebyshev': 'chebyshev'\n",
    "        }\n",
    "        \n",
    "        # Validate inputs\n",
    "        valid_metrics = list(self.metric_mapping.keys())\n",
    "        if metric not in valid_metrics:\n",
    "            raise ValueError(f\"metric must be one of {valid_metrics}\")\n",
    "            \n",
    "        valid_centers = ['mean', 'median', 'medoid']\n",
    "        if center_method not in valid_centers:\n",
    "            raise ValueError(f\"center_method must be one of {valid_centers}\")\n",
    "            \n",
    "        if self.n_init <= 0:\n",
    "            raise ValueError(\"n_init should be > 0\")\n",
    "    \n",
    "    def _compute_distances(self, X, centers):\n",
    "        \"\"\"Compute distances between points and centers using specified metric.\"\"\"\n",
    "        return cdist(X, centers, metric=self.metric_mapping[self.metric])\n",
    "    \n",
    "    def _compute_centers(self, X, labels):\n",
    "        \"\"\"Compute new centers using specified method.\"\"\"\n",
    "        new_centers = np.zeros((self.n_clusters, X.shape[1]))\n",
    "        \n",
    "        for i in range(self.n_clusters):\n",
    "            cluster_points = X[labels == i]\n",
    "            \n",
    "            if len(cluster_points) == 0:\n",
    "                continue\n",
    "                \n",
    "            if self.center_method == 'mean':\n",
    "                new_centers[i] = np.mean(cluster_points, axis=0)\n",
    "            \n",
    "            elif self.center_method == 'median':\n",
    "                new_centers[i] = np.median(cluster_points, axis=0)\n",
    "            \n",
    "            elif self.center_method == 'medoid':\n",
    "                # For medoid, find the point that minimizes sum of distances to other points\n",
    "                distances = self._compute_distances(cluster_points, cluster_points)\n",
    "                medoid_idx = np.argmin(np.sum(distances, axis=1))\n",
    "                new_centers[i] = cluster_points[medoid_idx]\n",
    "        \n",
    "        return new_centers\n",
    "    \n",
    "    def _single_fit(self, X, seed):\n",
    "        \"\"\"Perform a single run of k-means with given random seed.\"\"\"\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "            \n",
    "        # Initialize centers randomly\n",
    "        idx = np.random.choice(len(X), self.n_clusters, replace=False)\n",
    "        centers = X[idx].copy()\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            # Store old centers for convergence check\n",
    "            old_centers = centers.copy()\n",
    "            \n",
    "            # Assign points to nearest center\n",
    "            distances = self._compute_distances(X, centers)\n",
    "            labels = np.argmin(distances, axis=1)\n",
    "            \n",
    "            # Update centers\n",
    "            centers = self._compute_centers(X, labels)\n",
    "            \n",
    "            # Check for convergence\n",
    "            if np.allclose(old_centers, centers):\n",
    "                n_iter = iteration + 1\n",
    "                break\n",
    "        else:\n",
    "            n_iter = self.max_iter\n",
    "            \n",
    "        # Compute final inertia\n",
    "        final_distances = self._compute_distances(X, centers)\n",
    "        inertia = np.sum(np.min(final_distances, axis=1) ** 2)\n",
    "        \n",
    "        return centers, labels, inertia, n_iter\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"Fit the model to the data.\"\"\"\n",
    "        # Convert pandas DataFrame to numpy array if necessary\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "        X = np.asarray(X)\n",
    "        \n",
    "        # Initialize best solution tracking\n",
    "        best_inertia = np.inf\n",
    "        best_labels = None\n",
    "        best_centers = None\n",
    "        best_n_iter = None\n",
    "        \n",
    "        # Run k-means n_init times\n",
    "        for init in range(self.n_init):\n",
    "            # Generate seed for this initialization\n",
    "            if self.random_state is not None:\n",
    "                seed = self.random_state + init\n",
    "            else:\n",
    "                seed = None\n",
    "                \n",
    "            # Perform single k-means run\n",
    "            centers, labels, inertia, n_iter = self._single_fit(X, seed)\n",
    "            \n",
    "            # Update best solution if current one is better\n",
    "            if inertia < best_inertia:\n",
    "                best_centers = centers\n",
    "                best_labels = labels\n",
    "                best_inertia = inertia\n",
    "                best_n_iter = n_iter\n",
    "        \n",
    "        # Store best solution\n",
    "        self.cluster_centers_ = best_centers\n",
    "        self.labels_ = best_labels\n",
    "        self.inertia_ = best_inertia\n",
    "        self.n_iter_ = best_n_iter\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def fit_predict(self, X):\n",
    "        \"\"\"Fit the model and return cluster labels.\"\"\"\n",
    "        return self.fit(X).labels_\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict the closest cluster for each sample in X.\"\"\"\n",
    "        # Convert pandas DataFrame to numpy array if necessary\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "        X = np.asarray(X)\n",
    "        \n",
    "        distances = self._compute_distances(X, self.cluster_centers_)\n",
    "        return np.argmin(distances, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c7be18-a915-4bc1-ae86-ae1f39cc1084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_kmeans(data, n, dist, link):\n",
    "    kmeans = FlexibleKMeans(\n",
    "        n_clusters = n,\n",
    "        metric = dist,\n",
    "        center_method = link,\n",
    "        n_init = 15)\n",
    "\n",
    "    pred_clust = kmeans.fit_predict(data)\n",
    "    \n",
    "    model = 'kmeans'\n",
    "    params = f\"dist = {dist}, link = {link}\"\n",
    "    \n",
    "    return get_metrics(model, params, n, data, pred_clust)\n",
    "\n",
    "clust_range = range(1, max_clust+1)\n",
    "distances = ['euclidean', 'manhattan', 'chebyshev']\n",
    "linkages = ['mean', 'median', 'medoid']\n",
    "params = product(clust_range, distances, linkages)\n",
    "\n",
    "results = Parallel(n_jobs=max_threads)(delayed(do_kmeans)(data_n, n, dist, link) for n, dist, link in params)\n",
    "kmeans_all = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5915d91a-a1dd-4254-a7cb-3703fedb45a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_AHC(data, n, dist, link):\n",
    "    ahc = AgglomerativeClustering(\n",
    "        n_clusters = n,\n",
    "        metric = dist,\n",
    "        linkage = link)\n",
    "    \n",
    "    ahc.fit(data)\n",
    "    pred_clust = ahc.labels_\n",
    "\n",
    "    model = 'AHC'\n",
    "    params = f\"dist = {dist}, link = {link}\"\n",
    "\n",
    "    return get_metrics(model, params, n, data, pred_clust)\n",
    "\n",
    "clust_range = range(1, max_clust+1)\n",
    "distances = ['manhattan', 'euclidean', 'chebyshev', 'hamming']\n",
    "linkages = ['single', 'average', 'complete']\n",
    "params = product(clust_range, distances, linkages)\n",
    "\n",
    "results = Parallel(n_jobs=max_threads)(delayed(do_AHC)(data_n, n, dist, link) for n, dist, link in params)\n",
    "results.extend([do_AHC(data_n, n, 'euclidean', 'ward') for n in clust_range])\n",
    "ahc_all = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce063cf-1433-4461-851a-646030af0069",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = pd.concat([LPA_all, kmeans_all, ahc_all])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786c50d9-373a-413b-85c7-e62abf303d95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d8e1dc-5d1b-415a-b51f-a7b75be5ca46",
   "metadata": {},
   "source": [
    "# Gap stat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c478565-f20e-412d-b776-b880d8261cfa",
   "metadata": {},
   "source": [
    "Adapted from: https://www.geeksforgeeks.org/gap-statistics-for-optimal-number-of-cluster/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd706014-394e-46fb-bec1-4d42168e536f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate reference data from a uniform distribution\n",
    "def gen_ref_data(data):\n",
    "    return np.random.uniform(low=data.min(axis=0), \n",
    "                            high=data.max(axis=0), \n",
    "                            size=data.shape)\n",
    "\n",
    "# Create empty df to store results\n",
    "def create_empty_df(indices):\n",
    "    cols = ['model', 'params', 'n_clust'] + \\\n",
    "       [f'{idx}_gs' for idx in val_indexes] + \\\n",
    "       [f'{idx}_s' for idx in val_indexes]\n",
    "    float_cols = [col for col in cols if col not in ['model', 'params', 'n_clust']]\n",
    "\n",
    "    df = pd.DataFrame(columns=cols)\n",
    "\n",
    "    df[float_cols] = df[float_cols].astype('float64')\n",
    "    df['model'] = df['model'].astype('object')\n",
    "    df['params'] = df['params'].astype('object')\n",
    "    df['n_clust'] = df['n_clust'].astype('int64')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Retrieve parameters from the models list\n",
    "def extract_params(str):\n",
    "    pairs = [pair.strip() for pair in str.split(',')]\n",
    "    params = {}\n",
    "    for pair in pairs:\n",
    "        key, value = [x.strip() for x in pair.split('=')]\n",
    "        params[key] = value  \n",
    "    return params\n",
    "\n",
    "# Compute the Gap Statistic\n",
    "def compute_gap_statistic(data, n_min, n_max, iters, model, params):\n",
    "    gap_values = create_empty_df(val_indexes)\n",
    "    \n",
    "    # Loop over n values\n",
    "    for n in range(n_min, n_max + 1):\n",
    "    \n",
    "        # Fit the model on random datasets\n",
    "        rand_scores_all = pd.DataFrame()\n",
    "        \n",
    "        for _ in range(iters):\n",
    "            rand_data = gen_ref_data(data)\n",
    "            \n",
    "            if model == 'LPA' and params == 'without covariates':\n",
    "                rand_scores = do_StepMix(n, 'continuous', rand_data)\n",
    "\n",
    "            elif model == 'kmeans':\n",
    "                config = extract_params(params)\n",
    "                rand_scores = do_kmeans(rand_data, n, **config)\n",
    "\n",
    "            elif model == 'AHC':\n",
    "                config = extract_params(params)\n",
    "                rand_scores = do_AHC(rand_data, n, **config)\n",
    "            \n",
    "            rand_scores = pd.DataFrame([rand_scores])\n",
    "            rand_scores_all = pd.concat([rand_scores_all, rand_scores], ignore_index=True)\n",
    "\n",
    "        # Retrive scores for the assessed model\n",
    "        mod_scores = all_models.loc[(all_models['model'] == model) & \n",
    "                                     (all_models['params'] == params) & \n",
    "                                     (all_models['n_clust'] == n)]\n",
    "\n",
    "        # Calculate the Gap statistic and s value\n",
    "        for index in val_indexes:\n",
    "            rand_ind = rand_scores_all[index]\n",
    "            mod_ind = mod_scores[index]\n",
    "            \n",
    "            gap = np.log(np.mean(rand_ind)) - np.log(mod_ind)\n",
    "            s = np.std(np.log(rand_ind)) * np.sqrt(1 + (1 / iters))\n",
    "\n",
    "            # Define the condition to check if the row exists\n",
    "            row_id = ((gap_values['model'] == model) & \n",
    "                      (gap_values['params'] == params) & \n",
    "                      (gap_values['n_clust'] == n))\n",
    "\n",
    "            # Check if there is any row that matches the condition\n",
    "            if gap_values[row_id].empty:\n",
    "            # If the row does not exist, append a new one\n",
    "                new_row = {\n",
    "                    'model': model,\n",
    "                    'params': params,\n",
    "                    'n_clust': n,\n",
    "                    f'{index}_gs': gap.values[0],\n",
    "                    f'{index}_s': s\n",
    "                }\n",
    "                new_row = pd.DataFrame([new_row])\n",
    "                \n",
    "                gap_values = pd.concat([gap_values, new_row], ignore_index=True)\n",
    "            else:\n",
    "            # If the row exists, update it\n",
    "                gap_values.loc[row_id, f'{index}_gs'] = gap.values[0]\n",
    "                gap_values.loc[row_id, f'{index}_s'] = s\n",
    "    \n",
    "    return gap_values\n",
    "\n",
    "# Select the optimal number of clusters\n",
    "def get_best_gap(df, n_min, n_max):\n",
    "    stats = []\n",
    "    for i in range(0, n_max):\n",
    "        stat = df[i][1] - df[i-1][1] + df[i][2]\n",
    "        # Select rows such that GS(k) >= GS(k+1) + s(k+1)\n",
    "        if stat >= 0: \n",
    "            stats.append([i+1, stat])\n",
    "\n",
    "    # Find min value\n",
    "    stats = np.array(stats)\n",
    "    if stats.size == 0:\n",
    "        best_n = 'none'\n",
    "    else:\n",
    "        best_n = int(stats[np.argmin(stats[:, 1]), 0])\n",
    "\n",
    "    return best_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2531313e-b729-48d6-9528-7c3ae22dcb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_min = 1\n",
    "n_max = 4\n",
    "iters = 2\n",
    "model = 'LPA'\n",
    "params = 'without covariates'\n",
    "\n",
    "gs = compute_gap_statistic(data_n, n_min, n_max, iters, model, params)\n",
    "gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18157b36-3a4d-4182-9709-a56b66f4d788",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_min = 1\n",
    "n_max = 8\n",
    "iters = 5\n",
    "model = 'kmeans'\n",
    "params = 'dist = manhattan, link = mean'\n",
    "\n",
    "gs = compute_gap_statistic(data_n, n_min, n_max, iters, model, params)\n",
    "gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d20eb39-53e4-48dd-b06b-ef4651c3c0bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3337c752-a230-407e-a697-be3302458166",
   "metadata": {},
   "source": [
    "# Legacy 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde74a0f-f326-4cdc-9280-fbee5b6d1432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate reference data from a uniform distribution\n",
    "def gen_ref_data(data):\n",
    "    return np.random.uniform(low=data.min(axis=0), \n",
    "                            high=data.max(axis=0), \n",
    "                            size=data.shape)\n",
    "\n",
    "# To retrieve parameters from the models list\n",
    "def extract_params(str):\n",
    "    pairs = [pair.strip() for pair in str.split(',')]\n",
    "    params = {}\n",
    "    for pair in pairs:\n",
    "        key, value = [x.strip() for x in pair.split('=')]\n",
    "        params[key] = value  \n",
    "    return params\n",
    "\n",
    "# Compute the Gap Statistic\n",
    "def compute_gap_statistic(data, n_min, n_max, n_replicates, model, params):\n",
    "    gap_values = []\n",
    "    \n",
    "    # Loop over n values\n",
    "    for n in range(n_min, n_max + 1):\n",
    "    \n",
    "        # Compute the average inertia for the random datasets\n",
    "        sim_inertia = []\n",
    "        \n",
    "        for _ in range(n_replicates):\n",
    "            rand_data = gen_ref_data(data)\n",
    "            \n",
    "            if model == 'LPA' and params == 'without covariates':\n",
    "                rand_inertia = float(do_StepMix(n, 'continuous', rand_data)['inertia'])\n",
    "            \n",
    "            if model == 'kmeans':\n",
    "                config = extract_params(params)\n",
    "                rand_inertia = float(do_kmeans(rand_data, n, **config)['inertia'])\n",
    "                \n",
    "            if model == 'AHC':\n",
    "                config = extract_params(params)\n",
    "                rand_inertia = float(do_AHC(rand_data, n, **config)['inertia'])\n",
    "            \n",
    "            sim_inertia.append(rand_inertia)\n",
    "\n",
    "        # Retrive inertia for the assessed model\n",
    "        mod_inertia = all_models.loc[(all_models['model'] == model) & \n",
    "                                     (all_models['params'] == params) & \n",
    "                                     (all_models['n_clust'] == n), 'inertia'].values\n",
    "\n",
    "        # Calculate the Gap statistic and s value\n",
    "        gap = np.log(np.mean(sim_inertia)) - np.log(mod_inertia)\n",
    "        s = np.std(np.log(sim_inertia)) * np.sqrt(1 + (1 / n_replicates))\n",
    "        gap_values.append([n, gap.item(), s.item()])\n",
    "\n",
    "    return gap_values\n",
    "\n",
    "# Select the optimal number of clusters\n",
    "def get_best_gap(df):\n",
    "    stats = []\n",
    "    for i in range(0,4):\n",
    "        stat = df[i][1] - df[i-1][1] + df[i][2]\n",
    "        # Select rows such that GS(k) >= GS(k+1) + s(k+1)\n",
    "        if stat >= 0: \n",
    "            stats.append([i+1, stat])\n",
    "\n",
    "    # Find min value\n",
    "    stats = np.array(stats)\n",
    "    best_n = int(stats[np.argmin(stats[:, 1]), 0])\n",
    "\n",
    "    return best_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45e72d0-303f-42ad-b2ad-24bf5d8ecf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_min = 1\n",
    "n_max = 8\n",
    "n_replicates = 10\n",
    "model = 'LPA'\n",
    "params = 'without covariates'\n",
    "\n",
    "gs = compute_gap_statistic(data_n, n_min, n_max, n_replicates, model, params)\n",
    "get_best_gap(gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ce1274-58b0-4ed8-b70a-1accdd49404e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9790509-0ccd-4f25-876d-02de7652c034",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_min = 1\n",
    "n_max = 12\n",
    "n_replicates = 10\n",
    "model = 'kmeans'\n",
    "params = 'dist = manhattan, link = mean'\n",
    "\n",
    "gs = compute_gap_statistic(data_n, n_min, n_max, n_replicates, model, params)\n",
    "get_best_gap(gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a31d2d-6f34-48a0-b588-abbf55069456",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_min = 1\n",
    "n_max = 12\n",
    "n_replicates = 10\n",
    "model = 'AHC'\n",
    "params = 'dist = euclidean, link = average'\n",
    "\n",
    "gs = compute_gap_statistic(data_n, n_min, n_max, n_replicates, model, params)\n",
    "get_best_gap(gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3927d767-6ac3-4cf1-9aea-b18b4e1efe37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d900af75-f47a-43be-99b9-c0f44f788ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4c605c9-c2ab-4043-9931-58d907643c48",
   "metadata": {},
   "source": [
    "# Legacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3d0985-5e84-46c9-b12c-9c660272f2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the Gap Statistic\n",
    "def compute_gap_statistic(data, k_min, k_max, n_replicates, model, params):\n",
    "    \n",
    "    # Generate reference data from a uniform distribution\n",
    "    def gen_ref_data(data):\n",
    "        return np.random.uniform(low=data.min(axis=0), \n",
    "                                 high=data.max(axis=0), \n",
    "                                 size=data.shape)\n",
    "\n",
    "    gap_values = []\n",
    "    \n",
    "    # Loop over k values\n",
    "    for k in range(k_min, k_max + 1):\n",
    "        # Retrive inertia for the assessed model\n",
    "        mod_inertia = all_models.loc[(all_models['model'] == model) & (LCA_all['params'] == params) & (LCA_all['n_clust'] == k), 'inertia'].values\n",
    "        \n",
    "        # Compute the average inertia for the reference datasets\n",
    "        reference_inertia = []\n",
    "        for _ in range(n_replicates):\n",
    "            random_data = gen_ref_data(data)\n",
    "            ref_inertia = float(do_StepMix(k, 'continuous', data_n)['inertia'])\n",
    "            reference_inertia.append(ref_inertia)\n",
    "\n",
    "        # Calculate the Gap statistic and se\n",
    "        gap = np.log(np.mean(ref_inertia)) - np.log(mod_inertia)\n",
    "        s = np.std(np.log(sim_inertia)) * np.sqrt(1 + (1 / n_replicates))\n",
    "        gap_values.append([k, float(gap[0]), float(s)])\n",
    "\n",
    "    return gap_values\n",
    "\n",
    "gap_values = compute_gap_statistic(data_n, 1, 8, n_replicates=10, model='LCA', params='without covariates')\n",
    "\n",
    "# Compute GS(k) - GS(k+1) + s(k+1)\n",
    "stats = []\n",
    "for i in range(0,4):\n",
    "    stat = gap_values[i][1] - gap_values[i-1][1] + gap_values[i][2]\n",
    "    # Select rows such that GS(k) >= GS(k+1) + s(k+1)\n",
    "    if stat >= 0: \n",
    "        stats.append([i+1, stat])\n",
    "\n",
    "# Find min value\n",
    "stats = np.array(stats)\n",
    "best_k = int(stats[np.argmin(stats[:, 1]), 0])\n",
    "best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7c465f-b844-4758-bc03-dd396adf03f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
