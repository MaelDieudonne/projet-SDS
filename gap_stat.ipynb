{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ed18269-9962-41f2-beb3-45893a3ad8f9",
   "metadata": {},
   "source": [
    "Experimental implementation of the gap statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4887e505-68fb-40c0-adb5-06ef3199e312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "from joblib import Parallel, delayed # for parallelization\n",
    "from itertools import product\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Clustering\n",
    "from stepmix.stepmix import StepMix\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import AgglomerativeClustering, HDBSCAN\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.neighbors import BallTree\n",
    "import torch\n",
    "from torchmetrics.clustering import DunnIndex\n",
    "from collections import Counter\n",
    "from kneed import KneeLocator\n",
    "\n",
    "# Visualization\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import ConvexHull"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d1e61c-3574-4482-9529-59268b7fb8d6",
   "metadata": {},
   "source": [
    "# Preparation\n",
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1190dd79-e4b5-4684-98b0-0f6ce7797686",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2004_i = pd.read_parquet(\"data/data2004_i.parquet\") # load imputed data\n",
    "\n",
    "# Dataset with numeric outcomes\n",
    "data_n = data2004_i[[\n",
    "    # Q2\n",
    "    'clseusa_n', # 'clsetown_n', 'clsestat_n', 'clsenoam_n',\n",
    "    # Q3\n",
    "    'ambornin_n', 'amcit_n', 'amlived_n', 'amenglsh_n', \n",
    "    'amchrstn_n', 'amgovt_n', 'amfeel_n', # 'amancstr_n',\n",
    "    # Q4\n",
    "    'amcitizn_n', 'amshamed_n', 'belikeus_n', 'ambetter_n', 'ifwrong_n', # 'amsports_n', 'lessprd_n',\n",
    "    # Q5\n",
    "    'proudsss_n', 'proudgrp_n', 'proudpol_n', 'prouddem_n', 'proudeco_n',\n",
    "    'proudspt_n', 'proudart_n', 'proudhis_n', 'proudmil_n', 'proudsci_n']]\n",
    "\n",
    "# Dataset with categorical outcomes\n",
    "data_f = data2004_i[[\n",
    "    # Q2\n",
    "    'clseusa_f', # 'clsetown_f', 'clsestat_f', 'clsenoam_f',\n",
    "    # Q3\n",
    "    'ambornin_f', 'amcit_f', 'amlived_f', 'amenglsh_f', \n",
    "    'amchrstn_f', 'amgovt_f', 'amfeel_f', # 'amancstr_f',\n",
    "    # Q4\n",
    "    'amcitizn_f', 'amshamed_f', 'belikeus_f', 'ambetter_f', 'ifwrong_f', # 'amsports_f', 'lessprd_f',\n",
    "    # Q5\n",
    "    'proudsss_f', 'proudgrp_f', 'proudpol_f', 'prouddem_f', 'proudeco_f',\n",
    "    'proudspt_f', 'proudart_f', 'proudhis_f', 'proudmil_f', 'proudsci_f']]\n",
    "\n",
    "# Dataset with controls\n",
    "controls = data2004_i[[\n",
    "    'sex', 'race_f', 'born_usa', 'party_fs', 'religstr_f', \n",
    "    'reltrad_f', 'region_f']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0b97bf-43e0-4516-b3ee-f226e4ebd772",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55b24ac-f239-4dea-9283-e3491ec65ee5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_clust = 12\n",
    "max_threads = 8\n",
    "\n",
    "CVI = ['silhouette', 'calinski_harabasz', 'davies_bouldin', 'dunn', 'inertia']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d170e0b-2c98-40ec-bda1-12743bfd5b9e",
   "metadata": {},
   "source": [
    "## Validity indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c84df2-d928-437c-b4f8-200cc8b0d208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom score functions to avoid throwing errors when undefined\n",
    "def sil_score(data, pred_clust):\n",
    "    try: \n",
    "        sil_score = silhouette_score(data, pred_clust)\n",
    "    except ValueError:\n",
    "        sil_score = np.nan\n",
    "    return sil_score\n",
    "\n",
    "def ch_score(data, pred_clust):\n",
    "    try:\n",
    "        ch_score = calinski_harabasz_score(data, pred_clust)\n",
    "    except ValueError:\n",
    "        ch_score = np.nan\n",
    "    return ch_score\n",
    "\n",
    "def db_score(data, pred_clust):\n",
    "    try:\n",
    "        db_score = davies_bouldin_score(data, pred_clust)\n",
    "    except ValueError:\n",
    "        db_score = np.nan\n",
    "    return db_score\n",
    "\n",
    "def dunn_score(data, pred_clust):\n",
    "    torch_data = np.array(data)\n",
    "    torch_data = torch.tensor(torch_data, dtype=torch.float32)\n",
    "    torch_pred_clust = torch.tensor(pred_clust, dtype=torch.int64)\n",
    "\n",
    "    dunn_metric = DunnIndex()\n",
    "    \n",
    "    try:\n",
    "        dunn_score = float(dunn_metric(torch_data, torch_pred_clust))\n",
    "    except Exception:\n",
    "        dunn_score = np.nan\n",
    " \n",
    "    return dunn_score\n",
    "\n",
    "def inertia(data, pred_clust):\n",
    "    data = np.asarray(data)\n",
    "    \n",
    "    inertia = 0\n",
    "    for cluster in np.unique(pred_clust):\n",
    "        cluster_points = data[pred_clust == cluster]\n",
    "        cluster_centroid = np.mean(cluster_points, axis=0)\n",
    "        inertia += np.sum((cluster_points - cluster_centroid) ** 2)\n",
    "        \n",
    "    return inertia\n",
    "\n",
    "def clust_size(pred_clust):\n",
    "    cluster_sizes = Counter(pred_clust)\n",
    "    min_size = min(cluster_sizes.values())\n",
    "    max_size = max(cluster_sizes.values())\n",
    "    \n",
    "    return min_size, max_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf65c0e6-238a-4879-87e3-074f85d8333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return all validity indexes at once\n",
    "def get_metrics(model, params, n, data, pred_clust, **additional_metrics):\n",
    "    base_metrics = {\n",
    "        'model': model,\n",
    "        'params': params,\n",
    "        'n_clust': n,\n",
    "        'min_clust_size': clust_size(pred_clust)[0],\n",
    "        'max_clust_size': clust_size(pred_clust)[1],\n",
    "        'silhouette': float(sil_score(data, pred_clust)),\n",
    "        'calinski_harabasz': float(ch_score(data, pred_clust)),\n",
    "        'davies_bouldin': float(db_score(data, pred_clust)),\n",
    "        'dunn': float(dunn_score(data, pred_clust)),\n",
    "        'inertia': float(inertia(data, pred_clust))\n",
    "    }\n",
    "\n",
    "    base_metrics.update(additional_metrics)\n",
    "    return base_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1513b5bc-963b-41e5-a6bd-12f45f22c2f5",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78aabed-4cb1-4e87-a35f-797240d34b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "clust_range = range(1, max_clust+1)\n",
    "\n",
    "opt_params = {\n",
    "    'method': 'gradient',\n",
    "    'intercept': True,\n",
    "    'max_iter': 2500,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa743e87-3ef4-4e07-9610-59a15540bca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit models without covariates\n",
    "def do_StepMix(data, n, msrt, covar):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "\n",
    "        latent_mod = StepMix(\n",
    "            n_components = n, \n",
    "            measurement = msrt, \n",
    "            n_init = 3,\n",
    "            init_params = 'kmeans',\n",
    "            structural_params = opt_params,\n",
    "            random_state = 123,\n",
    "            progress_bar = 0)\n",
    "        \n",
    "        latent_mod.fit(data)\n",
    "        pred_clust = latent_mod.predict(data)\n",
    "\n",
    "        model = 'latent'\n",
    "        params = f\"msrt = {msrt}, covar = {covar}\"\n",
    "        loglik = latent_mod.score(data)\n",
    "        aic = latent_mod.aic(data)\n",
    "        bic = latent_mod.aic(data)\n",
    "        entropy = latent_mod.entropy(data)\n",
    "\n",
    "    return get_metrics(model, params, n, data, pred_clust, LL = loglik, aic = aic, bic = bic, entropy = entropy)\n",
    "\n",
    "data = data_f.apply(lambda col: LabelEncoder().fit_transform(col))\n",
    "cat_results = Parallel(n_jobs=8)(delayed(do_StepMix)(data, n, 'categorical', 'without') for n in clust_range)\n",
    "\n",
    "num_results = Parallel(n_jobs=8)(delayed(do_StepMix)(data_n, n, 'continuous', 'without') for n in clust_range)\n",
    "\n",
    "latent_all = pd.concat([pd.DataFrame(cat_results), pd.DataFrame(num_results)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9748fe56-0c8f-4493-be53-969f3b3ccf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlexibleKMeans:\n",
    "    \"\"\"\n",
    "    K-Means implementation supporting different distance metrics and center computation methods.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_clusters : int\n",
    "        Number of clusters\n",
    "    metric : str, default='euclidean'\n",
    "        Distance metric: 'euclidean', 'manhattan', 'chebyshev'\n",
    "    center_method : str, default='mean'\n",
    "        Method to compute cluster centers: 'mean', 'median', 'medoid'\n",
    "    max_iter : int, default=100\n",
    "        Maximum number of iterations\n",
    "    n_init : int, default=10\n",
    "        Number of times the k-means algorithm will be run with different centroid seeds.\n",
    "        The final result will be the best output of n_init consecutive runs in terms of inertia.\n",
    "    random_state : int or None, default=None\n",
    "        Random state for reproducibility\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_clusters, metric='euclidean', center_method='mean', \n",
    "                 max_iter=100, n_init=10, random_state=None):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.metric = metric\n",
    "        self.center_method = center_method\n",
    "        self.max_iter = max_iter\n",
    "        self.n_init = n_init\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Define mapping from user-friendly names to scipy metrics\n",
    "        self.metric_mapping = {\n",
    "            'euclidean': 'euclidean',\n",
    "            'manhattan': 'cityblock',\n",
    "            'chebyshev': 'chebyshev'\n",
    "        }\n",
    "        \n",
    "        # Validate inputs\n",
    "        valid_metrics = list(self.metric_mapping.keys())\n",
    "        if metric not in valid_metrics:\n",
    "            raise ValueError(f\"metric must be one of {valid_metrics}\")\n",
    "            \n",
    "        valid_centers = ['mean', 'median', 'medoid']\n",
    "        if center_method not in valid_centers:\n",
    "            raise ValueError(f\"center_method must be one of {valid_centers}\")\n",
    "            \n",
    "        if self.n_init <= 0:\n",
    "            raise ValueError(\"n_init should be > 0\")\n",
    "    \n",
    "    def _compute_distances(self, X, centers):\n",
    "        \"\"\"Compute distances between points and centers using specified metric.\"\"\"\n",
    "        return cdist(X, centers, metric=self.metric_mapping[self.metric])\n",
    "    \n",
    "    def _compute_centers(self, X, labels):\n",
    "        \"\"\"Compute new centers using specified method.\"\"\"\n",
    "        new_centers = np.zeros((self.n_clusters, X.shape[1]))\n",
    "        \n",
    "        for i in range(self.n_clusters):\n",
    "            cluster_points = X[labels == i]\n",
    "            \n",
    "            if len(cluster_points) == 0:\n",
    "                continue\n",
    "                \n",
    "            if self.center_method == 'mean':\n",
    "                new_centers[i] = np.mean(cluster_points, axis=0)\n",
    "            \n",
    "            elif self.center_method == 'median':\n",
    "                new_centers[i] = np.median(cluster_points, axis=0)\n",
    "            \n",
    "            elif self.center_method == 'medoid':\n",
    "                # For medoid, find the point that minimizes sum of distances to other points\n",
    "                distances = self._compute_distances(cluster_points, cluster_points)\n",
    "                medoid_idx = np.argmin(np.sum(distances, axis=1))\n",
    "                new_centers[i] = cluster_points[medoid_idx]\n",
    "        \n",
    "        return new_centers\n",
    "    \n",
    "    def _single_fit(self, X, seed):\n",
    "        \"\"\"Perform a single run of k-means with given random seed.\"\"\"\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "            \n",
    "        # Initialize centers randomly\n",
    "        idx = np.random.choice(len(X), self.n_clusters, replace=False)\n",
    "        centers = X[idx].copy()\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            # Store old centers for convergence check\n",
    "            old_centers = centers.copy()\n",
    "            \n",
    "            # Assign points to nearest center\n",
    "            distances = self._compute_distances(X, centers)\n",
    "            labels = np.argmin(distances, axis=1)\n",
    "            \n",
    "            # Update centers\n",
    "            centers = self._compute_centers(X, labels)\n",
    "            \n",
    "            # Check for convergence\n",
    "            if np.allclose(old_centers, centers):\n",
    "                n_iter = iteration + 1\n",
    "                break\n",
    "        else:\n",
    "            n_iter = self.max_iter\n",
    "            \n",
    "        # Compute final inertia\n",
    "        final_distances = self._compute_distances(X, centers)\n",
    "        inertia = np.sum(np.min(final_distances, axis=1) ** 2)\n",
    "        \n",
    "        return centers, labels, inertia, n_iter\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"Fit the model to the data.\"\"\"\n",
    "        # Convert pandas DataFrame to numpy array if necessary\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "        X = np.asarray(X)\n",
    "        \n",
    "        # Initialize best solution tracking\n",
    "        best_inertia = np.inf\n",
    "        best_labels = None\n",
    "        best_centers = None\n",
    "        best_n_iter = None\n",
    "        \n",
    "        # Run k-means n_init times\n",
    "        for init in range(self.n_init):\n",
    "            # Generate seed for this initialization\n",
    "            if self.random_state is not None:\n",
    "                seed = self.random_state + init\n",
    "            else:\n",
    "                seed = None\n",
    "                \n",
    "            # Perform single k-means run\n",
    "            centers, labels, inertia, n_iter = self._single_fit(X, seed)\n",
    "            \n",
    "            # Update best solution if current one is better\n",
    "            if inertia < best_inertia:\n",
    "                best_centers = centers\n",
    "                best_labels = labels\n",
    "                best_inertia = inertia\n",
    "                best_n_iter = n_iter\n",
    "        \n",
    "        # Store best solution\n",
    "        self.cluster_centers_ = best_centers\n",
    "        self.labels_ = best_labels\n",
    "        self.inertia_ = best_inertia\n",
    "        self.n_iter_ = best_n_iter\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def fit_predict(self, X):\n",
    "        \"\"\"Fit the model and return cluster labels.\"\"\"\n",
    "        return self.fit(X).labels_\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict the closest cluster for each sample in X.\"\"\"\n",
    "        # Convert pandas DataFrame to numpy array if necessary\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "        X = np.asarray(X)\n",
    "        \n",
    "        distances = self._compute_distances(X, self.cluster_centers_)\n",
    "        return np.argmin(distances, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c7be18-a915-4bc1-ae86-ae1f39cc1084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_kmeans(data, n, dist, link):\n",
    "    kmeans = FlexibleKMeans(\n",
    "        n_clusters = n,\n",
    "        metric = dist,\n",
    "        center_method = link,\n",
    "        n_init = 15)\n",
    "\n",
    "    pred_clust = kmeans.fit_predict(data)\n",
    "    \n",
    "    model = 'kmeans'\n",
    "    params = f\"dist = {dist}, link = {link}\"\n",
    "    \n",
    "    return get_metrics(model, params, n, data, pred_clust)\n",
    "\n",
    "distances = ['euclidean', 'manhattan', 'chebyshev']\n",
    "linkages = ['mean', 'median', 'medoid']\n",
    "kmeans_params = product(distances, linkages)\n",
    "\n",
    "clust_range = range(1, max_clust+1)\n",
    "kmeans_params_range = product(clust_range, kmeans_params)\n",
    "# kmeans config instead\n",
    "\n",
    "results = Parallel(n_jobs=max_threads)(delayed(do_kmeans)(data_n, n, dist, link) for n, (dist, link) in kmeans_params_range)\n",
    "kmeans_all = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd030be-4f91-4c05-85e4-c46ac1620d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_AHC(data, n, dist, link):\n",
    "    ahc = AgglomerativeClustering(\n",
    "        n_clusters = n,\n",
    "        metric = dist,\n",
    "        linkage = link)\n",
    "    \n",
    "    ahc.fit(data)\n",
    "    pred_clust = ahc.labels_\n",
    "\n",
    "    model = 'AHC'\n",
    "    params = f\"dist = {dist}, link = {link}\"\n",
    "\n",
    "    return get_metrics(model, params, n, data, pred_clust)\n",
    "\n",
    "distances = ['manhattan', 'euclidean', 'chebyshev', 'hamming']\n",
    "linkages = ['single', 'average', 'complete']\n",
    "ahc_params = [*product(distances, linkages), ('euclidean', 'ward')]\n",
    "\n",
    "clust_range = range(1, max_clust+1)\n",
    "ahc_params_range = product(clust_range, ahc_params)\n",
    "\n",
    "results = Parallel(n_jobs=max_threads)(delayed(do_AHC)(data_n, n, dist, link) for n, (dist, link) in ahc_params_range)\n",
    "ahc_all = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce063cf-1433-4461-851a-646030af0069",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = pd.concat([latent_all, kmeans_all, ahc_all]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d8e1dc-5d1b-415a-b51f-a7b75be5ca46",
   "metadata": {},
   "source": [
    "# Gap stat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c478565-f20e-412d-b776-b880d8261cfa",
   "metadata": {},
   "source": [
    "Adapted from: https://www.geeksforgeeks.org/gap-statistics-for-optimal-number-of-cluster/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23666dfd-c874-40ea-bae5-2d3f22099d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_strg(d):\n",
    "    return ', '.join(f\"{key} = {value}\" for key, value in d.items())\n",
    "\n",
    "# Generate reference data from a uniform distribution\n",
    "def gen_ref_data(data):\n",
    "    return np.random.uniform(low=data.min(axis=0), \n",
    "                            high=data.max(axis=0), \n",
    "                            size=data.shape)\n",
    "\n",
    "# Create empty df to store results\n",
    "def create_empty_df(indices):\n",
    "    cols = ['model', 'params', 'n_clust'] + \\\n",
    "       [f'{index}_gs' for index in CVI] + \\\n",
    "       [f'{index}_s' for index in CVI]\n",
    "    \n",
    "    df = pd.DataFrame(columns=cols)\n",
    "\n",
    "    float_cols = [col for col in cols if col not in ['model', 'params', 'n_clust']]\n",
    "    df[float_cols] = df[float_cols].astype('float64')\n",
    "    \n",
    "    df['model'] = df['model'].astype('object')\n",
    "    df['params'] = df['params'].astype('object')\n",
    "    df['n_clust'] = df['n_clust'].astype('int64')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6726f137-707b-4c61-9b88-04f607d25026",
   "metadata": {},
   "source": [
    "## Step 1: compute the gap statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd706014-394e-46fb-bec1-4d42168e536f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Gap Statistic\n",
    "def compute_gap_statistic(data, model, params, iters):   \n",
    "\n",
    "    str_params = dict_to_strg(params)\n",
    "    gap_values = create_empty_df(CVI)\n",
    "\n",
    "    if model == 'latent': \n",
    "        n_min = 1\n",
    "    else: \n",
    "        n_min = 2\n",
    "\n",
    "    # Loop over n values\n",
    "    for n in range(n_min, max_clust+1):\n",
    "    \n",
    "        # Fit the model on random datasets\n",
    "        rand_scores_all = pd.DataFrame()\n",
    "        \n",
    "        for _ in range(iters):\n",
    "            rand_data = gen_ref_data(data)\n",
    "            \n",
    "            if model == 'latent':\n",
    "                if params.get('covar') == 'without':\n",
    "                    rand_scores = do_StepMix(rand_data, n, **params)\n",
    "                else:\n",
    "                    return None\n",
    "\n",
    "            elif model == 'kmeans':\n",
    "                rand_scores = do_kmeans(rand_data, n, **params)\n",
    "\n",
    "            elif model == 'AHC':\n",
    "                rand_scores = do_AHC(rand_data, n, **params)\n",
    "            \n",
    "            rand_scores = pd.DataFrame([rand_scores])\n",
    "            rand_scores_all = pd.concat([rand_scores_all, rand_scores], ignore_index=True)\n",
    "\n",
    "        # Retrive scores for the assessed model\n",
    "        mod_scores = all_models.loc[(all_models['model'] == model) & \n",
    "                                    (all_models['params'] == str_params) & \n",
    "                                    (all_models['n_clust'] == n)]\n",
    "\n",
    "        # Calculate the Gap statistic and s value for each validity index\n",
    "        for index in CVI:\n",
    "            rand_ind = rand_scores_all[index]\n",
    "            mod_ind = mod_scores[index]\n",
    "\n",
    "            # Rescale the Silhouette index on [0,1] to avoid errors when it is negative\n",
    "            if index == 'silhouette':\n",
    "                rand_ind = (rand_ind + 1) / 2\n",
    "                mod_ind = (mod_ind + 1) / 2\n",
    "                \n",
    "            gap = np.log(np.mean(rand_ind)) - np.log(mod_ind)\n",
    "            s = np.std(np.log(rand_ind)) * np.sqrt(1 + (1 / iters))\n",
    "\n",
    "            # Store the results\n",
    "            ## Check if the corresponding row exists in the df\n",
    "            row_id = ((gap_values['model'] == model) & \n",
    "                      (gap_values['params'] == str_params) & \n",
    "                      (gap_values['n_clust'] == n))\n",
    "\n",
    "            if gap_values[row_id].empty:\n",
    "            ## If not, create a new one\n",
    "                new_row = {\n",
    "                    'model': model,\n",
    "                    'params': str_params,\n",
    "                    'n_clust': n,\n",
    "                    f'{index}_gs': gap.values[0],\n",
    "                    f'{index}_s': s\n",
    "                }\n",
    "                new_row = pd.DataFrame([new_row])\n",
    "                gap_values = pd.concat([gap_values, new_row], ignore_index=True)\n",
    "            \n",
    "            else:\n",
    "            # Otherwise, update the existing row\n",
    "                gap_values.loc[row_id, f'{index}_gs'] = gap.values[0]\n",
    "                gap_values.loc[row_id, f'{index}_s'] = s\n",
    "\n",
    "    return gap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eed8683-ac4a-4048-a5a7-4d31e355fb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters grid\n",
    "\n",
    "models = ['latent', 'kmeans', 'AHC']\n",
    "\n",
    "msrt = ['categorical', 'continuous']\n",
    "covar = ['without', 'with']\n",
    "latent_params = list(product(msrt, covar))\n",
    "\n",
    "dist = ['euclidean', 'manhattan', 'chebyshev']\n",
    "link = ['mean', 'median', 'medoid']\n",
    "kmeans_params = list(product(dist, link))\n",
    "\n",
    "dist = ['manhattan', 'euclidean', 'chebyshev', 'hamming']\n",
    "link = ['single', 'average', 'complete']\n",
    "ahc_params = [*product(dist, link), ('euclidean', 'ward')]\n",
    "\n",
    "params = {\n",
    "    'latent': latent_params,\n",
    "    'kmeans': kmeans_params,\n",
    "    'AHC': ahc_params\n",
    "}\n",
    "\n",
    "param_names = {\n",
    "    'latent': ['msrt', 'covar'],\n",
    "    'kmeans': ['dist', 'link'],\n",
    "    'AHC': ['dist', 'link']\n",
    "}\n",
    "\n",
    "grid = [\n",
    "    (model, dict(zip(param_names[model], param_values)))\n",
    "    for model in models\n",
    "    for param_values in params[model]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dda374a-ffa5-48e8-a4fe-5079cc94cff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gap values for all models\n",
    "results = Parallel(n_jobs=max_threads)(\n",
    "    delayed(compute_gap_statistic)(data_n, iters=2, model=model, params=config)\n",
    "    for model, config in grid\n",
    ")\n",
    "\n",
    "gap_values = pd.concat(results).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657f0eaa-0fbf-4fe5-9459-2f6d251da20e",
   "metadata": {},
   "source": [
    "## Step 2: identify the optimal number of clusters for each model-config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e4c7b9-5a5d-44df-b59f-4a6fe058d993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the optimal number of clusters\n",
    "def get_best_gap(model, params, index):\n",
    "    # Subset gap_values to the right model and params \n",
    "    rows_id = ((gap_values['model'] == model) & (gap_values['params'] == dict_to_strg(params)))\n",
    "    df = gap_values[rows_id].reset_index(drop=True)\n",
    "\n",
    "    # Extract gap and s values\n",
    "    gap = df[f'{index}_gs']\n",
    "    s = df[f'{index}_s']\n",
    "\n",
    "    # Select rows such that GS(k) >= GS(k+1) - s(k+1)\n",
    "    # Skipping the last row and adjusting for index-based calculations\n",
    "    n_min = df['n_clust'].min()\n",
    "    stats = []\n",
    "    \n",
    "    for i in range(0, len(df) - 1):\n",
    "        stat = gap[i] - gap[i+1] + s[i+1]\n",
    "        if stat >= 0: \n",
    "            stats.append([i+n_min, stat])\n",
    "\n",
    "    # Return optimal cluster number\n",
    "    stats = np.array(stats)\n",
    "    if stats.size == 0:\n",
    "        best_n = 'none'\n",
    "    else:\n",
    "        best_n = int(stats[np.argmin(stats[:, 1]), 0])\n",
    "\n",
    "    return best_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baad9661-0ffa-4ce3-ba43-d0f5c60a73d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df to store results\n",
    "cols = ['model', 'params', 'n_clust'] + \\\n",
    "       [index for index in CVI] + \\\n",
    "       [f'{index}_abs' for index in CVI] + \\\n",
    "       [f'{index}_elbow' for index in CVI] + \\\n",
    "       [f'{index}_gap' for index in CVI]\n",
    "\n",
    "candidate_models = pd.DataFrame(columns=cols)\n",
    "\n",
    "candidate_models['model'] = candidate_models['model'].astype('object')\n",
    "candidate_models['params'] = candidate_models['params'].astype('object')\n",
    "\n",
    "float_cols = [col for col in cols if col not in ['model', 'params', 'n_clust'] + CVI]\n",
    "candidate_models[float_cols] = candidate_models[float_cols].astype('float64')\n",
    "\n",
    "int_cols = [col for col in cols if col in ['n_clust'] + CVI]\n",
    "candidate_models[int_cols] = candidate_models[int_cols].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95226fed-3e91-4bf2-bc52-6de7cae48c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best n\n",
    "for model, config in grid:\n",
    "    for index in CVI:\n",
    "        best_n = get_best_gap(model, config, index)\n",
    "\n",
    "        # Check if a best value has been identified\n",
    "        if best_n != 'none':\n",
    "            row_id = ((candidate_models['model'] == model) & \n",
    "                      (candidate_models['params'] == config) &\n",
    "                      (candidate_models['n_clust'] == best_n))\n",
    "            \n",
    "            # Check if the corresponding row exists in the df\n",
    "            if candidate_models[row_id].empty:\n",
    "\n",
    "                model_id = ((all_models['model'] == model) & \n",
    "                           (all_models['params'] == dict_to_strg(config)) &\n",
    "                           (all_models['n_clust'] == best_n))\n",
    "                \n",
    "                new_row = {\n",
    "                    'model': model,\n",
    "                    'params': config,\n",
    "                    'n_clust': best_n,\n",
    "                    'silhouette': all_models.loc[model_id, 'silhouette'].values[0],\n",
    "                    'calinski_harabasz': all_models.loc[model_id, 'calinski_harabasz'].values[0],\n",
    "                    'davies_bouldin': all_models.loc[model_id, 'davies_bouldin'].values[0],\n",
    "                    'dunn': all_models.loc[model_id, 'dunn'].values[0],\n",
    "                    'inertia': all_models.loc[model_id, 'inertia'].values[0],\n",
    "                    f'{index}_gap': 1\n",
    "                }\n",
    "                \n",
    "                new_row = pd.DataFrame([new_row])\n",
    "                candidate_models = pd.concat([candidate_models, new_row], ignore_index=True)\n",
    "\n",
    "            # Otherwise, update the existing row\n",
    "            else:\n",
    "                candidate_models.loc[row_id, f'{index}_gap'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d900af75-f47a-43be-99b9-c0f44f788ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cfd4b8-26e2-44c8-94a4-4784e487c101",
   "metadata": {},
   "source": [
    "## Step 3: identify the best model for each class among the candidates\n",
    "\n",
    "For each unique model, find the max value of each index.\n",
    "\n",
    "Then fuse models appearing twice or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea25d0bb-942e-4e38-b63f-25f19780fa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = pd.DataFrame()\n",
    "\n",
    "for model in models:\n",
    "    df = candidate_models[candidate_models['model'] == model]\n",
    "\n",
    "    best_silhouette = df.sort_values('silhouette', ascending=False).iloc[0]\n",
    "    best_ch = df.sort_values('calinski_harabasz', ascending=False).iloc[0]\n",
    "    best_db = df.sort_values('davies_bouldin', ascending=True).iloc[0]\n",
    "    best_dunn = df.sort_values('dunn', ascending=False).iloc[0]\n",
    "    best_inertia = df.sort_values('inertia', ascending=False).iloc[0]\n",
    "\n",
    "    selected_models = pd.DataFrame([best_silhouette, best_ch, best_db, best_dunn, best_inertia])\n",
    "    best_models = pd.concat([best_models, selected_models]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bf42fe-c984-4a0f-aef7-8decc9eaf7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = pd.DataFrame()\n",
    "\n",
    "for model in ['AHC']:\n",
    "    df = candidate_models[candidate_models['model'] == model]\n",
    "\n",
    "    best_silhouette = df.sort_values('silhouette', ascending=False).iloc[0]\n",
    "    best_ch = df.sort_values('calinski_harabasz', ascending=False).iloc[0]\n",
    "    best_db = df.sort_values('davies_bouldin', ascending=True).iloc[0]\n",
    "    best_dunn = df.sort_values('dunn', ascending=False).iloc[0]\n",
    "    best_inertia = df.sort_values('inertia', ascending=False).iloc[0]\n",
    "\n",
    "    selected_models = pd.DataFrame([best_silhouette, best_ch, best_db, best_dunn, best_inertia])\n",
    "    best_models = pd.concat([best_models, selected_models]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ba587b-3ae6-4c09-84bd-fb57f17c3372",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models['params'] = best_models['params'].astype(str)\n",
    "best_models = best_models.drop_duplicates(subset=['model', 'params', 'n_clust'], keep='first')\n",
    "# If the code is done cleanly for elbow and abs, nothing more should be necessary to retrive the best models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cf7404-4351-4574-bc23-b5717fc13b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d29658d-17f8-4d9d-8ff8-18d46264ce89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
