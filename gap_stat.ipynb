{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ed18269-9962-41f2-beb3-45893a3ad8f9",
   "metadata": {},
   "source": [
    "Experimental implementation of the gap statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4887e505-68fb-40c0-adb5-06ef3199e312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "from joblib import Parallel, delayed # for parallelization\n",
    "from itertools import product\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Clustering\n",
    "from stepmix.stepmix import StepMix\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import AgglomerativeClustering, HDBSCAN\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.neighbors import BallTree\n",
    "import torch\n",
    "from torchmetrics.clustering import DunnIndex\n",
    "from collections import Counter\n",
    "from kneed import KneeLocator\n",
    "\n",
    "# Visualization\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import ConvexHull"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d1e61c-3574-4482-9529-59268b7fb8d6",
   "metadata": {},
   "source": [
    "# Preparation\n",
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1190dd79-e4b5-4684-98b0-0f6ce7797686",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2004_i = pd.read_parquet(\"data/data2004_i.parquet\") # load imputed data\n",
    "\n",
    "# Dataset with numeric outcomes\n",
    "data_n = data2004_i[[\n",
    "    # Q2\n",
    "    'clseusa_n', # 'clsetown_n', 'clsestat_n', 'clsenoam_n',\n",
    "    # Q3\n",
    "    'ambornin_n', 'amcit_n', 'amlived_n', 'amenglsh_n', \n",
    "    'amchrstn_n', 'amgovt_n', 'amfeel_n', # 'amancstr_n',\n",
    "    # Q4\n",
    "    'amcitizn_n', 'amshamed_n', 'belikeus_n', 'ambetter_n', 'ifwrong_n', # 'amsports_n', 'lessprd_n',\n",
    "    # Q5\n",
    "    'proudsss_n', 'proudgrp_n', 'proudpol_n', 'prouddem_n', 'proudeco_n',\n",
    "    'proudspt_n', 'proudart_n', 'proudhis_n', 'proudmil_n', 'proudsci_n']]\n",
    "\n",
    "# Dataset with categorical outcomes\n",
    "data_f = data2004_i[[\n",
    "    # Q2\n",
    "    'clseusa_f', # 'clsetown_f', 'clsestat_f', 'clsenoam_f',\n",
    "    # Q3\n",
    "    'ambornin_f', 'amcit_f', 'amlived_f', 'amenglsh_f', \n",
    "    'amchrstn_f', 'amgovt_f', 'amfeel_f', # 'amancstr_f',\n",
    "    # Q4\n",
    "    'amcitizn_f', 'amshamed_f', 'belikeus_f', 'ambetter_f', 'ifwrong_f', # 'amsports_f', 'lessprd_f',\n",
    "    # Q5\n",
    "    'proudsss_f', 'proudgrp_f', 'proudpol_f', 'prouddem_f', 'proudeco_f',\n",
    "    'proudspt_f', 'proudart_f', 'proudhis_f', 'proudmil_f', 'proudsci_f']]\n",
    "\n",
    "# Dataset with controls\n",
    "controls = data2004_i[[\n",
    "    'sex', 'race_f', 'born_usa', 'party_fs', 'religstr_f', \n",
    "    'reltrad_f', 'region_f']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0b97bf-43e0-4516-b3ee-f226e4ebd772",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c55b24ac-f239-4dea-9283-e3491ec65ee5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_clust = 12\n",
    "max_threads = 8\n",
    "\n",
    "val_indexes = ['silhouette', 'calinski_harabasz', 'davies_bouldin', 'dunn', 'inertia']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d170e0b-2c98-40ec-bda1-12743bfd5b9e",
   "metadata": {},
   "source": [
    "## Validity indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01c84df2-d928-437c-b4f8-200cc8b0d208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom score functions to avoid throwing errors when undefined\n",
    "def sil_score(data, pred_clust):\n",
    "    try:\n",
    "        sil_score = silhouette_score(data, pred_clust)\n",
    "    except ValueError:\n",
    "        sil_score = np.nan\n",
    "    return sil_score\n",
    "\n",
    "def ch_score(data, pred_clust):\n",
    "    try:\n",
    "        ch_score = calinski_harabasz_score(data, pred_clust)\n",
    "    except ValueError:\n",
    "        ch_score = np.nan\n",
    "    return ch_score\n",
    "\n",
    "def db_score(data, pred_clust):\n",
    "    try:\n",
    "        db_score = davies_bouldin_score(data, pred_clust)\n",
    "    except ValueError:\n",
    "        db_score = np.nan\n",
    "    return db_score\n",
    "\n",
    "def dunn_score(data, pred_clust):\n",
    "    torch_data = np.array(data)\n",
    "    torch_data = torch.tensor(torch_data, dtype=torch.float32)\n",
    "    torch_pred_clust = torch.tensor(pred_clust, dtype=torch.int64)\n",
    "\n",
    "    dunn_metric = DunnIndex()\n",
    "    \n",
    "    try:\n",
    "        dunn_score = float(dunn_metric(torch_data, torch_pred_clust))\n",
    "    except Exception:\n",
    "        dunn_score = np.nan\n",
    " \n",
    "    return dunn_score\n",
    "\n",
    "def inertia(data, labels):\n",
    "    data = np.asarray(data)\n",
    "    \n",
    "    inertia = 0\n",
    "    for cluster in np.unique(labels):\n",
    "        cluster_points = data[labels == cluster]\n",
    "        cluster_centroid = np.mean(cluster_points, axis=0)\n",
    "        inertia += np.sum((cluster_points - cluster_centroid) ** 2)\n",
    "        \n",
    "    return inertia\n",
    "\n",
    "def clust_size(labels):\n",
    "    cluster_sizes = Counter(labels)\n",
    "    min_size = min(cluster_sizes.values())\n",
    "    max_size = max(cluster_sizes.values())\n",
    "    \n",
    "    return min_size, max_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf65c0e6-238a-4879-87e3-074f85d8333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return all validity indexes at once\n",
    "def get_metrics(model, params, n, data, pred_clust, **additional_metrics):\n",
    "    base_metrics = {\n",
    "        'model': model,\n",
    "        'params': params,\n",
    "        'n_clust': n,\n",
    "        'min_clust_size': clust_size(pred_clust)[0],\n",
    "        'max_clust_size': clust_size(pred_clust)[1],\n",
    "        'silhouette': sil_score(data, pred_clust),\n",
    "        'calinski_harabasz': ch_score(data, pred_clust),\n",
    "        'davies_bouldin': db_score(data, pred_clust),\n",
    "        'dunn': dunn_score(data, pred_clust),\n",
    "        'inertia': inertia(data, pred_clust)\n",
    "    }\n",
    "\n",
    "    base_metrics.update(additional_metrics)\n",
    "    return base_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7af8aaa-6bf2-4520-a53c-3850fc38cdb6",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aabf9fd9-5fbf-4b62-aa77-f9b4e5a00284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display the optimal numbers of clutsters according to each validity index\n",
    "def elbow_plot(df, val_index):\n",
    "    res = df.dropna(subset=[val_index])\n",
    "\n",
    "    x = res[\"n_clust\"]\n",
    "    y = res[val_index]\n",
    "\n",
    "    if val_index in ['davies_bouldin', 'entropy']:\n",
    "        knee_locator = KneeLocator(x, y, curve='concave', direction='increasing')\n",
    "    else:\n",
    "        knee_locator = KneeLocator(x, y, curve='convex', direction='decreasing')\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(x, y, marker=\"o\", linestyle=\"-\", label=val_index)\n",
    "    plt.axvline(x=knee_locator.knee, color=\"r\", linestyle=\"--\", label=f\"Optimal k={knee_locator.knee}\")\n",
    "    plt.xlabel(\"Number of Clusters\")\n",
    "    plt.ylabel(f\"{val_index} index\")\n",
    "    plt.title(f\"Elbow Method for {val_index} index\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1513b5bc-963b-41e5-a6bd-12f45f22c2f5",
   "metadata": {},
   "source": [
    "# Latent class models\n",
    "With the StepMix package, see: https://github.com/Labo-Lacourse/stepmix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c78aabed-4cb1-4e87-a35f-797240d34b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "clust_range = range(1, max_clust+1)\n",
    "\n",
    "opt_params = {\n",
    "    'method': 'gradient',\n",
    "    'intercept': True,\n",
    "    'max_iter': 2500,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa743e87-3ef4-4e07-9610-59a15540bca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit models without covariates\n",
    "def do_StepMix(n, type, data):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "\n",
    "        latent_mod = StepMix(\n",
    "            n_components = n, \n",
    "            measurement = type, \n",
    "            n_init = 3,\n",
    "            init_params = 'kmeans',\n",
    "            structural_params = opt_params,\n",
    "            random_state = 123,\n",
    "            progress_bar = 0)\n",
    "        \n",
    "        latent_mod.fit(data)\n",
    "        pred_clust = latent_mod.predict(data)\n",
    "\n",
    "        model = 'LCA' if type == 'categorical' else 'LPA'\n",
    "        params = 'without covariates'\n",
    "        loglik = latent_mod.score(data)\n",
    "        aic = latent_mod.aic(data)\n",
    "        bic = latent_mod.aic(data)\n",
    "        entropy = latent_mod.entropy(data)\n",
    "\n",
    "    return get_metrics(model, params, n, data, pred_clust, LL = loglik, aic = aic, bic = bic, entropy = entropy)\n",
    "\n",
    "data = data_f.apply(lambda col: LabelEncoder().fit_transform(col))\n",
    "cat_results = Parallel(n_jobs=8)(delayed(do_StepMix)(n, 'categorical', data) for n in clust_range)\n",
    "LCA_all = pd.DataFrame(cat_results)\n",
    "\n",
    "num_results = Parallel(n_jobs=8)(delayed(do_StepMix)(n, 'continuous', data_n) for n in clust_range)\n",
    "LPA_all = pd.DataFrame(num_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d8e1dc-5d1b-415a-b51f-a7b75be5ca46",
   "metadata": {},
   "source": [
    "# Gap stat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c478565-f20e-412d-b776-b880d8261cfa",
   "metadata": {},
   "source": [
    "Adapted from: https://www.geeksforgeeks.org/gap-statistics-for-optimal-number-of-cluster/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "590a04fb-b251-4371-ad67-967ecc9739dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the Gap Statistic\n",
    "def compute_gap_statistic(data, k_min, k_max, n_replicates, model, params):\n",
    "    \n",
    "    # Generate reference data from a uniform distribution\n",
    "    def gen_ref_data(data):\n",
    "        return np.random.uniform(low=data.min(axis=0), \n",
    "                                 high=data.max(axis=0), \n",
    "                                 size=data.shape)\n",
    "\n",
    "    gap_values = []\n",
    "    \n",
    "    # Loop over k values\n",
    "    for k in range(k_min, k_max + 1):\n",
    "        # Retrive inertia for the assessed model\n",
    "        mod_inertia = LCA_all.loc[(LCA_all['model'] == model) & (LCA_all['params'] == params) & (LCA_all['n_clust'] == k), 'inertia'].values\n",
    "        \n",
    "        # Compute the average inertia for the reference datasets\n",
    "        reference_inertia = []\n",
    "        for _ in range(n_replicates):\n",
    "            random_data = gen_ref_data(data)\n",
    "            ref_inertia = float(do_StepMix(k, 'continuous', data_n)['inertia'])\n",
    "            reference_inertia.append(ref_inertia)\n",
    "\n",
    "        # Calculate the Gap statistic and se\n",
    "        gap = np.log(np.mean(ref_inertia)) - np.log(mod_inertia)\n",
    "        se = np.std(ref_inertia) / np.sqrt(n_replicates)\n",
    "        gap_values.append([k, float(gap[0]), float(se)])\n",
    "\n",
    "    return gap_values\n",
    "\n",
    "gap_values = compute_gap_statistic(data_n, 1, 8, n_replicates=10, model='LCA', params='without covariates')\n",
    "\n",
    "# Compute GS(k) - GS(k+1) + se(k+1)\n",
    "stats = []\n",
    "for i in range(0,4):\n",
    "    stat = gap_values[i][1] - gap_values[i-1][1] + gap_values[i][2]\n",
    "    # Select rows such that GS(k) >= GS(k+1) + se(k+1)\n",
    "    if stat >= 0: \n",
    "        stats.append([i+1, stat])\n",
    "\n",
    "# Find min value\n",
    "stats = np.array(stats)\n",
    "best_k = int(stats[np.argmin(stats[:, 1]), 0])\n",
    "best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87a7062-9cb3-4a36-a20a-5c0a813c07b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf704e67-7b28-42b8-9e47-3e633068df86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aedffc-3af9-4237-bf89-8b9c5972d49d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9fa7ce-84ae-4f7c-818e-aec8cb2c57f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best model for each combination of parameters through the Elbow method\n",
    "def elbow_method(df, val_index):\n",
    "    res = df.dropna(subset=[val_index])\n",
    "\n",
    "    x = res[\"n_clust\"]\n",
    "    y = res[val_index]\n",
    "\n",
    "    if val_index in ['davies_bouldin', 'entropy']:\n",
    "        knee_locator = KneeLocator(x, y, curve='concave', direction='increasing')\n",
    "    else:\n",
    "        knee_locator = KneeLocator(x, y, curve='convex', direction='decreasing')\n",
    "    \n",
    "    return res[res[\"n_clust\"] == knee_locator.knee]\n",
    "\n",
    "models = [LCA_all, LPA_all] # + [LCA_covar_all, LPA_covar_all]\n",
    "params = product(models, val_indexes + ['aic', 'bic', 'entropy'])\n",
    "\n",
    "latent_elbow = pd.DataFrame()\n",
    "for model, val_index in params:\n",
    "    best_model = elbow_method(model, val_index)\n",
    "    latent_elbow = pd.concat([latent_elbow, best_model], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a49207-56b4-4705-9a3a-b8afe7d740b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find absolute best models for each validity index\n",
    "latent_elbow = latent_elbow.drop_duplicates().reset_index(drop=True)\n",
    "# Need to add colums indicating which validity index is maximized.\n",
    "# After that, duplicate models should be merged, not dropped.\n",
    "\n",
    "best_silhouette = latent_elbow.sort_values('silhouette', ascending=False).iloc[0]\n",
    "best_ch = latent_elbow.sort_values('calinski_harabasz', ascending=False).iloc[0]\n",
    "best_db = latent_elbow.sort_values('davies_bouldin', ascending=True).iloc[0]\n",
    "best_dunn = latent_elbow.sort_values('dunn', ascending=False).iloc[0]\n",
    "best_inertia = latent_elbow.sort_values('inertia', ascending=False).iloc[0]\n",
    "\n",
    "best_aic = latent_elbow.sort_values('aic', ascending=True).iloc[0]\n",
    "best_bic = latent_elbow.sort_values('bic', ascending=True).iloc[0]\n",
    "best_entropy = latent_elbow.sort_values('entropy', ascending=False).iloc[0]\n",
    "\n",
    "latent_best = pd.DataFrame([best_silhouette, best_ch, best_db, best_dunn, best_inertia])\n",
    "latent_best = latent_best.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa6a49d-a534-4240-a6ee-c0a08e48e7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
