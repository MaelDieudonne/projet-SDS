{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676fe747-bd8b-4115-83bc-0a18c2c9cd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torchmetrics\n",
    "# pip install stepmix\n",
    "# pip install kneed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4887e505-68fb-40c0-adb5-06ef3199e312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "from joblib import Parallel, delayed # for parallelization\n",
    "from itertools import product\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, HDBSCAN\n",
    "from stepmix.stepmix import StepMix\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import torch\n",
    "from torchmetrics.clustering import DunnIndex\n",
    "from collections import Counter\n",
    "from kneed import KneeLocator\n",
    "\n",
    "# Visualization\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import ConvexHull"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d1e61c-3574-4482-9529-59268b7fb8d6",
   "metadata": {},
   "source": [
    "# Preparation\n",
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1190dd79-e4b5-4684-98b0-0f6ce7797686",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2004_i = pd.read_parquet(\"data/data2004_i.parquet\") # load imputed data\n",
    "\n",
    "# Dataset with numeric outcomes\n",
    "data_n = data2004_i[[\n",
    "    'clseusa_n', 'ambornin_n', 'amcit_n', 'amlived_n', 'amenglsh_n', \n",
    "     'amchrstn_n', 'amgovt_n', 'amfeel_n', 'amcitizn_n', 'amshamed_n', \n",
    "     'belikeus_n', 'ambetter_n', 'ifwrong_n', 'proudsss_n', 'proudgrp_n', \n",
    "     'proudpol_n', 'prouddem_n', 'proudeco_n', 'proudspt_n', 'proudart_n', \n",
    "     'proudhis_n', 'proudmil_n', 'proudsci_n']]\n",
    "\n",
    "# Dataset with categorical outcomes\n",
    "data_f = data2004_i[[\n",
    "     'clseusa_f', 'ambornin_f', 'amcit_f', 'amlived_f', 'amenglsh_f', \n",
    "     'amchrstn_f', 'amgovt_f', 'amfeel_f', 'amcitizn_f', 'amshamed_f', \n",
    "     'belikeus_f', 'ambetter_f', 'ifwrong_f', 'proudsss_f', 'proudgrp_f', \n",
    "     'proudpol_f', 'prouddem_f', 'proudeco_f', 'proudspt_f', 'proudart_f', \n",
    "     'proudhis_f', 'proudmil_f', 'proudsci_f']]\n",
    "\n",
    "# Dataset with controls\n",
    "controls = data2004_i[[\n",
    "    'sex', 'race_f', 'born_usa', 'party_fs', 'religstr_f', \n",
    "    'reltrad_f', 'region_f']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d170e0b-2c98-40ec-bda1-12743bfd5b9e",
   "metadata": {},
   "source": [
    "## Validity indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01c84df2-d928-437c-b4f8-200cc8b0d208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom score functions to avoid throwing errors when undefined\n",
    "def sil_score(data, pred_clust):\n",
    "    try:\n",
    "        sil_score = silhouette_score(data, pred_clust)\n",
    "    except ValueError:\n",
    "        sil_score = np.nan\n",
    "    return sil_score\n",
    "\n",
    "def ch_score(data, pred_clust):\n",
    "    try:\n",
    "        ch_score = calinski_harabasz_score(data, pred_clust)\n",
    "    except ValueError:\n",
    "        ch_score = np.nan\n",
    "    return ch_score\n",
    "\n",
    "def db_score(data, pred_clust):\n",
    "    try:\n",
    "        db_score = davies_bouldin_score(data, pred_clust)\n",
    "    except ValueError:\n",
    "        db_score = np.nan\n",
    "    return db_score\n",
    "\n",
    "def dunn_score(data, pred_clust):\n",
    "    torch_data = np.array(data)\n",
    "    torch_data = torch.tensor(torch_data, dtype=torch.float32)\n",
    "    torch_pred_clust = torch.tensor(pred_clust, dtype=torch.int64)\n",
    "\n",
    "    dunn_metric = DunnIndex()\n",
    "    \n",
    "    try:\n",
    "        dunn_score = float(dunn_metric(torch_data, torch_pred_clust))\n",
    "    except Exception:\n",
    "        dunn_score = np.nan\n",
    " \n",
    "    return dunn_score\n",
    "\n",
    "def inertia(data, labels):\n",
    "    data = np.asarray(data)\n",
    "    \n",
    "    inertia = 0\n",
    "    for cluster in np.unique(labels):\n",
    "        cluster_points = data[labels == cluster]\n",
    "        cluster_centroid = np.mean(cluster_points, axis=0)\n",
    "        inertia += np.sum((cluster_points - cluster_centroid) ** 2)\n",
    "        \n",
    "    return inertia\n",
    "\n",
    "def clust_size(labels):\n",
    "    cluster_sizes = Counter(labels)\n",
    "    min_size = min(cluster_sizes.values())\n",
    "    max_size = max(cluster_sizes.values())\n",
    "    \n",
    "    return min_size, max_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf65c0e6-238a-4879-87e3-074f85d8333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return all validity indexes at once\n",
    "def get_metrics(model, params, n, data, pred_clust, **additional_metrics):\n",
    "    base_metrics = {\n",
    "        'model': model,\n",
    "        'params': params,\n",
    "        'n_clust': n,\n",
    "        'min_clust_size': clust_size(pred_clust)[0],\n",
    "        'max_clust_size': clust_size(pred_clust)[1],\n",
    "        'silhouette': sil_score(data, pred_clust),\n",
    "        'calinski_harabasz': ch_score(data, pred_clust),\n",
    "        'davies_bouldin': db_score(data, pred_clust),\n",
    "        'dunn': dunn_score(data, pred_clust),\n",
    "        'inertia': inertia(data, pred_clust)\n",
    "    }\n",
    "\n",
    "    base_metrics.update(additional_metrics)\n",
    "    return base_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aabf9fd9-5fbf-4b62-aa77-f9b4e5a00284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display the optimal numbers of clutsters according to each validity index\n",
    "def elbow_plot(df, val_index):\n",
    "    res = df.dropna(subset=[val_index])\n",
    "\n",
    "    x = res[\"n_clust\"]\n",
    "    y = res[val_index]\n",
    "\n",
    "    if val_index in ['davies_bouldin', 'entropy']:\n",
    "        knee_locator = KneeLocator(x, y, curve='concave', direction='increasing')\n",
    "    else:\n",
    "        knee_locator = KneeLocator(x, y, curve='convex', direction='decreasing')\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(x, y, marker=\"o\", linestyle=\"-\", label=val_index)\n",
    "    plt.axvline(x=knee_locator.knee, color=\"r\", linestyle=\"--\", label=f\"Optimal k={knee_locator.knee}\")\n",
    "    plt.xlabel(\"Number of Clusters\")\n",
    "    plt.ylabel(f\"{val_index} index\")\n",
    "    plt.title(f\"Elbow Method for {val_index} index\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0b97bf-43e0-4516-b3ee-f226e4ebd772",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c55b24ac-f239-4dea-9283-e3491ec65ee5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_clust = 12\n",
    "max_threads = 8\n",
    "\n",
    "val_indexes = ['silhouette', 'calinski_harabasz', 'davies_bouldin', 'dunn', 'inertia']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1513b5bc-963b-41e5-a6bd-12f45f22c2f5",
   "metadata": {},
   "source": [
    "# Latent models\n",
    "With the StepMix package, see: https://github.com/Labo-Lacourse/stepmix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78aabed-4cb1-4e87-a35f-797240d34b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "clust_range = range(1, max_clust+1)\n",
    "\n",
    "opt_params = {\n",
    "    'method': 'gradient',\n",
    "    'intercept': True,\n",
    "    'max_iter': 2500,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa743e87-3ef4-4e07-9610-59a15540bca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without covariates\n",
    "def do_StepMix(n, type, data):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "\n",
    "        latent_mod = StepMix(\n",
    "            n_components = n, \n",
    "            measurement = type, \n",
    "            n_init = 3,\n",
    "            init_params = 'kmeans',\n",
    "            structural_params = opt_params,\n",
    "            random_state = 123)\n",
    "        \n",
    "        latent_mod.fit(data)\n",
    "        pred_clust = latent_mod.predict(data)\n",
    "\n",
    "        model = 'LCA' if type == 'categorical' else 'LPA'\n",
    "        params = 'without covariates'\n",
    "        loglik = latent_mod.score(data)\n",
    "        aic = latent_mod.aic(data)\n",
    "        bic = latent_mod.aic(data)\n",
    "        entropy = latent_mod.entropy(data)\n",
    "\n",
    "    return get_metrics(model, params, n, data, pred_clust, LL = loglik, aic = aic, bic = bic, entropy = entropy)\n",
    "\n",
    "data = data_f.apply(lambda col: LabelEncoder().fit_transform(col))\n",
    "cat_results = Parallel(n_jobs=8)(delayed(do_StepMix)(n, 'categorical', data) for n in clust_range)\n",
    "LCA_all = pd.DataFrame(cat_results)\n",
    "\n",
    "# Data preprocessing?\n",
    "num_results = Parallel(n_jobs=8)(delayed(do_StepMix)(n, 'continuous', data_n) for n in clust_range)\n",
    "LPA_all = pd.DataFrame(num_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d177789-db00-4153-8227-5ec8932dd193",
   "metadata": {},
   "outputs": [],
   "source": [
    "for val_index in val_indexes + ['aic', 'bic', 'entropy']:\n",
    "    elbow_plot(LCA_all, val_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1461c4-fd28-4831-96e2-c4edc004c06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With covariates\n",
    "def do_StepMix_covar(n, type, data):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "        \n",
    "        latent_mod = StepMix(\n",
    "            n_components = n,\n",
    "            measurement = type,\n",
    "            n_init = 3,\n",
    "            init_params = 'kmeans',\n",
    "            structural = 'covariate', \n",
    "            n_steps = 1,\n",
    "            structural_params = opt_params,\n",
    "            random_state = 123)\n",
    "        \n",
    "        latent_mod.fit(data, controls_dum)\n",
    "        pred_clust = latent_mod.predict(data)\n",
    "        \n",
    "        model = 'LCA' if type == 'categorical' else 'LPA'\n",
    "        params = 'with covariates'\n",
    "        loglik = latent_mod.score(data)\n",
    "        aic = latent_mod.aic(data)\n",
    "        bic = latent_mod.aic(data)\n",
    "        entropy = latent_mod.entropy(data)\n",
    "\n",
    "    return get_metrics(model, params, n, data, pred_clust, LL = loglik, aic = aic, bic = bic, entropy = entropy)\n",
    "\n",
    "controls_dum = pd.get_dummies(controls)\n",
    "\n",
    "data = data_f.apply(lambda col: LabelEncoder().fit_transform(col))\n",
    "cat_results = Parallel(n_jobs=max_threads)(delayed(do_StepMix_covar)(n, 'categorical', data) for n in clust_range)\n",
    "LCA_covar_all = pd.DataFrame(cat_results)\n",
    "\n",
    "# Data preprocessing?\n",
    "num_results = Parallel(n_jobs=max_threads)(delayed(do_StepMix_covar)(n, 'continuous', data_n) for n in clust_range)\n",
    "LPA_covar_all = pd.DataFrame(num_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d8e1dc-5d1b-415a-b51f-a7b75be5ca46",
   "metadata": {},
   "source": [
    "## Best latent models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e6a8b3-eaad-4b81-a4cf-19367debf6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to select models based on aic / bic: using their absolute minimum, or an elbow method?\n",
    "# Absolute minimum yields the model with the most classes, so not appropriate\n",
    "LCA_aic_min = LCA_all.sort_values('aic', ascending=True).iloc[0]\n",
    "LCA_bic_min = LCA_all.sort_values('bic', ascending=True).iloc[0]\n",
    "\n",
    "LPA_aic_min = LPA_all.sort_values('aic', ascending=True).iloc[0]\n",
    "LPA_bic_min = LPA_all.sort_values('bic', ascending=True).iloc[0]\n",
    "\n",
    "abs_fit = pd.DataFrame([LCA_aic_min, LCA_bic_min, LPA_aic_min, LPA_bic_min])\n",
    "abs_fit = abs_fit.drop_duplicates().reset_index(drop=True)\n",
    "abs_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50aa4fe-c8bf-490b-a51e-cb3287f70b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best models according to relative fit = LRT / BLRT / BVR (LCA only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9fa7ce-84ae-4f7c-818e-aec8cb2c57f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best model through the Elbow method\n",
    "def elbow_method(df, val_index):\n",
    "    res = df.dropna(subset=[val_index])\n",
    "\n",
    "    x = res[\"n_clust\"]\n",
    "    y = res[val_index]\n",
    "\n",
    "    if val_index in ['davies_bouldin', 'entropy']:\n",
    "        knee_locator = KneeLocator(x, y, curve='concave', direction='increasing')\n",
    "    else:\n",
    "        knee_locator = KneeLocator(x, y, curve='convex', direction='decreasing')\n",
    "    \n",
    "    return res[res[\"n_clust\"] == knee_locator.knee]\n",
    "\n",
    "models = [LCA_all, LPA_all] + [LCA_covar_all, LPA_covar_all]\n",
    "\n",
    "params = product(models, val_indexes + ['aic', 'bic', 'entropy'])\n",
    "\n",
    "latent_elbow = pd.DataFrame()\n",
    "for model, val_index in params:\n",
    "    best_model = elbow_method(model, val_index)\n",
    "    latent_elbow = pd.concat([latent_elbow, best_model], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a49207-56b4-4705-9a3a-b8afe7d740b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find absolute best models for each validity index\n",
    "latent_elbow = latent_elbow.drop_duplicates().reset_index(drop=True)\n",
    "# Need to add colums indicating which validity index is maximized.\n",
    "# After that, duplicate models should be merged, not dropped.\n",
    "\n",
    "best_silhouette = latent_elbow.sort_values('silhouette', ascending=False).iloc[0]\n",
    "best_ch = latent_elbow.sort_values('calinski_harabasz', ascending=False).iloc[0]\n",
    "best_db = latent_elbow.sort_values('davies_bouldin', ascending=True).iloc[0]\n",
    "best_dunn = latent_elbow.sort_values('dunn', ascending=False).iloc[0]\n",
    "best_inertia = latent_elbow.sort_values('inertia', ascending=False).iloc[0]\n",
    "\n",
    "best_aic = latent_elbow.sort_values('aic', ascending=True).iloc[0]\n",
    "best_bic = latent_elbow.sort_values('bic', ascending=True).iloc[0]\n",
    "best_entropy = latent_elbow.sort_values('entropy', ascending=False).iloc[0]\n",
    "\n",
    "latent_best = pd.DataFrame([best_silhouette, best_ch, best_db, best_dunn, best_inertia])\n",
    "latent_best = latent_best.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa6a49d-a534-4240-a6ee-c0a08e48e7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f439f8-bcc6-4251-a179-ba8dc0bdfa7a",
   "metadata": {},
   "source": [
    "The inclusion of covariates makes almost no difference.\n",
    "\n",
    "All selected models have 3-4 clusters\n",
    "\n",
    "The best model overall seems to be the LPA one.\n",
    "- It has lower entropy, meaning it classifies the individuals with better certainty.\n",
    "- It has lower aic and bic, meaning better model fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d893bba-c892-432d-9cb0-85d85acd6304",
   "metadata": {},
   "source": [
    "# K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5633f1ec-b9c8-439d-a1e6-cd6f14b71e57",
   "metadata": {},
   "source": [
    "## Flexible kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54b8d0bb-ea26-4654-bc81-768df477837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "class FlexibleKMeans:\n",
    "    \"\"\"\n",
    "    K-Means implementation supporting different distance metrics and center computation methods.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_clusters : int\n",
    "        Number of clusters\n",
    "    metric : str, default='euclidean'\n",
    "        Distance metric: 'euclidean', 'manhattan', 'chebyshev'\n",
    "    center_method : str, default='mean'\n",
    "        Method to compute cluster centers: 'mean' (centroid), 'median', 'medoid'\n",
    "    max_iter : int, default=100\n",
    "        Maximum number of iterations\n",
    "    random_state : int or None, default=None\n",
    "        Random state for reproducibility\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_clusters, metric='euclidean', center_method='mean', \n",
    "                 max_iter=100, random_state=None):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.metric = metric\n",
    "        self.center_method = center_method\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Define mapping from user-friendly names to scipy metrics\n",
    "        self.metric_mapping = {\n",
    "            'euclidean': 'euclidean',\n",
    "            'manhattan': 'cityblock',\n",
    "            'chebyshev': 'chebyshev'\n",
    "        }\n",
    "        \n",
    "        # Validate inputs\n",
    "        valid_metrics = list(self.metric_mapping.keys())\n",
    "        if metric not in valid_metrics:\n",
    "            raise ValueError(f\"metric must be one of {valid_metrics}\")\n",
    "            \n",
    "        valid_centers = ['mean', 'median', 'medoid']\n",
    "        if center_method not in valid_centers:\n",
    "            raise ValueError(f\"center_method must be one of {valid_centers}\")\n",
    "    \n",
    "    def _compute_distances(self, X, centers):\n",
    "        \"\"\"Compute distances between points and centers using specified metric.\"\"\"\n",
    "        return cdist(X, centers, metric=self.metric_mapping[self.metric])\n",
    "    \n",
    "    def _compute_centers(self, X, labels):\n",
    "        \"\"\"Compute new centers using specified method.\"\"\"\n",
    "        new_centers = np.zeros((self.n_clusters, X.shape[1]))\n",
    "        \n",
    "        for i in range(self.n_clusters):\n",
    "            cluster_points = X[labels == i]\n",
    "            \n",
    "            if len(cluster_points) == 0:\n",
    "                continue\n",
    "                \n",
    "            if self.center_method == 'mean':\n",
    "                new_centers[i] = np.mean(cluster_points, axis=0)\n",
    "            \n",
    "            elif self.center_method == 'median':\n",
    "                new_centers[i] = np.median(cluster_points, axis=0)\n",
    "            \n",
    "            elif self.center_method == 'medoid':\n",
    "                # For medoid, find the point that minimizes sum of distances to other points\n",
    "                distances = self._compute_distances(cluster_points, cluster_points)\n",
    "                medoid_idx = np.argmin(np.sum(distances, axis=1))\n",
    "                new_centers[i] = cluster_points[medoid_idx]\n",
    "        \n",
    "        return new_centers\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit the model to the data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data (numpy array or pandas DataFrame)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "            Fitted estimator\n",
    "        \"\"\"\n",
    "        # Convert pandas DataFrame to numpy array if necessary\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "        X = np.asarray(X)\n",
    "        \n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "            \n",
    "        # Initialize centers randomly\n",
    "        idx = np.random.choice(len(X), self.n_clusters, replace=False)\n",
    "        self.cluster_centers_ = X[idx].copy()\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            # Store old centers for convergence check\n",
    "            old_centers = self.cluster_centers_.copy()\n",
    "            \n",
    "            # Assign points to nearest center\n",
    "            distances = self._compute_distances(X, self.cluster_centers_)\n",
    "            self.labels_ = np.argmin(distances, axis=1)\n",
    "            \n",
    "            # Update centers\n",
    "            self.cluster_centers_ = self._compute_centers(X, self.labels_)\n",
    "            \n",
    "            # Check for convergence\n",
    "            if np.allclose(old_centers, self.cluster_centers_):\n",
    "                self.n_iter_ = iteration + 1\n",
    "                break\n",
    "        else:\n",
    "            self.n_iter_ = self.max_iter\n",
    "            \n",
    "        # Compute final inertia (sum of squared distances to closest center)\n",
    "        self.inertia_ = np.sum(np.min(distances, axis=1) ** 2)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def fit_predict(self, X):\n",
    "        \"\"\"\n",
    "        Fit the model and return cluster labels.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        labels : array of shape (n_samples,)\n",
    "            Index of the cluster each sample belongs to\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.labels_\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the closest cluster for each sample in X.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            New data to predict (numpy array or pandas DataFrame)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        labels : array of shape (n_samples,)\n",
    "            Index of the cluster each sample belongs to\n",
    "        \"\"\"\n",
    "        # Convert pandas DataFrame to numpy array if necessary\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "        X = np.asarray(X)\n",
    "        \n",
    "        distances = self._compute_distances(X, self.cluster_centers_)\n",
    "        return np.argmin(distances, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8c980f3-14fc-4f4b-955a-24cacb5dc06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_kmeans(n, dist, link):\n",
    "    kmeans = FlexibleKMeans(\n",
    "        n_clusters = n,\n",
    "        metric = dist,\n",
    "        center_method = link,\n",
    "        random_state = 42)\n",
    "\n",
    "    pred_clust = kmeans.fit_predict(data)\n",
    "    \n",
    "    model = 'kmeans'\n",
    "    params = f\"distance = {dist}, linkage = {link}\"\n",
    "    \n",
    "    return get_metrics(model, params, n, data, pred_clust)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data_n)\n",
    "\n",
    "clust_range = range(1, max_clust+1)\n",
    "distances = ['euclidean', 'manhattan', 'chebyshev']\n",
    "linkages = ['mean', 'median', 'medoid']\n",
    "params = product(clust_range, distances, linkages)\n",
    "\n",
    "results = Parallel(n_jobs=max_threads)(delayed(do_kmeans)(n, dist, link) for n, dist, link in params)\n",
    "kmeans_all = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "506f5dc4-921c-48c0-9433-5ff49e6b91b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best model for each combination of parameters through the Elbow method\n",
    "def elbow_method(dist, link, val_index):\n",
    "    params = f\"distance = {dist}, linkage = {link}\"\n",
    "    res = kmeans_all[kmeans_all['params'] == params]\n",
    "    \n",
    "    res = res.dropna(subset=[val_index])\n",
    "\n",
    "    x = res[\"n_clust\"]\n",
    "    y = res[val_index]\n",
    "\n",
    "    if val_index == 'davies_bouldin':\n",
    "        knee_locator = KneeLocator(x, y, curve='concave', direction='increasing')\n",
    "    else:\n",
    "        knee_locator = KneeLocator(x, y, curve='convex', direction='decreasing')\n",
    "    \n",
    "    return res[res[\"n_clust\"] == knee_locator.knee]\n",
    "\n",
    "kmeans_elbow = pd.DataFrame()\n",
    "\n",
    "distances = ['euclidean', 'manhattan', 'chebyshev']\n",
    "linkages = ['mean', 'median', 'medoid']\n",
    "models = product(distances, linkages)\n",
    "\n",
    "for dist, link in models:\n",
    "    for val_index in val_indexes:\n",
    "        best_mod = elbow_method(dist, link, val_index)\n",
    "        kmeans_elbow = pd.concat([kmeans_elbow, best_mod], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88a605a2-5736-415f-ad55-1f358a3b0fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find absolute best models for each validity index\n",
    "kmeans_elbow = kmeans_elbow.drop_duplicates().reset_index(drop=True)\n",
    "# Need to add colums indicating which validity index is maximized.\n",
    "# After that, duplicate models should be merged, not dropped.\n",
    "\n",
    "best_silhouette = kmeans_elbow.sort_values('silhouette', ascending=False).iloc[0]\n",
    "best_ch = kmeans_elbow.sort_values('calinski_harabasz', ascending=False).iloc[0]\n",
    "best_db = kmeans_elbow.sort_values('davies_bouldin', ascending=True).iloc[0]\n",
    "best_dunn = kmeans_elbow.sort_values('dunn', ascending=False).iloc[0]\n",
    "best_inertia = kmeans_elbow.sort_values('inertia', ascending=False).iloc[0]\n",
    "\n",
    "kmeans_best = pd.DataFrame([best_silhouette, best_ch, best_db, best_dunn, best_inertia])\n",
    "kmeans_best = kmeans_best.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "761079d6-6a2b-4b72-8aaf-e2942c5312af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>params</th>\n",
       "      <th>n_clust</th>\n",
       "      <th>min_clust_size</th>\n",
       "      <th>max_clust_size</th>\n",
       "      <th>silhouette</th>\n",
       "      <th>calinski_harabasz</th>\n",
       "      <th>davies_bouldin</th>\n",
       "      <th>dunn</th>\n",
       "      <th>inertia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kmeans</td>\n",
       "      <td>distance = manhattan, linkage = mean</td>\n",
       "      <td>3</td>\n",
       "      <td>117</td>\n",
       "      <td>980</td>\n",
       "      <td>0.242334</td>\n",
       "      <td>142.351508</td>\n",
       "      <td>2.403297</td>\n",
       "      <td>0.403745</td>\n",
       "      <td>22629.298954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kmeans</td>\n",
       "      <td>distance = manhattan, linkage = medoid</td>\n",
       "      <td>2</td>\n",
       "      <td>277</td>\n",
       "      <td>938</td>\n",
       "      <td>0.231802</td>\n",
       "      <td>213.129413</td>\n",
       "      <td>2.080685</td>\n",
       "      <td>0.420379</td>\n",
       "      <td>23768.730029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kmeans</td>\n",
       "      <td>distance = chebyshev, linkage = median</td>\n",
       "      <td>2</td>\n",
       "      <td>259</td>\n",
       "      <td>956</td>\n",
       "      <td>-0.018157</td>\n",
       "      <td>30.503825</td>\n",
       "      <td>4.555947</td>\n",
       "      <td>0.144715</td>\n",
       "      <td>27259.493964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    model                                  params  n_clust  min_clust_size  \\\n",
       "0  kmeans    distance = manhattan, linkage = mean        3             117   \n",
       "1  kmeans  distance = manhattan, linkage = medoid        2             277   \n",
       "2  kmeans  distance = chebyshev, linkage = median        2             259   \n",
       "\n",
       "   max_clust_size  silhouette  calinski_harabasz  davies_bouldin      dunn  \\\n",
       "0             980    0.242334         142.351508        2.403297  0.403745   \n",
       "1             938    0.231802         213.129413        2.080685  0.420379   \n",
       "2             956   -0.018157          30.503825        4.555947  0.144715   \n",
       "\n",
       "        inertia  \n",
       "0  22629.298954  \n",
       "1  23768.730029  \n",
       "2  27259.493964  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d52b42-7a2e-4050-a093-ef46877f8464",
   "metadata": {},
   "source": [
    "## Simple kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "553fc039-719c-418c-bf1a-e6eca6a7d826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_kmeans(n):\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=n,\n",
    "        init='k-means++',\n",
    "        n_init=25,\n",
    "        random_state=42)\n",
    "    pred_clust = kmeans.fit_predict(data)\n",
    "    \n",
    "    model = 'kmeans'\n",
    "    params = 'centroid'\n",
    "    \n",
    "    return get_metrics(model, params, n, data, pred_clust)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data_n)\n",
    "\n",
    "clust_range = range(1, max_clust+1)\n",
    "\n",
    "results = Parallel(n_jobs=max_threads)(delayed(do_kmeans)(n) for n in clust_range)\n",
    "kmeans_all = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08613620-a1be-4a57-8e70-3b424df77619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best model for each combination of parameters through the Elbow method\n",
    "def elbow_method(val_index):\n",
    "    res = kmeans_all.dropna(subset=[val_index])\n",
    "\n",
    "    x = res[\"n_clust\"]\n",
    "    y = res[val_index]\n",
    "\n",
    "    if val_index == 'davies_bouldin':\n",
    "        knee_locator = KneeLocator(x, y, curve='concave', direction='increasing')\n",
    "    else:\n",
    "        knee_locator = KneeLocator(x, y, curve='convex', direction='decreasing')\n",
    "    \n",
    "    return res[res[\"n_clust\"] == knee_locator.knee]\n",
    "\n",
    "kmeans_elbow = pd.DataFrame()\n",
    "\n",
    "for val_index in val_indexes:\n",
    "    best_mod = elbow_method(val_index)\n",
    "    kmeans_elbow = pd.concat([kmeans_elbow, best_mod], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aefa7f6-75a4-43e1-9b09-6e9379f34a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find absolute best models for each validity index\n",
    "kmeans_elbow = kmeans_elbow.drop_duplicates().reset_index(drop=True)\n",
    "# Need to add colums indicating which validity index is maximized.\n",
    "# After that, duplicate models should be merged, not dropped.\n",
    "\n",
    "best_silhouette = kmeans_elbow.sort_values('silhouette', ascending=False).iloc[0]\n",
    "best_ch = kmeans_elbow.sort_values('calinski_harabasz', ascending=False).iloc[0]\n",
    "best_db = kmeans_elbow.sort_values('davies_bouldin', ascending=True).iloc[0]\n",
    "best_dunn = kmeans_elbow.sort_values('dunn', ascending=False).iloc[0]\n",
    "best_inertia = kmeans_elbow.sort_values('inertia', ascending=False).iloc[0]\n",
    "\n",
    "kmeans_best = pd.DataFrame([best_silhouette, best_ch, best_db, best_dunn, best_inertia])\n",
    "kmeans_best = kmeans_best.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40857af-6908-476b-b42e-82f9d8b191a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24849e55-57cb-4680-9a5b-c8951503d389",
   "metadata": {},
   "source": [
    "# AHC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b6d279-76b0-4cc9-a36c-d1b0db6bdd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the models\n",
    "def do_AHC(n, dist, link):\n",
    "    ahc = AgglomerativeClustering(\n",
    "        n_clusters = n,\n",
    "        metric = dist,\n",
    "        linkage = link)\n",
    "    \n",
    "    ahc.fit(data)\n",
    "    pred_clust = ahc.labels_\n",
    "\n",
    "    model = 'AHC'\n",
    "    params = f\"distance = {dist}, linkage = {link}\"\n",
    "\n",
    "    return get_metrics(model, params, n, data, pred_clust)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data_n)\n",
    "\n",
    "clust_range = range(1, max_clust+1)\n",
    "distances = ['manhattan', 'euclidean', 'chebyshev']\n",
    "linkages = ['single', 'average', 'complete']\n",
    "params = product(clust_range, distances, linkages)\n",
    "\n",
    "results = Parallel(n_jobs=max_threads)(delayed(do_AHC)(n, dist, link) for n, dist, link in params)\n",
    "results.extend([do_AHC(n, 'euclidean', 'ward') for n in clust_range])\n",
    "ahc_all = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7351b9-0ffc-4a85-b55f-569c986db981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best model for each combination of parameters through the Elbow method\n",
    "def elbow_method(dist, link, val_index):\n",
    "    params = f\"distance = {dist}, linkage = {link}\"\n",
    "    res = ahc_all[ahc_all['params'] == params]\n",
    "    \n",
    "    res = res.dropna(subset=[val_index])\n",
    "\n",
    "    x = res[\"n_clust\"]\n",
    "    y = res[val_index]\n",
    "\n",
    "    if val_index == 'davies_bouldin':\n",
    "        knee_locator = KneeLocator(x, y, curve='concave', direction='increasing')\n",
    "    else:\n",
    "        knee_locator = KneeLocator(x, y, curve='convex', direction='decreasing')\n",
    "    \n",
    "    return res[res[\"n_clust\"] == knee_locator.knee]\n",
    "\n",
    "ahc_elbow = pd.DataFrame()\n",
    "\n",
    "distances = ['manhattan', 'euclidean', 'chebyshev']\n",
    "linkages = ['single', 'average', 'complete']\n",
    "models = product(distances, linkages)\n",
    "\n",
    "for dist, link in models:\n",
    "    for val_index in val_indexes:\n",
    "        best_mod = elbow_method(dist, link, val_index)\n",
    "        ahc_elbow = pd.concat([ahc_elbow, best_mod], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8c12d3-f653-4861-a828-285aebb43187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find absolute best models for each validity index\n",
    "ahc_elbow = ahc_elbow.drop_duplicates().reset_index(drop=True)\n",
    "# Need to add colums indicating which validity index is maximized.\n",
    "# After that, duplicate models should be merged, not dropped.\n",
    "\n",
    "best_silhouette = ahc_elbow.sort_values('silhouette', ascending=False).iloc[0]\n",
    "best_ch = ahc_elbow.sort_values('calinski_harabasz', ascending=False).iloc[0]\n",
    "best_db = ahc_elbow.sort_values('davies_bouldin', ascending=True).iloc[0]\n",
    "best_dunn = ahc_elbow.sort_values('dunn', ascending=False).iloc[0]\n",
    "best_inertia = ahc_elbow.sort_values('inertia', ascending=False).iloc[0]\n",
    "\n",
    "ahc_best = pd.DataFrame([best_silhouette, best_ch, best_db, best_dunn, best_inertia])\n",
    "ahc_best = ahc_best.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987589e4-0282-450f-a274-c814fec146b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ahc_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f597a400-a862-40ee-b669-d7a65cdc2fdf",
   "metadata": {},
   "source": [
    "AHC yields only one interesting model, where the smallest cluster is not nearly empty. This model have 4 clusters. But its biggest cluster gathers 85 % of the individuals, meaning the others are really small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf5a07e-2a44-4e5e-826d-7a2ac4b6fe57",
   "metadata": {},
   "source": [
    "# HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b99687-9544-4c98-b39e-ca692cf0de68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the models\n",
    "def do_hdbscan(dist, min_c, min_s):\n",
    "    hdb = HDBSCAN(\n",
    "        metric = dist,\n",
    "        min_cluster_size = min_c, \n",
    "        min_samples = min_s)\n",
    "        \n",
    "    pred_clust = hdb.fit_predict(data)\n",
    "\n",
    "    model = 'HDBSCAN'\n",
    "    params = f\"distance = {dist}, min_cluster_size = {min_c}, min_samples = {min_s}\"\n",
    "    n = len(set(pred_clust[pred_clust != -1]))\n",
    "    noise_freq = 100 * sum(pred_clust == -1) / len(pred_clust)\n",
    "\n",
    "    return get_metrics(model, params, n, data, pred_clust, noise = noise_freq)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data_n)\n",
    "\n",
    "distances = ['euclidean', 'chebyshev']\n",
    "min_cluster_sizes = range(2, 16)\n",
    "min_samples_range = range(1, 16)\n",
    "params = product(distances, min_cluster_sizes, min_samples_range)\n",
    "\n",
    "results = Parallel(n_jobs=max_threads)(delayed(do_hdbscan)(dist, min_c, min_s) for dist, min_c, min_s in params)\n",
    "hdbscan_all = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968b43fd-3261-4b35-885b-679ae6215117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Elbow method is inapplicable here. We simply select the model maximizing each validity index.\n",
    "best_silhouette = hdbscan_all.sort_values('silhouette', ascending=False).iloc[0]\n",
    "best_ch = hdbscan_all.sort_values('calinski_harabasz', ascending=False).iloc[0]\n",
    "best_db = hdbscan_all.sort_values('davies_bouldin', ascending=True).iloc[0]\n",
    "best_dunn = hdbscan_all.sort_values('dunn', ascending=False).iloc[0]\n",
    "best_inertia = hdbscan_all.sort_values('inertia', ascending=False).iloc[0]\n",
    "\n",
    "hdbscan_best = pd.DataFrame([best_silhouette, best_ch, best_db, best_dunn, best_inertia])\n",
    "hdbscan_best = hdbscan_best.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8041b4-7446-4d90-a7c3-a3df81cb53f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a32c9a-09b3-4d8a-9b71-152d93c37c66",
   "metadata": {},
   "source": [
    "HDBSCAN clusters all the individuals together. Based on density, there is only one cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a21c7c9-0aa6-4edd-85ab-a71147062c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of the number of clusters selected by HDBSCAN modelsgrouping the values above 15\n",
    "bins = list(range(1, 16)) + [15.5]\n",
    "labels = list(range(1, 15)) + ['15+']\n",
    "plot_data = hdbscan_all['n_clust'].apply(lambda x: x if x <= 15 else 15.5)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(plot_data, bins=bins, edgecolor='black', align='left', rwidth=0.8)\n",
    "plt.xticks(bins[:-1], labels)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Number of Models')\n",
    "plt.title('Number of clusters selected by HDBSCAN models')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d64ccb-95d1-4cb2-8897-31d96b8ab6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting one of the best models for n=2\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data_n)\n",
    "\n",
    "hdb = HDBSCAN(metric = 'euclidean', min_cluster_size = 4, min_samples = 2)  \n",
    "pred_clust = hdb.fit_predict(data)\n",
    "n_clusters = len(set(pred_clust[pred_clust != -1]))\n",
    "\n",
    "# Plotting datapoints and clusters in 2D space\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "## PCA to define the 2D space\n",
    "pca = PCA(n_components=2)\n",
    "reduced_space = pca.fit_transform(data)\n",
    "\n",
    "## Create hulls around clusters\n",
    "hull_vertices = []\n",
    "hull_colors = []\n",
    "for i in range(n_clusters):\n",
    "    cluster_points = reduced_space[pred_clust == i]\n",
    "    if len(cluster_points) > 2:\n",
    "        hull = ConvexHull(cluster_points)\n",
    "        hull_vertices.append((\n",
    "            cluster_points[hull.vertices, 0],\n",
    "            cluster_points[hull.vertices, 1]\n",
    "        ))\n",
    "        hull_colors.append(i)\n",
    "\n",
    "## Plot datapoints\n",
    "scatter = plt.scatter(reduced_space[:, 0], reduced_space[:, 1], \n",
    "                     c=pred_clust, cmap='tab10', \n",
    "                     s=15, edgecolors='k')\n",
    "\n",
    "## Plot hulls using the same colormap\n",
    "for vertices, i in zip(hull_vertices, hull_colors):\n",
    "    plt.fill(vertices[0], vertices[1], \n",
    "             alpha=0.7,\n",
    "             color=scatter.cmap(scatter.norm(i)))\n",
    "\n",
    "legend = plt.legend(*scatter.legend_elements())\n",
    "plt.xlabel(\"Dim 1\")\n",
    "plt.ylabel(\"Dim 2\")\n",
    "plt.title(\"Clusters with Convex Hulls\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697f076e-6827-4bb5-8398-80c9a5151938",
   "metadata": {},
   "source": [
    "# Aggregate and display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44f939c-246a-4741-a7c1-810f1aaf35bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mod_list = [kmeans_best, ahc_best, hdbscan_best]\n",
    "best_models = pd.concat(best_mod_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5961a1-7104-4748-8390-554a430ca628",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb391ae1-da07-4c1e-bc6f-47bea22d93e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the best performing model on each criteria across model classes eliminates hdbscan models\n",
    "# Which could mean hdbscan is underperforming\n",
    "# Or is picking non-convex clusters\n",
    "# Or that data is non-clusterable!\n",
    "best_mod_list = [kmeans_best, ahc_best, hdbscan_best]\n",
    "best_models = pd.concat(best_mod_list, ignore_index=True)\n",
    "\n",
    "best_silhouette = best_models.sort_values('silhouette', ascending=False).iloc[0]\n",
    "best_ch = best_models.sort_values('calinski_harabasz', ascending=False).iloc[0]\n",
    "best_db = best_models.sort_values('davies_bouldin', ascending=True).iloc[0]\n",
    "best_dunn = best_models.sort_values('dunn', ascending=False).iloc[0]\n",
    "best_inertia = best_models.sort_values('inertia', ascending=False).iloc[0]\n",
    "\n",
    "best_models = pd.DataFrame([best_silhouette, best_ch, best_db, best_dunn, best_inertia])\n",
    "best_models = best_models.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42faee4a-4cee-41d1-8bcd-3ab5a699f1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a79babe-8f25-491a-a288-df2da0cff964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "bins = np.arange(best_models['n_clust'].min() - 0.5, best_models['n_clust'].max() + 1.5, 1)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(best_models['n_clust'], bins=bins, edgecolor='black', rwidth=0.8)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Number of Models')\n",
    "plt.title('Optimal number of clusters according to best models')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf4f29e-94cf-45df-b915-ee88922d7570",
   "metadata": {},
   "source": [
    "# Clusters visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbabdc5c-3473-4a8f-8338-d4b5d2959e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA to represent the clusters in 2D\n",
    "pca = PCA(n_components=2)\n",
    "X_reduced = pca.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185b683a-3576-42be-8cd3-1b042ea88870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit an arbitrary kmeans model\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data_n)\n",
    "\n",
    "kmeans = KMeans(n_clusters=7, random_state=42)\n",
    "pred_clust = kmeans.fit_predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66241561-c493-43ab-b55c-0b21e0d88bcb",
   "metadata": {},
   "source": [
    "### Datapoints alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016a8155-394c-43f1-9094-f6b9506bc506",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=pred_clust, cmap='tab10', s=20, edgecolors='k')\n",
    "plt.xlabel(\"Dim 1\")\n",
    "plt.ylabel(\"Dim 2\")\n",
    "plt.axhline(y=0, color='#333333', linestyle='--', linewidth=1)\n",
    "plt.axvline(x=0, color='#333333', linestyle='--', linewidth=1)\n",
    "plt.title(\"Clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc86e037-57fc-4e66-8c18-9decd265bb6b",
   "metadata": {},
   "source": [
    "### With decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b7bdaa-210e-4c32-93c2-b33373851eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid for boundary visualization in 2D space\n",
    "x_min, x_max = X_reduced[:, 0].min() - 0.5, X_reduced[:, 0].max() + 0.5\n",
    "y_min, y_max = X_reduced[:, 1].min() - 0.5, X_reduced[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300), np.linspace(y_min, y_max, 300))\n",
    "\n",
    "# Project grid points back to original space\n",
    "grid_points_2D = np.c_[xx.ravel(), yy.ravel()]\n",
    "grid_points_original = pca.inverse_transform(grid_points_2D)\n",
    "\n",
    "# Predict clusters in the original space\n",
    "grid_clusters = kmeans.predict(grid_points_original).reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create scatter plot first to get the color mapping\n",
    "scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], \n",
    "                     c=pred_clust, cmap='tab10', \n",
    "                     s=15, edgecolors='k')\n",
    "\n",
    "# Plot boundaries using the same colormap and normalization\n",
    "plt.contourf(xx, yy, grid_clusters, \n",
    "             alpha=0.3, \n",
    "             cmap=scatter.cmap,\n",
    "             norm=scatter.norm)\n",
    "\n",
    "# Plot centroids with labels\n",
    "centroids_pca = pca.transform(kmeans.cluster_centers_)\n",
    "for i, (x, y) in enumerate(centroids_pca):\n",
    "    plt.text(x, y, str(i), color='white', fontsize=12, \n",
    "             ha='center', va='center', fontweight='bold',\n",
    "             bbox=dict(facecolor='black', edgecolor='none', boxstyle='round,pad=0.2'))\n",
    "\n",
    "plt.xlabel(\"Dim 1\")\n",
    "plt.ylabel(\"Dim 2\")\n",
    "plt.title(\"Clusters with Decision Boundaries\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fe364d-5884-412d-8e14-688a268096d7",
   "metadata": {},
   "source": [
    "### With convex hulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662b1ff5-b4d8-4a6e-bdf9-b09d7756ba4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Collect all hull vertices\n",
    "hull_vertices = []\n",
    "hull_colors = []\n",
    "for i in range(kmeans.n_clusters):\n",
    "    cluster_points = X_reduced[pred_clust == i]\n",
    "    if len(cluster_points) > 2:\n",
    "        hull = ConvexHull(cluster_points)\n",
    "        hull_vertices.append((\n",
    "            cluster_points[hull.vertices, 0],\n",
    "            cluster_points[hull.vertices, 1]\n",
    "        ))\n",
    "        hull_colors.append(i)\n",
    "\n",
    "# Plot datapoints\n",
    "scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], \n",
    "                     c=pred_clust, cmap='tab10', \n",
    "                     s=15, edgecolors='k')\n",
    "\n",
    "# Plot all hulls using the same colormap\n",
    "for vertices, i in zip(hull_vertices, hull_colors):\n",
    "    plt.fill(vertices[0], vertices[1], \n",
    "             alpha=0.3,\n",
    "             color=scatter.cmap(scatter.norm(i)))\n",
    "\n",
    "legend = plt.legend(*scatter.legend_elements())\n",
    "plt.xlabel(\"Dim 1\")\n",
    "plt.ylabel(\"Dim 2\")\n",
    "plt.title(\"Clusters with Convex Hulls\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91d0f2f5-49b2-4e43-908f-142e8f3ef340",
   "metadata": {},
   "source": [
    "# Hopkins Statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19619b2d-75a6-4033-ac85-8edbcf35c8de",
   "metadata": {},
   "source": [
    "Function from the pyclustertend package, which could not be installed because its depencies are outdated.\n",
    "See: https://pyclustertend.readthedocs.io/en/latest/_modules/pyclustertend/hopkins.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabe45e4-854e-4523-baa7-f6e03b471f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "def hopkins(data_frame, sampling_size):\n",
    "    \"\"\"Assess the clusterability of a dataset. A score between 0 and 1, a score around 0.5 express\n",
    "    no clusterability and a score tending to 0 express a high cluster tendency.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_frame : numpy array\n",
    "        The input dataset\n",
    "    sampling_size : int\n",
    "        The sampling size which is used to evaluate the number of DataFrame.\n",
    "\n",
    "    Returns\n",
    "    ---------------------\n",
    "    score : float\n",
    "        The hopkins score of the dataset (between 0 and 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    if type(data_frame) == np.ndarray:\n",
    "        data_frame = pd.DataFrame(data_frame)\n",
    "\n",
    "    # Sample n observations from D:P\n",
    "    if sampling_size > data_frame.shape[0]:\n",
    "        raise Exception(\n",
    "            'The number of sample of sample is bigger than the shape of D')\n",
    "\n",
    "    data_frame_sample = data_frame.sample(n=sampling_size)\n",
    "\n",
    "    # Get the distance to their neirest neighbors in D:X\n",
    "    tree = BallTree(data_frame, leaf_size=2)\n",
    "    dist, _ = tree.query(data_frame_sample, k=2)\n",
    "    data_frame_sample_distances_to_nearest_neighbours = dist[:, 1]\n",
    "\n",
    "    # Randomly simulate n points with the same variation as in D:Q\n",
    "    max_data_frame = data_frame.max()\n",
    "    min_data_frame = data_frame.min()\n",
    "\n",
    "    uniformly_selected_values_0 = np.random.uniform(min_data_frame[0], max_data_frame[0], sampling_size)\n",
    "    uniformly_selected_values_1 = np.random.uniform(min_data_frame[1], max_data_frame[1], sampling_size)\n",
    "\n",
    "    uniformly_selected_observations = np.column_stack((uniformly_selected_values_0, uniformly_selected_values_1))\n",
    "    if len(max_data_frame) >= 2:\n",
    "        for i in range(2, len(max_data_frame)):\n",
    "            uniformly_selected_values_i = np.random.uniform(min_data_frame[i], max_data_frame[i], sampling_size)\n",
    "            to_stack = (uniformly_selected_observations, uniformly_selected_values_i)\n",
    "            uniformly_selected_observations = np.column_stack(to_stack)\n",
    "\n",
    "    uniformly_selected_observations_df = pd.DataFrame(uniformly_selected_observations)\n",
    "\n",
    "    # Get the distance to their neirest neighbors in D:Y\n",
    "    tree = BallTree(data_frame, leaf_size=2)\n",
    "    dist, _ = tree.query(uniformly_selected_observations_df, k=1)\n",
    "    uniformly_df_distances_to_nearest_neighbours = dist\n",
    "\n",
    "    # Return the hopkins score\n",
    "    x = sum(data_frame_sample_distances_to_nearest_neighbours)\n",
    "    y = sum(uniformly_df_distances_to_nearest_neighbours)\n",
    "\n",
    "    if x + y == 0:\n",
    "        raise Exception('The denominator of the hopkins statistics is null')\n",
    "\n",
    "    return x / (x + y)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fc5cdf-91b2-48ed-bef2-0a9c0fcfc470",
   "metadata": {},
   "outputs": [],
   "source": [
    "float(hopkins(data_n.values, data_n.shape[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
