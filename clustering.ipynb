{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676fe747-bd8b-4115-83bc-0a18c2c9cd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torchmetrics\n",
    "# pip install stepmix\n",
    "# pip install kneed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4887e505-68fb-40c0-adb5-06ef3199e312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "from joblib import Parallel, delayed # for parallelization\n",
    "from itertools import product\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, HDBSCAN\n",
    "from stepmix.stepmix import StepMix\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import torch\n",
    "from torchmetrics.clustering import DunnIndex\n",
    "from kneed import KneeLocator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d1e61c-3574-4482-9529-59268b7fb8d6",
   "metadata": {},
   "source": [
    "# Data, parameters and validity indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1190dd79-e4b5-4684-98b0-0f6ce7797686",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2004_i = pd.read_parquet(\"data/data2004_i.parquet\") # load imputed data\n",
    "\n",
    "# Dataset with numeric outcomes\n",
    "data_n = data2004_i[[\n",
    "    'clseusa_n', 'ambornin_n', 'amcit_n', 'amlived_n', 'amenglsh_n', \n",
    "     'amchrstn_n', 'amgovt_n', 'amfeel_n', 'amcitizn_n', 'amshamed_n', \n",
    "     'belikeus_n', 'ambetter_n', 'ifwrong_n', 'proudsss_n', 'proudgrp_n', \n",
    "     'proudpol_n', 'prouddem_n', 'proudeco_n', 'proudspt_n', 'proudart_n', \n",
    "     'proudhis_n', 'proudmil_n', 'proudsci_n']]\n",
    "\n",
    "# Dataset with categorical outcomes\n",
    "data_f = data2004_i[[\n",
    "     'clseusa_f', 'ambornin_f', 'amcit_f', 'amlived_f', 'amenglsh_f', \n",
    "     'amchrstn_f', 'amgovt_f', 'amfeel_f', 'amcitizn_f', 'amshamed_f', \n",
    "     'belikeus_f', 'ambetter_f', 'ifwrong_f', 'proudsss_f', 'proudgrp_f', \n",
    "     'proudpol_f', 'prouddem_f', 'proudeco_f', 'proudspt_f', 'proudart_f', \n",
    "     'proudhis_f', 'proudmil_f', 'proudsci_f']]\n",
    "\n",
    "# Dataset with controls\n",
    "controls = data2004_i[[\n",
    "    'sex', 'race_f', 'born_usa', 'party_fs', 'religstr_f', \n",
    "    'reltrad_f', 'region_f']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c55b24ac-f239-4dea-9283-e3491ec65ee5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_clust = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01c84df2-d928-437c-b4f8-200cc8b0d208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom score functions to avoid throwing errors when undefined\n",
    "def sil_score(data, pred_clust):\n",
    "    try:\n",
    "        sil_score = silhouette_score(data, pred_clust)\n",
    "    except ValueError:\n",
    "        sil_score = np.nan\n",
    "    return sil_score\n",
    "\n",
    "def ch_score(data, pred_clust):\n",
    "    try:\n",
    "        ch_score = calinski_harabasz_score(data, pred_clust)\n",
    "    except ValueError:\n",
    "        ch_score = np.nan\n",
    "    return ch_score\n",
    "\n",
    "def db_score(data, pred_clust):\n",
    "    try:\n",
    "        db_score = davies_bouldin_score(data, pred_clust)\n",
    "    except ValueError:\n",
    "        db_score = np.nan\n",
    "    return db_score\n",
    "\n",
    "def dunn_score(data, pred_clust):\n",
    "    torch_data = np.array(data)\n",
    "    torch_data = torch.tensor(torch_data, dtype=torch.float32)\n",
    "    torch_pred_clust = torch.tensor(pred_clust, dtype=torch.int64)\n",
    "\n",
    "    dunn_metric = DunnIndex()\n",
    "    \n",
    "    try:\n",
    "        dunn_score = float(dunn_metric(torch_data, torch_pred_clust))\n",
    "    except Exception:\n",
    "        dunn_score = np.nan\n",
    " \n",
    "    return dunn_score\n",
    "\n",
    "def inertia(data, labels):\n",
    "    data = np.asarray(data)\n",
    "    \n",
    "    inertia = 0\n",
    "    for cluster in np.unique(labels):\n",
    "        cluster_points = data[labels == cluster]\n",
    "        cluster_centroid = np.mean(cluster_points, axis=0)\n",
    "        inertia += np.sum((cluster_points - cluster_centroid) ** 2)\n",
    "        \n",
    "    return inertia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1513b5bc-963b-41e5-a6bd-12f45f22c2f5",
   "metadata": {},
   "source": [
    "# Latent models\n",
    "With the StepMix package\n",
    "\n",
    "Documentation : https://github.com/Labo-Lacourse/stepmix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c78aabed-4cb1-4e87-a35f-797240d34b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_range = range(1, max_clust)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320b6ebb-5f19-48f5-835e-444d8d2cc759",
   "metadata": {},
   "source": [
    "## Without covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e852e72e-ca78-477f-84a7-de05f2580e32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting StepMix...\n",
      "Fitting StepMix...\n",
      "Fitting StepMix...\n",
      "Fitting StepMix...\n",
      "Fitting StepMix...\n",
      "Fitting StepMix...\n",
      "Fitting StepMix...\n",
      "Fitting StepMix...\n",
      "Fitting StepMix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializations (n_init) : 100%|██████████| 3/3 [00:00<00:00, 322.85it/s, max_LL=-3.07e+4, max_avg_LL=-25.3]\n",
      "Initializations (n_init) : 100%|██████████| 3/3 [00:00<00:00, 17.86it/s, max_LL=-2.89e+4, max_avg_LL=-23.8]\n",
      "Initializations (n_init) : 100%|██████████| 3/3 [00:00<00:00,  9.04it/s, max_LL=-2.82e+4, max_avg_LL=-23.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting StepMix...\n",
      "Fitting StepMix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializations (n_init) : 100%|██████████| 3/3 [00:00<00:00,  6.89it/s, max_LL=-2.71e+4, max_avg_LL=-22.3]\n",
      "Initializations (n_init) : 100%|██████████| 3/3 [00:00<00:00,  4.83it/s, max_LL=-2.77e+4, max_avg_LL=-22.8]\n",
      "Initializations (n_init) : 100%|██████████| 3/3 [00:00<00:00,  3.30it/s, max_LL=-2.74e+4, max_avg_LL=-22.5]\n",
      "Initializations (n_init) : 100%|██████████| 3/3 [00:01<00:00,  2.72it/s, max_LL=-2.65e+4, max_avg_LL=-21.8]\n",
      "Initializations (n_init) : 100%|██████████| 3/3 [00:01<00:00,  1.79it/s, max_LL=-2.67e+4, max_avg_LL=-22]8]\n",
      "Initializations (n_init) : 100%|██████████| 3/3 [00:02<00:00,  1.19it/s, max_LL=-2.68e+4, max_avg_LL=-22.1]\n",
      "Initializations (n_init) : 100%|██████████| 3/3 [00:02<00:00,  1.14it/s, max_LL=-2.69e+4, max_avg_LL=-22.2]\n",
      "Initializations (n_init) : 100%|██████████| 3/3 [00:02<00:00,  1.19it/s, max_LL=-2.64e+4, max_avg_LL=-21.8]\n",
      "Initializations (n_init) : 100%|██████████| 3/3 [00:00<00:00, 1660.23it/s, max_LL=-3.92e+4, max_avg_LL=-32.2]\n",
      "Initializations (n_init) : 100%|██████████| 3/3 [00:00<00:00, 71.72it/s, max_LL=-2.6e+4, max_avg_LL=-21.4]\n",
      "Initializations (n_init) : 100%|██████████| 3/3 [00:00<00:00, 64.18it/s, max_LL=-2.1e+4, max_avg_LL=-17.3]\n",
      "Initializations (n_init) : 100%|██████████| 3/3 [00:00<00:00, 56.62it/s, max_LL=-1.7e+4, max_avg_LL=-14]\n",
      "Initializations (n_init) : 100%|██████████| 3/3 [00:00<00:00, 43.01it/s, max_LL=-1.55e+4, max_avg_LL=-12.8]\n",
      "Initializations (n_init) : 100%|██████████| 3/3 [00:00<00:00, 36.92it/s, max_LL=-9.15e+3, max_avg_LL=-7.53]\n",
      "Initializations (n_init) : 100%|██████████| 3/3 [00:00<00:00, 30.11it/s, max_LL=-9.08e+3, max_avg_LL=-7.48]\n",
      "Initializations (n_init) : 100%|██████████| 3/3 [00:00<00:00, 27.54it/s, max_LL=-1.06e+4, max_avg_LL=-8.75]\n",
      "Initializations (n_init) : 100%|██████████| 3/3 [00:00<00:00, 22.23it/s, max_LL=-1.44e+4, max_avg_LL=-11.9]\n",
      "Initializations (n_init) : 100%|██████████| 3/3 [00:00<00:00, 32.09it/s, max_LL=-1.11e+4, max_avg_LL=-9.16]\n",
      "Initializations (n_init) : 100%|██████████| 3/3 [00:00<00:00, 28.62it/s, max_LL=-5.65e+3, max_avg_LL=-4.65]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting StepMix...\n",
      "Fitting StepMix...\n",
      "Fitting StepMix...\n",
      "Fitting StepMix...\n",
      "Fitting StepMix...\n",
      "Fitting StepMix...\n",
      "Fitting StepMix...\n",
      "Fitting StepMix...\n",
      "Fitting StepMix...\n",
      "Fitting StepMix...\n",
      "Fitting StepMix...\n"
     ]
    }
   ],
   "source": [
    "def do_StepMix(n, type, data):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "\n",
    "        model = StepMix(\n",
    "            n_components = n, \n",
    "            measurement = type, \n",
    "            n_init = 3)\n",
    "        \n",
    "        model.fit(data)\n",
    "        pred_clust = model.predict(data)\n",
    "\n",
    "        return {\n",
    "            'model': 'LCA' if type == 'categorical' else 'LPA',\n",
    "            'params': 'no covariates',\n",
    "            'n_clust': n,\n",
    "            'aic': model.aic(data),\n",
    "            'bic': model.bic(data),\n",
    "            'silhouette': sil_score(data, pred_clust),\n",
    "            'calinski_harabasz': ch_score(data, pred_clust),\n",
    "            'davies_bouldin': db_score(data, pred_clust),\n",
    "            'dunn': dunn_score(data, pred_clust),\n",
    "            'inertia': inertia(data, pred_clust)\n",
    "        }\n",
    "\n",
    "data = data_f.apply(lambda col: LabelEncoder().fit_transform(col))\n",
    "cat_res = Parallel(n_jobs=8)(delayed(do_StepMix)(n, 'categorical', data) for n in clust_range)\n",
    "LCA_res = pd.DataFrame(cat_res)\n",
    "\n",
    "num_res = Parallel(n_jobs=8)(delayed(do_StepMix)(n, 'continuous', data_n) for n in clust_range)\n",
    "LPA_res = pd.DataFrame(num_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c2fb00-c16f-4869-8334-4aaa1a193de9",
   "metadata": {},
   "source": [
    "## With covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d063ae3-82a3-4853-9d38-15418f0cfb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_StepMix_covar(n, type, data):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "\n",
    "        model = StepMix(\n",
    "            n_components = n, \n",
    "            measurement = type, \n",
    "            n_init = 3,\n",
    "            n_steps = 1,\n",
    "            structural = 'covariate', \n",
    "            structural_params = opt_params,\n",
    "            init_params = 'kmeans',\n",
    "            random_state = 123)\n",
    "        \n",
    "        model.fit(data, controls_dum)\n",
    "        pred_clust = model.predict(data)\n",
    "\n",
    "        return {\n",
    "            'model': 'LCA' if type == 'categorical' else 'LPA',\n",
    "            'params': 'with covariates',\n",
    "            'n_clust': n,\n",
    "            'aic': model.aic(data),\n",
    "            'bic': model.bic(data),\n",
    "            'silhouette': sil_score(data, pred_clust),\n",
    "            'calinski_harabasz': ch_score(data, pred_clust),\n",
    "            'davies_bouldin': db_score(data, pred_clust),\n",
    "            'dunn': dunn_score(data, pred_clust),\n",
    "            'inertia': inertia(data, pred_clust)\n",
    "        }\n",
    "\n",
    "opt_params = {\n",
    "    'method': 'gradient',\n",
    "    'intercept': True,\n",
    "    'max_iter': 2500,\n",
    "}\n",
    "\n",
    "controls_dum = pd.get_dummies(controls)\n",
    "\n",
    "data = data_f.apply(lambda col: LabelEncoder().fit_transform(col))\n",
    "cat_res = Parallel(n_jobs=8)(delayed(do_StepMix_covar)(n, 'categorical', data) for n in clust_range)\n",
    "LCA_covar_res = pd.DataFrame(cat_res)\n",
    "\n",
    "num_res = Parallel(n_jobs=8)(delayed(do_StepMix_covar)(n, 'continuous', data_n) for n in clust_range)\n",
    "LPA_covar_res = pd.DataFrame(num_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d8e1dc-5d1b-415a-b51f-a7b75be5ca46",
   "metadata": {},
   "source": [
    "## Best latent models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e6a8b3-eaad-4b81-a4cf-19367debf6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best models according to absolute fit = min aic / bic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939d848b-1152-4782-b6a3-f2d06db9933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best models according to relative fit = LRT / BLRT / BVR (LCA only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d893bba-c892-432d-9cb0-85d85acd6304",
   "metadata": {},
   "source": [
    "# K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40c7df35-9998-4899-aefe-049e80a984b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data_n)\n",
    "\n",
    "results = []\n",
    "\n",
    "def do_kmeans(n): \n",
    "    kmeans = KMeans(\n",
    "        n_clusters = n, \n",
    "        init = 'k-means++', \n",
    "        n_init = 25,\n",
    "        random_state=42)\n",
    "\n",
    "    pred_clust = kmeans.fit_predict(data)\n",
    "            \n",
    "    return{\n",
    "        'model': 'kmeans',\n",
    "        'params': 'centroid',\n",
    "        'n_clust': n,\n",
    "        'silhouette': sil_score(data, pred_clust),\n",
    "        'calinski_harabasz': ch_score(data, pred_clust),\n",
    "        'davies_bouldin': db_score(data, pred_clust),\n",
    "        'dunn': dunn_score(data, pred_clust),\n",
    "        'inertia': inertia(data, pred_clust)\n",
    "    }   \n",
    "\n",
    "clust_range = range(1, max_clust)\n",
    "\n",
    "results = Parallel(n_jobs=8)(delayed(do_kmeans)(n) for n in clust_range)\n",
    "\n",
    "kmeans_res = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d088363-80ec-475d-a431-4d72d78d9e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add other models, which are not implemented in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4aefa7f6-75a4-43e1-9b09-6e9379f34a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_silhouette = kmeans_res.sort_values('silhouette', ascending=False).iloc[0]\n",
    "best_calinski = kmeans_res.sort_values('calinski_harabasz', ascending=False).iloc[0]\n",
    "best_davies = kmeans_res.sort_values('davies_bouldin', ascending=True).iloc[0] # Lower is better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24849e55-57cb-4680-9a5b-c8951503d389",
   "metadata": {},
   "source": [
    "# AHC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b57b6080-574e-4d0a-bcf3-209d3284dfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data_n)\n",
    "\n",
    "results = []\n",
    "\n",
    "def do_AHC(n, dist, linkage):\n",
    "    ahc = AgglomerativeClustering(\n",
    "        n_clusters = n,\n",
    "        metric = dist,\n",
    "        linkage = linkage)\n",
    "    \n",
    "    ahc.fit(data)\n",
    "    \n",
    "    pred_clust = ahc.labels_\n",
    "\n",
    "    return {\n",
    "        'model': 'AHC',\n",
    "        'params': f\"distance = {dist}, linkage = {linkage}\",\n",
    "        'n_clust': n,\n",
    "        'silhouette': sil_score(data, pred_clust),\n",
    "        'calinski_harabasz': ch_score(data, pred_clust),\n",
    "        'davies_bouldin': db_score(data, pred_clust),\n",
    "        'dunn': dunn_score(data, pred_clust),\n",
    "        'inertia': inertia(data, pred_clust)\n",
    "    }\n",
    "\n",
    "clust_range = range(1, max_clust)\n",
    "distances = ['manhattan', 'euclidean', 'chebyshev']\n",
    "linkages = ['single', 'average', 'complete']\n",
    "params = product(clust_range, distances, linkages)\n",
    "\n",
    "results = Parallel(n_jobs=8)(delayed(do_AHC)(n, dist, linkage) for n, dist, linkage in params)\n",
    "\n",
    "results.extend([do_AHC(n, 'euclidean', 'ward') for n in clust_range])\n",
    "\n",
    "ahc_res = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf5a07e-2a44-4e5e-826d-7a2ac4b6fe57",
   "metadata": {},
   "source": [
    "# HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c8b6bc8-18fe-44b9-bd27-26a30c706672",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data_n)\n",
    "\n",
    "results = []\n",
    "\n",
    "def do_hdbscan(dist, min_c, min_s):\n",
    "    hdb = HDBSCAN(\n",
    "        metric = dist,\n",
    "        min_cluster_size = min_c, \n",
    "        min_samples = min_s)\n",
    "        \n",
    "    pred_clust = hdb.fit_predict(data)\n",
    "        \n",
    "    n_clusters = len(set(pred_clust[pred_clust != -1]))\n",
    "    noise_freq = 100 * sum(pred_clust == -1) / len(pred_clust)\n",
    "        \n",
    "    return {\n",
    "        'model': 'HDBSCAN',\n",
    "        'params': f\"distance = {dist}, min_cluster_size = {min_c}, min_samples = {min_s}\",\n",
    "        'n_clust': n_clusters,\n",
    "        'noise': noise_freq,\n",
    "        'silhouette': sil_score(data, pred_clust),\n",
    "        'calinski_harabasz': ch_score(data, pred_clust),\n",
    "        'davies_bouldin': db_score(data, pred_clust),\n",
    "        'dunn': dunn_score(data, pred_clust),\n",
    "        'inertia': inertia(data, pred_clust)\n",
    "    }\n",
    "\n",
    "distances = ['euclidean', 'chebyshev']\n",
    "min_cluster_sizes = range(2, 16)\n",
    "min_samples_range = range(1, 16)\n",
    "params = product(distances, min_cluster_sizes, min_samples_range)\n",
    "\n",
    "results = Parallel(n_jobs=8)(delayed(do_hdbscan)(dist, min_c, min_s) for dist, min_c, min_s in params)\n",
    "\n",
    "hdbscan_res = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968b43fd-3261-4b35-885b-679ae6215117",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_silhouette = hdbscan_res.sort_values('silhouette', ascending=False).iloc[0]\n",
    "best_calinski = hdbscan_res.sort_values('calinski_harabasz', ascending=False).iloc[0]\n",
    "best_davies = hdbscan_res.sort_values('davies_bouldin', ascending=True).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dda7a35-e8f0-49df-b5d0-bfaf4148749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_res['n_clust'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697f076e-6827-4bb5-8398-80c9a5151938",
   "metadata": {},
   "source": [
    "# Aggregate and display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c44f939c-246a-4741-a7c1-810f1aaf35bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = [LCA_res, LPA_res, kmeans_res, ahc_res, hdbscan_res]\n",
    "# add LCA_covar_res and LPA_covar_res for the final run\n",
    "\n",
    "combined_res = pd.concat(res_df, ignore_index=True)\n",
    "combined_res = combined_res.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6872ed4-ddbd-44cd-8966-67083c19f0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the optimal numbers of clutsters according to validity indexes\n",
    "def elbow_plot(df, val_index):\n",
    "    res = df.dropna(subset=[val_index])\n",
    "\n",
    "    x = res[\"n_clust\"]\n",
    "    y = res[val_index]\n",
    "\n",
    "    if metric == 'davies_bouldin':\n",
    "        knee_locator = KneeLocator(x, y, curve='concave', direction='increasing')\n",
    "    else:\n",
    "        knee_locator = KneeLocator(x, y, curve='convex', direction='decreasing')\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(x, y, marker=\"o\", linestyle=\"-\", label=val_index)\n",
    "    plt.axvline(x=knee_locator.knee, color=\"r\", linestyle=\"--\", label=f\"Optimal k={knee_locator.knee}\")\n",
    "    plt.xlabel(\"Number of Clusters\")\n",
    "    plt.ylabel(f\"{val_index} index\")\n",
    "    plt.title(f\"Elbow Method for {val_index} index\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "for val_index in ('silhouette','calinski_harabasz', 'davies_bouldin', 'dunn', 'inertia'):\n",
    "    elbow_plot(LPA_res, val_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8ba048e-140b-4746-9b8a-a9e10c2d3ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best models according to validity indexes\n",
    "best_models = pd.DataFrame()\n",
    "\n",
    "def elbow_method(df, val_index):\n",
    "    res = df.dropna(subset=[val_index])\n",
    "\n",
    "    x = res[\"n_clust\"]\n",
    "    y = res[val_index]\n",
    "\n",
    "    if val_index == 'davies_bouldin':\n",
    "        knee_locator = KneeLocator(x, y, curve='concave', direction='increasing')\n",
    "    else:\n",
    "        knee_locator = KneeLocator(x, y, curve='convex', direction='decreasing')\n",
    "    \n",
    "    return res[res[\"n_clust\"] == knee_locator.knee]\n",
    "\n",
    "models = [LCA_res, LPA_res, kmeans_res, ahc_res, hdbscan_res]\n",
    "val_indexes = ['silhouette', 'calinski_harabasz', 'davies_bouldin', 'dunn', 'inertia']\n",
    "params = product(models, val_indexes)\n",
    "\n",
    "for model, val_index in params:\n",
    "    best_model = elbow_method(model, val_index)\n",
    "    best_models = pd.concat([best_models, best_model], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a552745a-531b-40fa-9642-2f173cf936ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should work in 2 steps:\n",
    "## apply knee locator to each unique combination of model and params\n",
    "## then keep the best model, for each validity indexes, for each class of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6b617ab-3a8c-4746-9c77-e0e8bcabe47f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>params</th>\n",
       "      <th>n_clust</th>\n",
       "      <th>aic</th>\n",
       "      <th>bic</th>\n",
       "      <th>silhouette</th>\n",
       "      <th>calinski_harabasz</th>\n",
       "      <th>davies_bouldin</th>\n",
       "      <th>dunn</th>\n",
       "      <th>inertia</th>\n",
       "      <th>noise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LCA</td>\n",
       "      <td>no covariates</td>\n",
       "      <td>3</td>\n",
       "      <td>56955.449635</td>\n",
       "      <td>58373.944456</td>\n",
       "      <td>0.050062</td>\n",
       "      <td>126.102058</td>\n",
       "      <td>3.165817</td>\n",
       "      <td>0.195118</td>\n",
       "      <td>24374.255479</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LCA</td>\n",
       "      <td>no covariates</td>\n",
       "      <td>6</td>\n",
       "      <td>55352.664858</td>\n",
       "      <td>58194.756999</td>\n",
       "      <td>0.031038</td>\n",
       "      <td>71.040141</td>\n",
       "      <td>3.630874</td>\n",
       "      <td>0.224198</td>\n",
       "      <td>22759.576952</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LCA</td>\n",
       "      <td>no covariates</td>\n",
       "      <td>4</td>\n",
       "      <td>56157.435618</td>\n",
       "      <td>58050.462879</td>\n",
       "      <td>0.050236</td>\n",
       "      <td>104.346795</td>\n",
       "      <td>3.006889</td>\n",
       "      <td>0.228913</td>\n",
       "      <td>23397.961936</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LPA</td>\n",
       "      <td>no covariates</td>\n",
       "      <td>5</td>\n",
       "      <td>31566.696483</td>\n",
       "      <td>32760.681332</td>\n",
       "      <td>0.007748</td>\n",
       "      <td>64.007200</td>\n",
       "      <td>4.076536</td>\n",
       "      <td>0.152632</td>\n",
       "      <td>24463.602761</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LPA</td>\n",
       "      <td>no covariates</td>\n",
       "      <td>3</td>\n",
       "      <td>42378.838560</td>\n",
       "      <td>43093.188470</td>\n",
       "      <td>0.071582</td>\n",
       "      <td>92.974102</td>\n",
       "      <td>3.560600</td>\n",
       "      <td>0.225306</td>\n",
       "      <td>25697.394253</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>HDBSCAN</td>\n",
       "      <td>distance = chebyshev, min_cluster_size = 15, m...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.052586</td>\n",
       "      <td>57.014071</td>\n",
       "      <td>3.486657</td>\n",
       "      <td>0.178472</td>\n",
       "      <td>25541.946622</td>\n",
       "      <td>60.823045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>HDBSCAN</td>\n",
       "      <td>distance = chebyshev, min_cluster_size = 15, m...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.055658</td>\n",
       "      <td>56.480595</td>\n",
       "      <td>3.389945</td>\n",
       "      <td>0.185127</td>\n",
       "      <td>25562.514767</td>\n",
       "      <td>61.810700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>HDBSCAN</td>\n",
       "      <td>distance = chebyshev, min_cluster_size = 15, m...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.055591</td>\n",
       "      <td>56.415313</td>\n",
       "      <td>3.375179</td>\n",
       "      <td>0.177203</td>\n",
       "      <td>25565.034015</td>\n",
       "      <td>62.469136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>HDBSCAN</td>\n",
       "      <td>distance = chebyshev, min_cluster_size = 15, m...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.062728</td>\n",
       "      <td>54.729870</td>\n",
       "      <td>3.421150</td>\n",
       "      <td>0.166293</td>\n",
       "      <td>25630.247340</td>\n",
       "      <td>63.292181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>HDBSCAN</td>\n",
       "      <td>distance = chebyshev, min_cluster_size = 15, m...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.084138</td>\n",
       "      <td>53.771996</td>\n",
       "      <td>3.437310</td>\n",
       "      <td>0.140325</td>\n",
       "      <td>25667.458022</td>\n",
       "      <td>62.716049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>243 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       model                                             params  n_clust  \\\n",
       "0        LCA                                      no covariates        3   \n",
       "1        LCA                                      no covariates        6   \n",
       "2        LCA                                      no covariates        4   \n",
       "3        LPA                                      no covariates        5   \n",
       "4        LPA                                      no covariates        3   \n",
       "..       ...                                                ...      ...   \n",
       "238  HDBSCAN  distance = chebyshev, min_cluster_size = 15, m...        2   \n",
       "239  HDBSCAN  distance = chebyshev, min_cluster_size = 15, m...        2   \n",
       "240  HDBSCAN  distance = chebyshev, min_cluster_size = 15, m...        2   \n",
       "241  HDBSCAN  distance = chebyshev, min_cluster_size = 15, m...        2   \n",
       "242  HDBSCAN  distance = chebyshev, min_cluster_size = 15, m...        2   \n",
       "\n",
       "              aic           bic  silhouette  calinski_harabasz  \\\n",
       "0    56955.449635  58373.944456    0.050062         126.102058   \n",
       "1    55352.664858  58194.756999    0.031038          71.040141   \n",
       "2    56157.435618  58050.462879    0.050236         104.346795   \n",
       "3    31566.696483  32760.681332    0.007748          64.007200   \n",
       "4    42378.838560  43093.188470    0.071582          92.974102   \n",
       "..            ...           ...         ...                ...   \n",
       "238           NaN           NaN   -0.052586          57.014071   \n",
       "239           NaN           NaN   -0.055658          56.480595   \n",
       "240           NaN           NaN   -0.055591          56.415313   \n",
       "241           NaN           NaN   -0.062728          54.729870   \n",
       "242           NaN           NaN   -0.084138          53.771996   \n",
       "\n",
       "     davies_bouldin      dunn       inertia      noise  \n",
       "0          3.165817  0.195118  24374.255479        NaN  \n",
       "1          3.630874  0.224198  22759.576952        NaN  \n",
       "2          3.006889  0.228913  23397.961936        NaN  \n",
       "3          4.076536  0.152632  24463.602761        NaN  \n",
       "4          3.560600  0.225306  25697.394253        NaN  \n",
       "..              ...       ...           ...        ...  \n",
       "238        3.486657  0.178472  25541.946622  60.823045  \n",
       "239        3.389945  0.185127  25562.514767  61.810700  \n",
       "240        3.375179  0.177203  25565.034015  62.469136  \n",
       "241        3.421150  0.166293  25630.247340  63.292181  \n",
       "242        3.437310  0.140325  25667.458022  62.716049  \n",
       "\n",
       "[243 rows x 11 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep unique models\n",
    "best_models.drop_duplicates().reset_index(drop=True)\n",
    "# In the best_models df, have as many columns has performance criteria, and use dummy for each that is maximized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf4f29e-94cf-45df-b915-ee88922d7570",
   "metadata": {},
   "source": [
    "# Clusters visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ffe861-d41d-4a10-94c0-124b32624dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import ConvexHull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbabdc5c-3473-4a8f-8338-d4b5d2959e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA to represent the clusters in 2D\n",
    "pca = PCA(n_components=2)\n",
    "X_reduced = pca.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c907feb6-f378-41bd-a527-46c9749fbc4f",
   "metadata": {},
   "source": [
    "## For kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185b683a-3576-42be-8cd3-1b042ea88870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit an arbitrary model\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data_n)\n",
    "\n",
    "kmeans = KMeans(n_clusters=7, random_state=42)\n",
    "pred_clust = kmeans.fit_predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66241561-c493-43ab-b55c-0b21e0d88bcb",
   "metadata": {},
   "source": [
    "### Datapoints alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016a8155-394c-43f1-9094-f6b9506bc506",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=pred_clust, cmap='tab10', s=20, edgecolors='k')\n",
    "plt.xlabel(\"Dim 1\")\n",
    "plt.ylabel(\"Dim 2\")\n",
    "plt.axhline(y=0, color='#333333', linestyle='--', linewidth=1)\n",
    "plt.axvline(x=0, color='#333333', linestyle='--', linewidth=1)\n",
    "plt.title(\"Clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc86e037-57fc-4e66-8c18-9decd265bb6b",
   "metadata": {},
   "source": [
    "### With decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b7bdaa-210e-4c32-93c2-b33373851eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid for boundary visualization in 2D space\n",
    "x_min, x_max = X_reduced[:, 0].min() - 0.5, X_reduced[:, 0].max() + 0.5\n",
    "y_min, y_max = X_reduced[:, 1].min() - 0.5, X_reduced[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300), np.linspace(y_min, y_max, 300))\n",
    "\n",
    "# Project grid points back to original space\n",
    "grid_points_2D = np.c_[xx.ravel(), yy.ravel()]\n",
    "grid_points_original = pca.inverse_transform(grid_points_2D)\n",
    "\n",
    "# Predict clusters in the original space\n",
    "grid_clusters = kmeans.predict(grid_points_original).reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create scatter plot first to get the color mapping\n",
    "scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], \n",
    "                     c=pred_clust, cmap='tab10', \n",
    "                     s=15, edgecolors='k')\n",
    "\n",
    "# Plot boundaries using the same colormap and normalization\n",
    "plt.contourf(xx, yy, grid_clusters, \n",
    "             alpha=0.3, \n",
    "             cmap=scatter.cmap,\n",
    "             norm=scatter.norm)\n",
    "\n",
    "# Plot centroids with labels\n",
    "centroids_pca = pca.transform(kmeans.cluster_centers_)\n",
    "for i, (x, y) in enumerate(centroids_pca):\n",
    "    plt.text(x, y, str(i), color='white', fontsize=12, \n",
    "             ha='center', va='center', fontweight='bold',\n",
    "             bbox=dict(facecolor='black', edgecolor='none', boxstyle='round,pad=0.2'))\n",
    "\n",
    "plt.xlabel(\"Dim 1\")\n",
    "plt.ylabel(\"Dim 2\")\n",
    "plt.title(\"Clusters with Decision Boundaries\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fe364d-5884-412d-8e14-688a268096d7",
   "metadata": {},
   "source": [
    "### With convex hulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662b1ff5-b4d8-4a6e-bdf9-b09d7756ba4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Collect all hull vertices\n",
    "hull_vertices = []\n",
    "hull_colors = []\n",
    "for i in range(kmeans.n_clusters):\n",
    "    cluster_points = X_reduced[pred_clust == i]\n",
    "    if len(cluster_points) > 2:\n",
    "        hull = ConvexHull(cluster_points)\n",
    "        hull_vertices.append((\n",
    "            cluster_points[hull.vertices, 0],\n",
    "            cluster_points[hull.vertices, 1]\n",
    "        ))\n",
    "        hull_colors.append(i)\n",
    "\n",
    "# Plot datapoints\n",
    "scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], \n",
    "                     c=pred_clust, cmap='tab10', \n",
    "                     s=15, edgecolors='k')\n",
    "\n",
    "# Plot all hulls using the same colormap\n",
    "for vertices, i in zip(hull_vertices, hull_colors):\n",
    "    plt.fill(vertices[0], vertices[1], \n",
    "             alpha=0.3,\n",
    "             color=scatter.cmap(scatter.norm(i)))\n",
    "\n",
    "legend = plt.legend(*scatter.legend_elements())\n",
    "plt.xlabel(\"Dim 1\")\n",
    "plt.ylabel(\"Dim 2\")\n",
    "plt.title(\"Clusters with Convex Hulls\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4dfb1a-a865-4a06-94f9-60777e73b877",
   "metadata": {},
   "source": [
    "## For HDBSCAN\n",
    "Example of non-convex clusters in the PCA space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc36761-34de-4f68-84a2-60faba46d8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit an arbitrary model\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data_n)\n",
    "\n",
    "hdb = HDBSCAN(min_cluster_size = 5, min_samples = 1)  \n",
    "pred_clust = hdb.fit_predict(data)\n",
    "n_clusters = len(set(pred_clust[pred_clust != -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429b6fd7-a4c7-4ec6-8c8d-76937af5c646",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Collect all hull vertices\n",
    "hull_vertices = []\n",
    "hull_colors = []\n",
    "for i in range(n_clusters):\n",
    "    cluster_points = X_reduced[pred_clust == i]\n",
    "    if len(cluster_points) > 2:\n",
    "        hull = ConvexHull(cluster_points)\n",
    "        hull_vertices.append((\n",
    "            cluster_points[hull.vertices, 0],\n",
    "            cluster_points[hull.vertices, 1]\n",
    "        ))\n",
    "        hull_colors.append(i)\n",
    "\n",
    "# Plot datapoints\n",
    "scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], \n",
    "                     c=pred_clust, cmap='tab10', \n",
    "                     s=15, edgecolors='k')\n",
    "\n",
    "# Plot all hulls using the same colormap\n",
    "for vertices, i in zip(hull_vertices, hull_colors):\n",
    "    plt.fill(vertices[0], vertices[1], \n",
    "             alpha=0.7,\n",
    "             color=scatter.cmap(scatter.norm(i)))\n",
    "\n",
    "legend = plt.legend(*scatter.legend_elements())\n",
    "plt.xlabel(\"Dim 1\")\n",
    "plt.ylabel(\"Dim 2\")\n",
    "plt.title(\"Clusters with Convex Hulls\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927a8549-babf-402e-8869-a80f25c416f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
