{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676fe747-bd8b-4115-83bc-0a18c2c9cd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torchmetrics\n",
    "# pip install stepmix\n",
    "# pip install kneed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4887e505-68fb-40c0-adb5-06ef3199e312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "from joblib import Parallel, delayed # for parallelization\n",
    "from itertools import product\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, HDBSCAN\n",
    "from stepmix.stepmix import StepMix\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import torch\n",
    "from torchmetrics.clustering import DunnIndex\n",
    "from kneed import KneeLocator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d1e61c-3574-4482-9529-59268b7fb8d6",
   "metadata": {},
   "source": [
    "# Data, parameters and validity indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1190dd79-e4b5-4684-98b0-0f6ce7797686",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2004_i = pd.read_parquet(\"data/data2004_i.parquet\") # load imputed data\n",
    "\n",
    "# Dataset with numeric outcomes\n",
    "data_n = data2004_i[[\n",
    "    'clseusa_n', 'ambornin_n', 'amcit_n', 'amlived_n', 'amenglsh_n', \n",
    "     'amchrstn_n', 'amgovt_n', 'amfeel_n', 'amcitizn_n', 'amshamed_n', \n",
    "     'belikeus_n', 'ambetter_n', 'ifwrong_n', 'proudsss_n', 'proudgrp_n', \n",
    "     'proudpol_n', 'prouddem_n', 'proudeco_n', 'proudspt_n', 'proudart_n', \n",
    "     'proudhis_n', 'proudmil_n', 'proudsci_n']]\n",
    "\n",
    "# Dataset with categorical outcomes\n",
    "data_f = data2004_i[[\n",
    "     'clseusa_f', 'ambornin_f', 'amcit_f', 'amlived_f', 'amenglsh_f', \n",
    "     'amchrstn_f', 'amgovt_f', 'amfeel_f', 'amcitizn_f', 'amshamed_f', \n",
    "     'belikeus_f', 'ambetter_f', 'ifwrong_f', 'proudsss_f', 'proudgrp_f', \n",
    "     'proudpol_f', 'prouddem_f', 'proudeco_f', 'proudspt_f', 'proudart_f', \n",
    "     'proudhis_f', 'proudmil_f', 'proudsci_f']]\n",
    "\n",
    "# Dataset with controls\n",
    "controls = data2004_i[[\n",
    "    'sex', 'race_f', 'born_usa', 'party_fs', 'religstr_f', \n",
    "    'reltrad_f', 'region_f']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55b24ac-f239-4dea-9283-e3491ec65ee5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_clust = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c84df2-d928-437c-b4f8-200cc8b0d208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom score functions to avoid throwing errors when undefined\n",
    "def sil_score(data, pred_clust):\n",
    "    try:\n",
    "        sil_score = silhouette_score(data, pred_clust)\n",
    "    except ValueError:\n",
    "        sil_score = np.nan\n",
    "    return sil_score\n",
    "\n",
    "def ch_score(data, pred_clust):\n",
    "    try:\n",
    "        ch_score = calinski_harabasz_score(data, pred_clust)\n",
    "    except ValueError:\n",
    "        ch_score = np.nan\n",
    "    return ch_score\n",
    "\n",
    "def db_score(data, pred_clust):\n",
    "    try:\n",
    "        db_score = davies_bouldin_score(data, pred_clust)\n",
    "    except ValueError:\n",
    "        db_score = np.nan\n",
    "    return db_score\n",
    "\n",
    "def dunn_score(data, pred_clust):\n",
    "    torch_data = np.array(data)\n",
    "    torch_data = torch.tensor(torch_data, dtype=torch.float32)\n",
    "    torch_pred_clust = torch.tensor(pred_clust, dtype=torch.int64)\n",
    "\n",
    "    dunn_metric = DunnIndex()\n",
    "    \n",
    "    try:\n",
    "        dunn_score = float(dunn_metric(torch_data, torch_pred_clust))\n",
    "    except Exception:\n",
    "        dunn_score = np.nan\n",
    " \n",
    "    return dunn_score\n",
    "\n",
    "def inertia(data, labels):\n",
    "    data = np.asarray(data)\n",
    "    \n",
    "    inertia = 0\n",
    "    for cluster in np.unique(labels):\n",
    "        cluster_points = data[labels == cluster]\n",
    "        cluster_centroid = np.mean(cluster_points, axis=0)\n",
    "        inertia += np.sum((cluster_points - cluster_centroid) ** 2)\n",
    "        \n",
    "    return inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13766c98-f4b6-431d-98c2-d07e1a7e97b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should add min and average cluster size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabf9fd9-5fbf-4b62-aa77-f9b4e5a00284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display the optimal numbers of clutsters according to each validity index\n",
    "def elbow_plot(df, val_index):\n",
    "    res = df.dropna(subset=[val_index])\n",
    "\n",
    "    x = res[\"n_clust\"]\n",
    "    y = res[val_index]\n",
    "\n",
    "    if val_index == 'davies_bouldin':\n",
    "        knee_locator = KneeLocator(x, y, curve='concave', direction='increasing')\n",
    "    else:\n",
    "        knee_locator = KneeLocator(x, y, curve='convex', direction='decreasing')\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(x, y, marker=\"o\", linestyle=\"-\", label=val_index)\n",
    "    plt.axvline(x=knee_locator.knee, color=\"r\", linestyle=\"--\", label=f\"Optimal k={knee_locator.knee}\")\n",
    "    plt.xlabel(\"Number of Clusters\")\n",
    "    plt.ylabel(f\"{val_index} index\")\n",
    "    plt.title(f\"Elbow Method for {val_index} index\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1513b5bc-963b-41e5-a6bd-12f45f22c2f5",
   "metadata": {},
   "source": [
    "# Latent models\n",
    "With the StepMix package\n",
    "\n",
    "Documentation : https://github.com/Labo-Lacourse/stepmix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78aabed-4cb1-4e87-a35f-797240d34b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_range = range(1, max_clust)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320b6ebb-5f19-48f5-835e-444d8d2cc759",
   "metadata": {},
   "source": [
    "## Without covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e852e72e-ca78-477f-84a7-de05f2580e32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def do_StepMix(n, type, data):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "\n",
    "        model = StepMix(\n",
    "            n_components = n, \n",
    "            measurement = type, \n",
    "            n_init = 3)\n",
    "        \n",
    "        model.fit(data)\n",
    "        pred_clust = model.predict(data)\n",
    "\n",
    "        return {\n",
    "            'model': 'LCA' if type == 'categorical' else 'LPA',\n",
    "            'params': 'no covariates',\n",
    "            'n_clust': n,\n",
    "            'aic': model.aic(data),\n",
    "            'bic': model.bic(data),\n",
    "            'silhouette': sil_score(data, pred_clust),\n",
    "            'calinski_harabasz': ch_score(data, pred_clust),\n",
    "            'davies_bouldin': db_score(data, pred_clust),\n",
    "            'dunn': dunn_score(data, pred_clust),\n",
    "            'inertia': inertia(data, pred_clust)\n",
    "        }\n",
    "\n",
    "data = data_f.apply(lambda col: LabelEncoder().fit_transform(col))\n",
    "cat_results = Parallel(n_jobs=8)(delayed(do_StepMix)(n, 'categorical', data) for n in clust_range)\n",
    "LCA_all = pd.DataFrame(cat_results)\n",
    "\n",
    "num_results = Parallel(n_jobs=8)(delayed(do_StepMix)(n, 'continuous', data_n) for n in clust_range)\n",
    "LPA_all = pd.DataFrame(num_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d177789-db00-4153-8227-5ec8932dd193",
   "metadata": {},
   "outputs": [],
   "source": [
    "for val_index in ('silhouette','calinski_harabasz', 'davies_bouldin', 'dunn', 'inertia'):\n",
    "    elbow_plot(LPA_all, val_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c2fb00-c16f-4869-8334-4aaa1a193de9",
   "metadata": {},
   "source": [
    "## With covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d063ae3-82a3-4853-9d38-15418f0cfb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_StepMix_covar(n, type, data):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "\n",
    "        model = StepMix(\n",
    "            n_components = n, \n",
    "            measurement = type, \n",
    "            n_init = 3,\n",
    "            n_steps = 1,\n",
    "            structural = 'covariate', \n",
    "            structural_params = opt_params,\n",
    "            init_params = 'kmeans',\n",
    "            random_state = 123)\n",
    "        \n",
    "        model.fit(data, controls_dum)\n",
    "        pred_clust = model.predict(data)\n",
    "\n",
    "        return {\n",
    "            'model': 'LCA' if type == 'categorical' else 'LPA',\n",
    "            'params': 'with covariates',\n",
    "            'n_clust': n,\n",
    "            'aic': model.aic(data),\n",
    "            'bic': model.bic(data),\n",
    "            'silhouette': sil_score(data, pred_clust),\n",
    "            'calinski_harabasz': ch_score(data, pred_clust),\n",
    "            'davies_bouldin': db_score(data, pred_clust),\n",
    "            'dunn': dunn_score(data, pred_clust),\n",
    "            'inertia': inertia(data, pred_clust)\n",
    "        }\n",
    "\n",
    "opt_params = {\n",
    "    'method': 'gradient',\n",
    "    'intercept': True,\n",
    "    'max_iter': 2500,\n",
    "}\n",
    "\n",
    "controls_dum = pd.get_dummies(controls)\n",
    "\n",
    "data = data_f.apply(lambda col: LabelEncoder().fit_transform(col))\n",
    "cat_results = Parallel(n_jobs=8)(delayed(do_StepMix_covar)(n, 'categorical', data) for n in clust_range)\n",
    "LCA_covar_all = pd.DataFrame(cat_results)\n",
    "\n",
    "num_results = Parallel(n_jobs=8)(delayed(do_StepMix_covar)(n, 'continuous', data_n) for n in clust_range)\n",
    "LPA_covar_all = pd.DataFrame(num_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d8e1dc-5d1b-415a-b51f-a7b75be5ca46",
   "metadata": {},
   "source": [
    "## Best latent models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e6a8b3-eaad-4b81-a4cf-19367debf6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best models according to absolute fit = min aic / bic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939d848b-1152-4782-b6a3-f2d06db9933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best models according to relative fit = LRT / BLRT / BVR (LCA only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9fa7ce-84ae-4f7c-818e-aec8cb2c57f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best model through the Elbow method\n",
    "## Work in progress\n",
    "best_latent = pd.DataFrame()\n",
    "\n",
    "def elbow_method(df, val_index):\n",
    "    res = df.dropna(subset=[val_index])\n",
    "\n",
    "    x = res[\"n_clust\"]\n",
    "    y = res[val_index]\n",
    "\n",
    "    if val_index == 'davies_bouldin':\n",
    "        knee_locator = KneeLocator(x, y, curve='concave', direction='increasing')\n",
    "    else:\n",
    "        knee_locator = KneeLocator(x, y, curve='convex', direction='decreasing')\n",
    "    \n",
    "    return res[res[\"n_clust\"] == knee_locator.knee]\n",
    "\n",
    "models = [LCA_all, LPA_all]\n",
    "val_indexes = ['silhouette', 'calinski_harabasz', 'davies_bouldin', 'dunn', 'inertia']\n",
    "params = product(models, val_indexes)\n",
    "\n",
    "for model, val_index in params:\n",
    "    best_model = elbow_method(model, val_index)\n",
    "    best_latent = pd.concat([best_latent, best_model], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a49207-56b4-4705-9a3a-b8afe7d740b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find absolute best models for each validity index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d893bba-c892-432d-9cb0-85d85acd6304",
   "metadata": {},
   "source": [
    "# K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c7df35-9998-4899-aefe-049e80a984b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data_n)\n",
    "\n",
    "results = []\n",
    "\n",
    "def do_kmeans(n): \n",
    "    kmeans = KMeans(\n",
    "        n_clusters = n, \n",
    "        init = 'k-means++', \n",
    "        n_init = 25,\n",
    "        random_state=42)\n",
    "\n",
    "    pred_clust = kmeans.fit_predict(data)\n",
    "            \n",
    "    return{\n",
    "        'model': 'kmeans',\n",
    "        'params': 'centroid',\n",
    "        'n_clust': n,\n",
    "        'silhouette': sil_score(data, pred_clust),\n",
    "        'calinski_harabasz': ch_score(data, pred_clust),\n",
    "        'davies_bouldin': db_score(data, pred_clust),\n",
    "        'dunn': dunn_score(data, pred_clust),\n",
    "        'inertia': inertia(data, pred_clust)\n",
    "    }   \n",
    "\n",
    "clust_range = range(1, max_clust)\n",
    "\n",
    "results = Parallel(n_jobs=8)(delayed(do_kmeans)(n) for n in clust_range)\n",
    "\n",
    "kmeans_all = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d088363-80ec-475d-a431-4d72d78d9e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add other models, which are not implemented in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08613620-a1be-4a57-8e70-3b424df77619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best model for each combination of parameters through the Elbow method\n",
    "kmeans_elbow = pd.DataFrame()\n",
    "\n",
    "def elbow_method(val_index):\n",
    "    res = kmeans_all.dropna(subset=[val_index])\n",
    "\n",
    "    x = res[\"n_clust\"]\n",
    "    y = res[val_index]\n",
    "\n",
    "    if val_index == 'davies_bouldin':\n",
    "        knee_locator = KneeLocator(x, y, curve='concave', direction='increasing')\n",
    "    else:\n",
    "        knee_locator = KneeLocator(x, y, curve='convex', direction='decreasing')\n",
    "    \n",
    "    return res[res[\"n_clust\"] == knee_locator.knee]\n",
    "\n",
    "val_indexes = ['silhouette', 'calinski_harabasz', 'davies_bouldin', 'dunn', 'inertia']\n",
    "\n",
    "for val_index in val_indexes:\n",
    "    best_mod = elbow_method(val_index)\n",
    "    kmeans_elbow = pd.concat([kmeans_elbow, best_mod], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aefa7f6-75a4-43e1-9b09-6e9379f34a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find absolute best models for each validity index\n",
    "kmeans_elbow = kmeans_elbow.drop_duplicates().reset_index(drop=True)\n",
    "# Need to add colums indicating which validity index is maximized.\n",
    "# After that, duplicate models should be merged, not dropped.\n",
    "\n",
    "best_silhouette = kmeans_elbow.sort_values('silhouette', ascending=False).iloc[0]\n",
    "best_ch = kmeans_elbow.sort_values('calinski_harabasz', ascending=False).iloc[0]\n",
    "best_db = kmeans_elbow.sort_values('davies_bouldin', ascending=True).iloc[0]\n",
    "best_dunn = kmeans_elbow.sort_values('dunn', ascending=False).iloc[0]\n",
    "best_inertia = kmeans_elbow.sort_values('inertia', ascending=False).iloc[0]\n",
    "\n",
    "kmeans_best = pd.DataFrame([best_silhouette, best_ch, best_db, best_dunn, best_inertia])\n",
    "kmeans_best = kmeans_best.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24849e55-57cb-4680-9a5b-c8951503d389",
   "metadata": {},
   "source": [
    "# AHC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57b6080-574e-4d0a-bcf3-209d3284dfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the models\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data_n)\n",
    "\n",
    "results = []\n",
    "\n",
    "def do_AHC(n, dist, link):\n",
    "    ahc = AgglomerativeClustering(\n",
    "        n_clusters = n,\n",
    "        metric = dist,\n",
    "        linkage = link)\n",
    "    \n",
    "    ahc.fit(data)\n",
    "    \n",
    "    pred_clust = ahc.labels_\n",
    "\n",
    "    return {\n",
    "        'model': 'AHC',\n",
    "        'params': f\"distance = {dist}, linkage = {link}\",\n",
    "        'n_clust': n,\n",
    "        'silhouette': sil_score(data, pred_clust),\n",
    "        'calinski_harabasz': ch_score(data, pred_clust),\n",
    "        'davies_bouldin': db_score(data, pred_clust),\n",
    "        'dunn': dunn_score(data, pred_clust),\n",
    "        'inertia': inertia(data, pred_clust)\n",
    "    }\n",
    "\n",
    "clust_range = range(1, max_clust)\n",
    "distances = ['manhattan', 'euclidean', 'chebyshev']\n",
    "linkages = ['single', 'average', 'complete']\n",
    "params = product(clust_range, distances, linkages)\n",
    "\n",
    "results = Parallel(n_jobs=8)(delayed(do_AHC)(n, dist, link) for n, dist, link in params)\n",
    "\n",
    "results.extend([do_AHC(n, 'euclidean', 'ward') for n in clust_range])\n",
    "\n",
    "ahc_all = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7351b9-0ffc-4a85-b55f-569c986db981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best model for each combination of parameters through the Elbow method\n",
    "ahc_elbow = pd.DataFrame()\n",
    "\n",
    "def elbow_method(dist, link, val_index):\n",
    "    params = f\"distance = {dist}, linkage = {link}\"\n",
    "    res = ahc_all[ahc_res['params'] == params]\n",
    "    \n",
    "    res = res.dropna(subset=[val_index])\n",
    "\n",
    "    x = res[\"n_clust\"]\n",
    "    y = res[val_index]\n",
    "\n",
    "    if val_index == 'davies_bouldin':\n",
    "        knee_locator = KneeLocator(x, y, curve='concave', direction='increasing')\n",
    "    else:\n",
    "        knee_locator = KneeLocator(x, y, curve='convex', direction='decreasing')\n",
    "    \n",
    "    return res[res[\"n_clust\"] == knee_locator.knee]\n",
    "\n",
    "distances = ['manhattan', 'euclidean', 'chebyshev']\n",
    "linkages = ['single', 'average', 'complete']\n",
    "models = product(distances, linkages)\n",
    "\n",
    "val_indexes = ['silhouette', 'calinski_harabasz', 'davies_bouldin', 'dunn', 'inertia']\n",
    "\n",
    "for dist, link in models:\n",
    "    for val_index in val_indexes:\n",
    "        best_mod = elbow_method(dist, link, val_index)\n",
    "        ahc_elbow = pd.concat([ahc_elbow, best_mod], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8c12d3-f653-4861-a828-285aebb43187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find absolute best models for each validity index\n",
    "ahc_elbow = ahc_elbow.drop_duplicates().reset_index(drop=True)\n",
    "# Need to add colums indicating which validity index is maximized.\n",
    "# After that, duplicate models should be merged, not dropped.\n",
    "\n",
    "best_silhouette = ahc_elbow.sort_values('silhouette', ascending=False).iloc[0]\n",
    "best_ch = ahc_elbow.sort_values('calinski_harabasz', ascending=False).iloc[0]\n",
    "best_db = ahc_elbow.sort_values('davies_bouldin', ascending=True).iloc[0]\n",
    "best_dunn = ahc_elbow.sort_values('dunn', ascending=False).iloc[0]\n",
    "best_inertia = ahc_elbow.sort_values('inertia', ascending=False).iloc[0]\n",
    "\n",
    "ahc_best = pd.DataFrame([best_silhouette, best_ch, best_db, best_dunn, best_inertia])\n",
    "ahc_best = ahc_best.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf5a07e-2a44-4e5e-826d-7a2ac4b6fe57",
   "metadata": {},
   "source": [
    "# HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8b6bc8-18fe-44b9-bd27-26a30c706672",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data_n)\n",
    "\n",
    "results = []\n",
    "\n",
    "def do_hdbscan(dist, min_c, min_s):\n",
    "    hdb = HDBSCAN(\n",
    "        metric = dist,\n",
    "        min_cluster_size = min_c, \n",
    "        min_samples = min_s)\n",
    "        \n",
    "    pred_clust = hdb.fit_predict(data)\n",
    "        \n",
    "    n_clusters = len(set(pred_clust[pred_clust != -1]))\n",
    "    noise_freq = 100 * sum(pred_clust == -1) / len(pred_clust)\n",
    "        \n",
    "    return {\n",
    "        'model': 'HDBSCAN',\n",
    "        'params': f\"distance = {dist}, min_cluster_size = {min_c}, min_samples = {min_s}\",\n",
    "        'n_clust': n_clusters,\n",
    "        'noise': noise_freq,\n",
    "        'silhouette': sil_score(data, pred_clust),\n",
    "        'calinski_harabasz': ch_score(data, pred_clust),\n",
    "        'davies_bouldin': db_score(data, pred_clust),\n",
    "        'dunn': dunn_score(data, pred_clust),\n",
    "        'inertia': inertia(data, pred_clust)\n",
    "    }\n",
    "\n",
    "distances = ['euclidean', 'chebyshev']\n",
    "min_cluster_sizes = range(2, 16)\n",
    "min_samples_range = range(1, 16)\n",
    "params = product(distances, min_cluster_sizes, min_samples_range)\n",
    "\n",
    "results = Parallel(n_jobs=8)(delayed(do_hdbscan)(dist, min_c, min_s) for dist, min_c, min_s in params)\n",
    "\n",
    "hdbscan_all = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968b43fd-3261-4b35-885b-679ae6215117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Elbow method is inapplicable here. We simply select the model maximizing each validity index.\n",
    "best_silhouette = hdbscan_all.sort_values('silhouette', ascending=False).iloc[0]\n",
    "best_ch = hdbscan_all.sort_values('calinski_harabasz', ascending=False).iloc[0]\n",
    "best_db = hdbscan_all.sort_values('davies_bouldin', ascending=True).iloc[0]\n",
    "best_dunn = hdbscan_all.sort_values('dunn', ascending=False).iloc[0]\n",
    "best_inertia = hdbscan_all.sort_values('inertia', ascending=False).iloc[0]\n",
    "\n",
    "hdbscan_best = pd.DataFrame([best_silhouette, best_ch, best_db, best_dunn, best_inertia])\n",
    "hdbscan_best = hdbscan_best.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a21c7c9-0aa6-4edd-85ab-a71147062c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram grouping the values above 15\n",
    "bins = list(range(1, 16)) + [15.5]\n",
    "labels = list(range(1, 15)) + ['15+']\n",
    "plot_data = hdbscan_all['n_clust'].apply(lambda x: x if x <= 15 else 15.5)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(plot_data, bins=bins, edgecolor='black', align='left', rwidth=0.8)\n",
    "plt.xticks(bins[:-1], labels)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Number of Models')\n",
    "plt.title('Number of clusters selected by HDBSCAN models')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697f076e-6827-4bb5-8398-80c9a5151938",
   "metadata": {},
   "source": [
    "# Aggregate and display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44f939c-246a-4741-a7c1-810f1aaf35bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mod_list = [kmeans_best, ahc_best, hdbscan_best]\n",
    "best_models = pd.concat(best_mod_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5961a1-7104-4748-8390-554a430ca628",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb391ae1-da07-4c1e-bc6f-47bea22d93e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the best performing model on each criteria across model classes eliminates hdbscan models\n",
    "# Which could mean hdbscan is underperforming\n",
    "# Or is picking non-convex clusters\n",
    "# Or that data is non-clusterable!\n",
    "best_mod_list = [kmeans_best, ahc_best, hdbscan_best]\n",
    "best_models = pd.concat(best_mod_list, ignore_index=True)\n",
    "\n",
    "best_silhouette = best_models.sort_values('silhouette', ascending=False).iloc[0]\n",
    "best_ch = best_models.sort_values('calinski_harabasz', ascending=False).iloc[0]\n",
    "best_db = best_models.sort_values('davies_bouldin', ascending=True).iloc[0]\n",
    "best_dunn = best_models.sort_values('dunn', ascending=False).iloc[0]\n",
    "best_inertia = best_models.sort_values('inertia', ascending=False).iloc[0]\n",
    "\n",
    "best_models = pd.DataFrame([best_silhouette, best_ch, best_db, best_dunn, best_inertia])\n",
    "best_models = best_models.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42faee4a-4cee-41d1-8bcd-3ab5a699f1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a79babe-8f25-491a-a288-df2da0cff964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "bins = np.arange(best_models['n_clust'].min() - 0.5, best_models['n_clust'].max() + 1.5, 1)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(best_models['n_clust'], bins=bins, edgecolor='black', rwidth=0.8)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Number of Models')\n",
    "plt.title('Optimal number of clusters according to best models')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf4f29e-94cf-45df-b915-ee88922d7570",
   "metadata": {},
   "source": [
    "# Clusters visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ffe861-d41d-4a10-94c0-124b32624dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import ConvexHull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbabdc5c-3473-4a8f-8338-d4b5d2959e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA to represent the clusters in 2D\n",
    "pca = PCA(n_components=2)\n",
    "X_reduced = pca.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c907feb6-f378-41bd-a527-46c9749fbc4f",
   "metadata": {},
   "source": [
    "## For kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185b683a-3576-42be-8cd3-1b042ea88870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit an arbitrary model\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data_n)\n",
    "\n",
    "kmeans = KMeans(n_clusters=7, random_state=42)\n",
    "pred_clust = kmeans.fit_predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66241561-c493-43ab-b55c-0b21e0d88bcb",
   "metadata": {},
   "source": [
    "### Datapoints alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016a8155-394c-43f1-9094-f6b9506bc506",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=pred_clust, cmap='tab10', s=20, edgecolors='k')\n",
    "plt.xlabel(\"Dim 1\")\n",
    "plt.ylabel(\"Dim 2\")\n",
    "plt.axhline(y=0, color='#333333', linestyle='--', linewidth=1)\n",
    "plt.axvline(x=0, color='#333333', linestyle='--', linewidth=1)\n",
    "plt.title(\"Clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc86e037-57fc-4e66-8c18-9decd265bb6b",
   "metadata": {},
   "source": [
    "### With decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b7bdaa-210e-4c32-93c2-b33373851eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid for boundary visualization in 2D space\n",
    "x_min, x_max = X_reduced[:, 0].min() - 0.5, X_reduced[:, 0].max() + 0.5\n",
    "y_min, y_max = X_reduced[:, 1].min() - 0.5, X_reduced[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300), np.linspace(y_min, y_max, 300))\n",
    "\n",
    "# Project grid points back to original space\n",
    "grid_points_2D = np.c_[xx.ravel(), yy.ravel()]\n",
    "grid_points_original = pca.inverse_transform(grid_points_2D)\n",
    "\n",
    "# Predict clusters in the original space\n",
    "grid_clusters = kmeans.predict(grid_points_original).reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create scatter plot first to get the color mapping\n",
    "scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], \n",
    "                     c=pred_clust, cmap='tab10', \n",
    "                     s=15, edgecolors='k')\n",
    "\n",
    "# Plot boundaries using the same colormap and normalization\n",
    "plt.contourf(xx, yy, grid_clusters, \n",
    "             alpha=0.3, \n",
    "             cmap=scatter.cmap,\n",
    "             norm=scatter.norm)\n",
    "\n",
    "# Plot centroids with labels\n",
    "centroids_pca = pca.transform(kmeans.cluster_centers_)\n",
    "for i, (x, y) in enumerate(centroids_pca):\n",
    "    plt.text(x, y, str(i), color='white', fontsize=12, \n",
    "             ha='center', va='center', fontweight='bold',\n",
    "             bbox=dict(facecolor='black', edgecolor='none', boxstyle='round,pad=0.2'))\n",
    "\n",
    "plt.xlabel(\"Dim 1\")\n",
    "plt.ylabel(\"Dim 2\")\n",
    "plt.title(\"Clusters with Decision Boundaries\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fe364d-5884-412d-8e14-688a268096d7",
   "metadata": {},
   "source": [
    "### With convex hulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662b1ff5-b4d8-4a6e-bdf9-b09d7756ba4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Collect all hull vertices\n",
    "hull_vertices = []\n",
    "hull_colors = []\n",
    "for i in range(kmeans.n_clusters):\n",
    "    cluster_points = X_reduced[pred_clust == i]\n",
    "    if len(cluster_points) > 2:\n",
    "        hull = ConvexHull(cluster_points)\n",
    "        hull_vertices.append((\n",
    "            cluster_points[hull.vertices, 0],\n",
    "            cluster_points[hull.vertices, 1]\n",
    "        ))\n",
    "        hull_colors.append(i)\n",
    "\n",
    "# Plot datapoints\n",
    "scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], \n",
    "                     c=pred_clust, cmap='tab10', \n",
    "                     s=15, edgecolors='k')\n",
    "\n",
    "# Plot all hulls using the same colormap\n",
    "for vertices, i in zip(hull_vertices, hull_colors):\n",
    "    plt.fill(vertices[0], vertices[1], \n",
    "             alpha=0.3,\n",
    "             color=scatter.cmap(scatter.norm(i)))\n",
    "\n",
    "legend = plt.legend(*scatter.legend_elements())\n",
    "plt.xlabel(\"Dim 1\")\n",
    "plt.ylabel(\"Dim 2\")\n",
    "plt.title(\"Clusters with Convex Hulls\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4dfb1a-a865-4a06-94f9-60777e73b877",
   "metadata": {},
   "source": [
    "## For HDBSCAN\n",
    "Example of non-convex clusters in the PCA space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc36761-34de-4f68-84a2-60faba46d8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of non-convex clusters in the PCA space\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data_n)\n",
    "\n",
    "hdb = HDBSCAN(min_cluster_size = 5, min_samples = 1)  \n",
    "pred_clust = hdb.fit_predict(data)\n",
    "n_clusters = len(set(pred_clust[pred_clust != -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0425d28d-ee7d-45e7-b612-180da9248ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best-performing model\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data_n)\n",
    "\n",
    "hdb = HDBSCAN(metric = 'euclidean', min_cluster_size = 4, min_samples = 2)  \n",
    "pred_clust = hdb.fit_predict(data)\n",
    "n_clusters = len(set(pred_clust[pred_clust != -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429b6fd7-a4c7-4ec6-8c8d-76937af5c646",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Collect all hull vertices\n",
    "hull_vertices = []\n",
    "hull_colors = []\n",
    "for i in range(n_clusters):\n",
    "    cluster_points = X_reduced[pred_clust == i]\n",
    "    if len(cluster_points) > 2:\n",
    "        hull = ConvexHull(cluster_points)\n",
    "        hull_vertices.append((\n",
    "            cluster_points[hull.vertices, 0],\n",
    "            cluster_points[hull.vertices, 1]\n",
    "        ))\n",
    "        hull_colors.append(i)\n",
    "\n",
    "# Plot datapoints\n",
    "scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], \n",
    "                     c=pred_clust, cmap='tab10', \n",
    "                     s=15, edgecolors='k')\n",
    "\n",
    "# Plot all hulls using the same colormap\n",
    "for vertices, i in zip(hull_vertices, hull_colors):\n",
    "    plt.fill(vertices[0], vertices[1], \n",
    "             alpha=0.7,\n",
    "             color=scatter.cmap(scatter.norm(i)))\n",
    "\n",
    "legend = plt.legend(*scatter.legend_elements())\n",
    "plt.xlabel(\"Dim 1\")\n",
    "plt.ylabel(\"Dim 2\")\n",
    "plt.title(\"Clusters with Convex Hulls\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927a8549-babf-402e-8869-a80f25c416f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
