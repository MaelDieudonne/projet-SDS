{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676fe747-bd8b-4115-83bc-0a18c2c9cd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torchmetrics\n",
    "# pip install stepmix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4887e505-68fb-40c0-adb5-06ef3199e312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from joblib import Parallel, delayed # for parallelization\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.cluster import KMeans, HDBSCAN\n",
    "\n",
    "import torch\n",
    "from torchmetrics.clustering import DunnIndex\n",
    "\n",
    "from stepmix.stepmix import StepMix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d1e61c-3574-4482-9529-59268b7fb8d6",
   "metadata": {},
   "source": [
    "# Data, parameters and performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1190dd79-e4b5-4684-98b0-0f6ce7797686",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2004_i = pd.read_parquet(\"data/data2004_i.parquet\") # load imputed data\n",
    "\n",
    "# Dataset with numeric outcomes\n",
    "data_n = data2004_i[[\n",
    "    'clseusa_n', 'ambornin_n', 'amcit_n', 'amlived_n', 'amenglsh_n', \n",
    "     'amchrstn_n', 'amgovt_n', 'amfeel_n', 'amcitizn_n', 'amshamed_n', \n",
    "     'belikeus_n', 'ambetter_n', 'ifwrong_n', 'proudsss_n', 'proudgrp_n', \n",
    "     'proudpol_n', 'prouddem_n', 'proudeco_n', 'proudspt_n', 'proudart_n', \n",
    "     'proudhis_n', 'proudmil_n', 'proudsci_n']]\n",
    "\n",
    "# Dataset with categorical outcomes\n",
    "data_f = data2004_i[[\n",
    "     'clseusa_f', 'ambornin_f', 'amcit_f', 'amlived_f', 'amenglsh_f', \n",
    "     'amchrstn_f', 'amgovt_f', 'amfeel_f', 'amcitizn_f', 'amshamed_f', \n",
    "     'belikeus_f', 'ambetter_f', 'ifwrong_f', 'proudsss_f', 'proudgrp_f', \n",
    "     'proudpol_f', 'prouddem_f', 'proudeco_f', 'proudspt_f', 'proudart_f', \n",
    "     'proudhis_f', 'proudmil_f', 'proudsci_f']]\n",
    "\n",
    "# Dataset with controls\n",
    "controls = data2004_i[[\n",
    "    'sex', 'race_f', 'born_usa', 'party_fs', 'religstr_f', \n",
    "    'reltrad_f', 'region_f']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55b24ac-f239-4dea-9283-e3491ec65ee5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_clust = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c84df2-d928-437c-b4f8-200cc8b0d208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom score functions to avoid throwing errors when undefined\n",
    "def sil_score(data, pred_clust):\n",
    "    try:\n",
    "        sil_score = silhouette_score(data, pred_clust)\n",
    "    except ValueError:\n",
    "        sil_score = np.nan\n",
    "    return sil_score\n",
    "\n",
    "def ch_score(data, pred_clust):\n",
    "    try:\n",
    "        ch_score = calinski_harabasz_score(data, pred_clust)\n",
    "    except ValueError:\n",
    "        ch_score = np.nan\n",
    "    return ch_score\n",
    "\n",
    "def db_score(data, pred_clust):\n",
    "    try:\n",
    "        db_score = davies_bouldin_score(data, pred_clust)\n",
    "    except ValueError:\n",
    "        db_score = np.nan\n",
    "    return db_score\n",
    "\n",
    "def dunn_score(data, pred_clust):\n",
    "    torch_data = np.array(data)\n",
    "    torch_data = torch.tensor(torch_data, dtype=torch.float32)\n",
    "    torch_pred_clust = torch.tensor(pred_clust, dtype=torch.int64)\n",
    "\n",
    "    dunn_metric = DunnIndex()\n",
    "    \n",
    "    try:\n",
    "        dunn_score = float(dunn_metric(torch_data, torch_pred_clust))\n",
    "    except Exception:\n",
    "        dunn_score = np.nan\n",
    " \n",
    "    return dunn_score\n",
    "\n",
    "def inertia(data, labels):\n",
    "    data = np.asarray(data)\n",
    "    \n",
    "    inertia = 0\n",
    "    for cluster in np.unique(labels):\n",
    "        cluster_points = data[labels == cluster]\n",
    "        cluster_centroid = np.mean(cluster_points, axis=0)\n",
    "        inertia += np.sum((cluster_points - cluster_centroid) ** 2)\n",
    "        \n",
    "    return inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1e4921-2c93-414c-8ce1-1ddd0710de96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df to store the results of kneelocator\n",
    "best_models = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1513b5bc-963b-41e5-a6bd-12f45f22c2f5",
   "metadata": {},
   "source": [
    "# Latent models\n",
    "With the StepMix package\n",
    "\n",
    "Documentation : https://github.com/Labo-Lacourse/stepmix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78aabed-4cb1-4e87-a35f-797240d34b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_range = range(1, max_clust)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320b6ebb-5f19-48f5-835e-444d8d2cc759",
   "metadata": {},
   "source": [
    "## Without covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e852e72e-ca78-477f-84a7-de05f2580e32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def do_StepMix(n, type, data):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "\n",
    "        model = StepMix(\n",
    "            n_components = n, \n",
    "            measurement = type, \n",
    "            n_init = 3)\n",
    "        \n",
    "        model.fit(data)\n",
    "        pred_clust = model.predict(data)\n",
    "\n",
    "        return {\n",
    "            'n_components': n,\n",
    "            'aic': model.aic(data),\n",
    "            'bic': model.bic(data),\n",
    "            'silhouette': sil_score(data, pred_clust),\n",
    "            'calinski_harabasz': ch_score(data, pred_clust),\n",
    "            'davies_bouldin': db_score(data, pred_clust),\n",
    "            'dunn': dunn_score(data, pred_clust),\n",
    "            'inertia': inertia(data, pred_clust)\n",
    "        }\n",
    "\n",
    "data = data_f.apply(lambda col: LabelEncoder().fit_transform(col))\n",
    "cat_res = Parallel(n_jobs=8)(delayed(do_StepMix)(n, 'categorical', data) for n in clust_range)\n",
    "LCA_res = pd.DataFrame(cat_res)\n",
    "\n",
    "num_res = Parallel(n_jobs=8)(delayed(do_StepMix)(n, 'continuous', data_n) for n in clust_range)\n",
    "LPA_res = pd.DataFrame(num_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9e5606-d1e9-4cd2-9138-af3c30050b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply kneelocator to LCA_res and LPA_res here (for each performance metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c2fb00-c16f-4869-8334-4aaa1a193de9",
   "metadata": {},
   "source": [
    "## With covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d063ae3-82a3-4853-9d38-15418f0cfb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_StepMix_covar(n, type, data):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "\n",
    "        model = StepMix(\n",
    "            n_components = n, \n",
    "            measurement = type, \n",
    "            n_init = 3,\n",
    "            n_steps = 1,\n",
    "            structural = 'covariate', \n",
    "            structural_params = opt_params,\n",
    "            init_params = 'kmeans',\n",
    "            random_state = 123)\n",
    "        \n",
    "        model.fit(data, controls_dum)\n",
    "        pred_clust = model.predict(data)\n",
    "\n",
    "        return {\n",
    "            'n_components': n,\n",
    "            'aic': model.aic(data),\n",
    "            'bic': model.bic(data),\n",
    "            'silhouette': sil_score(data, pred_clust),\n",
    "            'calinski_harabasz': ch_score(data, pred_clust),\n",
    "            'davies_bouldin': db_score(data, pred_clust),\n",
    "            'dunn': dunn_score(data, pred_clust),\n",
    "            'inertia': inertia(data, pred_clust)\n",
    "        }\n",
    "\n",
    "opt_params = {\n",
    "    'method': 'gradient',\n",
    "    'intercept': True,\n",
    "    'max_iter': 2500,\n",
    "}\n",
    "\n",
    "controls_dum = pd.get_dummies(controls)\n",
    "\n",
    "data = data_f.apply(lambda col: LabelEncoder().fit_transform(col))\n",
    "cat_res = Parallel(n_jobs=8)(delayed(do_StepMix_covar)(n, 'categorical', data) for n in clust_range)\n",
    "LCA_covar_res = pd.DataFrame(cat_res)\n",
    "\n",
    "num_res = Parallel(n_jobs=8)(delayed(do_StepMix_covar)(n, 'continuous', data_n) for n in clust_range)\n",
    "LPA_covar_res = pd.DataFrame(num_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb15a896-2b61-403f-a1e7-470f9bd13f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply kneelocator to LCA_covar_res and LPA_covar_res here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d893bba-c892-432d-9cb0-85d85acd6304",
   "metadata": {},
   "source": [
    "# K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c7df35-9998-4899-aefe-049e80a984b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data_n)\n",
    "\n",
    "results = []\n",
    "\n",
    "for n_init in [5, 10, 20]:\n",
    "    for n_clusters in range(2, max_clust):\n",
    "        kmeans = KMeans(\n",
    "            n_clusters = n_clusters, \n",
    "            init = 'k-means++', \n",
    "            n_init = n_init, \n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        pred_clust = kmeans.fit_predict(data)\n",
    "            \n",
    "        results.append({\n",
    "            'n_clusters': n_clusters,\n",
    "            'n_init': n_init,\n",
    "            'silhouette': sil_score(data, pred_clust),\n",
    "            'calinski_harabasz': ch_score(data, pred_clust),\n",
    "            'davies_bouldin': db_score(data, pred_clust),\n",
    "            'dunn': dunn_score(data, pred_clust),\n",
    "            'inertia': inertia(data, pred_clust)\n",
    "        })\n",
    "        \n",
    "        # kneelocator here\n",
    "        # add results to best_models\n",
    "\n",
    "kmeans_res = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc48685-df3e-41aa-9fe5-88368bffc130",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_silhouette = kmeans_res.sort_values('silhouette', ascending=False).iloc[0]\n",
    "best_calinski = kmeans_res.sort_values('calinski_harabasz', ascending=False).iloc[0]\n",
    "best_davies = kmeans_res.sort_values('davies_bouldin', ascending=True).iloc[0] # Lower is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca1fb4f-e4e4-4498-8ecd-d3b5a1715a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to compute the average inertia across models with the same number of clusters\n",
    "# kmeans_res.groupby('n_clusters')['inertia'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24849e55-57cb-4680-9a5b-c8951503d389",
   "metadata": {},
   "source": [
    "# AHC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7630b5d9-8194-4030-952f-1d3c3f4cf404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebf5a07e-2a44-4e5e-826d-7a2ac4b6fe57",
   "metadata": {},
   "source": [
    "# HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8b6bc8-18fe-44b9-bd27-26a30c706672",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data_n)\n",
    "\n",
    "min_cluster_sizes = range(2, 16)\n",
    "min_samples_range = range(1, 16)\n",
    "\n",
    "# min_samples_range = [5, 10, 20, 50]\n",
    "# min_cluster_sizes = [5, 10, 20, 30, 40, 50, 60]\n",
    "\n",
    "results = []\n",
    "\n",
    "for min_cluster_size in min_cluster_sizes:\n",
    "    for min_samples in min_samples_range:\n",
    "        hdb = HDBSCAN(\n",
    "            min_cluster_size = min_cluster_size, \n",
    "            min_samples = min_samples)\n",
    "        \n",
    "        pred_clust = hdb.fit_predict(data)\n",
    "        \n",
    "        n_clusters = len(set(pred_clust[pred_clust != -1]))\n",
    "        noise_freq = 100 * sum(pred_clust == -1) / len(pred_clust)\n",
    "        \n",
    "        results.append({\n",
    "            'min_cluster_size': min_cluster_size,\n",
    "            'min_samples': min_samples,\n",
    "            'n_clusters': n_clusters,\n",
    "            'noise': noise_freq,\n",
    "            'silhouette': sil_score(data, pred_clust),\n",
    "            'calinski_harabasz': ch_score(data, pred_clust),\n",
    "            'davies_bouldin': db_score(data, pred_clust),\n",
    "            'dunn': dunn_score(data, pred_clust),\n",
    "            'inertia': inertia(data, pred_clust)\n",
    "        })\n",
    "\n",
    "hdbscan_res = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968b43fd-3261-4b35-885b-679ae6215117",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_silhouette = hdbscan_res.sort_values('silhouette', ascending=False).iloc[0]\n",
    "best_calinski = hdbscan_res.sort_values('calinski_harabasz', ascending=False).iloc[0]\n",
    "best_davies = hdbscan_res.sort_values('davies_bouldin', ascending=True).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5bccaf-3c41-4f35-ba79-0bcb894ed112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply kneelocator to hdbscan_res here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dda7a35-e8f0-49df-b5d0-bfaf4148749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_res['n_clusters'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40e5f0e-a7c6-404d-abd4-bb302777397d",
   "metadata": {},
   "source": [
    "# Spectral clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518816f4-4411-4cd0-bf40-c46c3d9762f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "results = []\n",
    "affinity_methods = ['nearest_neighbors', 'rbf']  # Different affinity computations\n",
    "kernel_params = [0.1, 0.5, 1.0, 2.0]  # Different gamma values for RBF kernel\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "for n_clusters in range(2, max_clust):\n",
    "    for affinity in affinity_methods:\n",
    "        for gamma in kernel_params:\n",
    "            # For nearest_neighbors affinity, create a connectivity matrix\n",
    "            if affinity == 'nearest_neighbors':\n",
    "                connectivity = kneighbors_graph(data, n_neighbors=10, mode='connectivity')\n",
    "                connectivity = connectivity.toarray()\n",
    "            else:\n",
    "                connectivity = None\n",
    "            \n",
    "            spectral = SpectralClustering(\n",
    "                n_clusters = n_clusters,\n",
    "                affinity = affinity,\n",
    "                gamma = gamma,\n",
    "                random_state = 42\n",
    "            )\n",
    "            \n",
    "            pred_clust = spectral.fit_predict(data)\n",
    "            \n",
    "            unique_clusters = len(np.unique(pred_clust))\n",
    "\n",
    "            results.append({\n",
    "                'n_clusters': n_clusters,\n",
    "                'affinity': affinity,\n",
    "                'gamma': gamma,\n",
    "                'n_clusters': unique_clusters,\n",
    "                'silhouette': sil_score(data, pred_clust),\n",
    "                'calinski_harabasz': ch_score(data, pred_clust),\n",
    "                'davies_bouldin': db_score(data, pred_clust),\n",
    "                'dunn': dunn_score(data, pred_clust),\n",
    "                'inertia': inertia(data, pred_clust)\n",
    "            })\n",
    "\n",
    "spec_res = pd.DataFrame(results)\n",
    "\n",
    "best_silhouette = spec_res.sort_values('silhouette', ascending=False).iloc[0]\n",
    "best_calinski = spec_res.sort_values('calinski_harabasz', ascending=False).iloc[0]\n",
    "best_davies = spec_res.sort_values('davies_bouldin', ascending=True).iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f22813-ee80-4b86-a7f2-9df0a568a88d",
   "metadata": {},
   "source": [
    "# Affinity propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22142f6e-c3eb-4f95-acf4-ae88b297830c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "data = data_n\n",
    "\n",
    "model = AffinityPropagation(damping=0.7, max_iter=350, convergence_iter=25)\n",
    "model.fit(data)\n",
    "pred_clust = model.labels_\n",
    "\n",
    "results = []\n",
    "\n",
    "results.append({\n",
    "        'n_components': len(set(pred_clust)),\n",
    "        'silhouette': sil_score(data, pred_clust),\n",
    "        'calinski_harabasz': ch_score(data, pred_clust),\n",
    "        'davies_bouldin': db_score(data, pred_clust),\n",
    "        'dunn': dunn_score(data, pred_clust),\n",
    "        'inertia': inertia(data, pred_clust)\n",
    "})\n",
    "    \n",
    "af_res = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697f076e-6827-4bb5-8398-80c9a5151938",
   "metadata": {},
   "source": [
    "# Aggregate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f83ba3-fbb4-4c05-b22e-9bec4407eabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results df to merge, after harmonizing their structure\n",
    "# Or unecessary, if the best models are already stored in a dedicated df?\n",
    "# LCA_res\n",
    "# LPA_res\n",
    "# LCA_covar_res\n",
    "# LPA_covar_res\n",
    "# kmeans_res\n",
    "# hdbscan_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf4f29e-94cf-45df-b915-ee88922d7570",
   "metadata": {},
   "source": [
    "# Visualization for kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ffe861-d41d-4a10-94c0-124b32624dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import ConvexHull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbabdc5c-3473-4a8f-8338-d4b5d2959e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA to represent the clusters in 2D\n",
    "pca = PCA(n_components=2)\n",
    "X_reduced = pca.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185b683a-3576-42be-8cd3-1b042ea88870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit an arbitrary model\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data_n)\n",
    "\n",
    "kmeans = KMeans(n_clusters=7, random_state=42)\n",
    "pred_clust = kmeans.fit_predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66241561-c493-43ab-b55c-0b21e0d88bcb",
   "metadata": {},
   "source": [
    "## Datapoints alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016a8155-394c-43f1-9094-f6b9506bc506",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=pred_clust, cmap='tab10', s=20, edgecolors='k')\n",
    "plt.xlabel(\"Dim 1\")\n",
    "plt.ylabel(\"Dim 2\")\n",
    "plt.axhline(y=0, color='#333333', linestyle='--', linewidth=1)\n",
    "plt.axvline(x=0, color='#333333', linestyle='--', linewidth=1)\n",
    "plt.title(\"Clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc86e037-57fc-4e66-8c18-9decd265bb6b",
   "metadata": {},
   "source": [
    "## With decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b7bdaa-210e-4c32-93c2-b33373851eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid for boundary visualization in 2D space\n",
    "x_min, x_max = X_reduced[:, 0].min() - 0.5, X_reduced[:, 0].max() + 0.5\n",
    "y_min, y_max = X_reduced[:, 1].min() - 0.5, X_reduced[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300), np.linspace(y_min, y_max, 300))\n",
    "\n",
    "# Project grid points back to original space\n",
    "grid_points_2D = np.c_[xx.ravel(), yy.ravel()]\n",
    "grid_points_original = pca.inverse_transform(grid_points_2D)\n",
    "\n",
    "# Predict clusters in the original space\n",
    "grid_clusters = kmeans.predict(grid_points_original).reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create scatter plot first to get the color mapping\n",
    "scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], \n",
    "                     c=pred_clust, cmap='tab10', \n",
    "                     s=15, edgecolors='k')\n",
    "\n",
    "# Plot boundaries using the same colormap and normalization\n",
    "plt.contourf(xx, yy, grid_clusters, \n",
    "             alpha=0.3, \n",
    "             cmap=scatter.cmap,\n",
    "             norm=scatter.norm)\n",
    "\n",
    "# Plot centroids with labels\n",
    "centroids_pca = pca.transform(kmeans.cluster_centers_)\n",
    "for i, (x, y) in enumerate(centroids_pca):\n",
    "    plt.text(x, y, str(i), color='white', fontsize=12, \n",
    "             ha='center', va='center', fontweight='bold',\n",
    "             bbox=dict(facecolor='black', edgecolor='none', boxstyle='round,pad=0.2'))\n",
    "\n",
    "plt.xlabel(\"Dim 1\")\n",
    "plt.ylabel(\"Dim 2\")\n",
    "plt.title(\"Clusters with Decision Boundaries\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fe364d-5884-412d-8e14-688a268096d7",
   "metadata": {},
   "source": [
    "## With convex hulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662b1ff5-b4d8-4a6e-bdf9-b09d7756ba4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Collect all hull vertices\n",
    "hull_vertices = []\n",
    "hull_colors = []\n",
    "for i in range(kmeans.n_clusters):\n",
    "    cluster_points = X_reduced[pred_clust == i]\n",
    "    if len(cluster_points) > 2:\n",
    "        hull = ConvexHull(cluster_points)\n",
    "        hull_vertices.append((\n",
    "            cluster_points[hull.vertices, 0],\n",
    "            cluster_points[hull.vertices, 1]\n",
    "        ))\n",
    "        hull_colors.append(i)\n",
    "\n",
    "# Plot datapoints\n",
    "scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], \n",
    "                     c=pred_clust, cmap='tab10', \n",
    "                     s=15, edgecolors='k')\n",
    "\n",
    "# Plot all hulls using the same colormap\n",
    "for vertices, i in zip(hull_vertices, hull_colors):\n",
    "    plt.fill(vertices[0], vertices[1], \n",
    "             alpha=0.3,\n",
    "             color=scatter.cmap(scatter.norm(i)))\n",
    "\n",
    "legend = plt.legend(*scatter.legend_elements())\n",
    "plt.xlabel(\"Dim 1\")\n",
    "plt.ylabel(\"Dim 2\")\n",
    "plt.title(\"Clusters with Convex Hulls\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4dfb1a-a865-4a06-94f9-60777e73b877",
   "metadata": {},
   "source": [
    "# Visualization for HDBSCAN\n",
    "Example of non-convex clusters in the PCA place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc36761-34de-4f68-84a2-60faba46d8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data_n)\n",
    "\n",
    "hdb = HDBSCAN(min_cluster_size = 5, min_samples = 1)  \n",
    "pred_clust = hdb.fit_predict(data)\n",
    "n_clusters = len(set(pred_clust[pred_clust != -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429b6fd7-a4c7-4ec6-8c8d-76937af5c646",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Collect all hull vertices\n",
    "hull_vertices = []\n",
    "hull_colors = []\n",
    "for i in range(n_clusters):\n",
    "    cluster_points = X_reduced[pred_clust == i]\n",
    "    if len(cluster_points) > 2:\n",
    "        hull = ConvexHull(cluster_points)\n",
    "        hull_vertices.append((\n",
    "            cluster_points[hull.vertices, 0],\n",
    "            cluster_points[hull.vertices, 1]\n",
    "        ))\n",
    "        hull_colors.append(i)\n",
    "\n",
    "# Plot datapoints\n",
    "scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], \n",
    "                     c=pred_clust, cmap='tab10', \n",
    "                     s=15, edgecolors='k')\n",
    "\n",
    "# Plot all hulls using the same colormap\n",
    "for vertices, i in zip(hull_vertices, hull_colors):\n",
    "    plt.fill(vertices[0], vertices[1], \n",
    "             alpha=0.7,\n",
    "             color=scatter.cmap(scatter.norm(i)))\n",
    "\n",
    "legend = plt.legend(*scatter.legend_elements())\n",
    "plt.xlabel(\"Dim 1\")\n",
    "plt.ylabel(\"Dim 2\")\n",
    "plt.title(\"Clusters with Convex Hulls\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927a8549-babf-402e-8869-a80f25c416f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
